[2024-01-21 14:05:51 root] (main_tome.py 216): INFO Namespace(batch_size=256, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:05:55 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:05:57 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:05:59 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:15:13 root] (main_tome.py 216): INFO Namespace(batch_size=256, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:15:17 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:15:20 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:15:21 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:15:21 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:16:41 root] (main_tome.py 216): INFO Namespace(batch_size=256, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:16:46 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:16:48 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:16:50 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:16:50 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:17:53 root] (main_tome.py 216): INFO Namespace(batch_size=100, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:17:58 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:18:00 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:18:01 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:18:01 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:18:32 root] (main_tome.py 216): INFO Namespace(batch_size=100, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:18:36 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:18:39 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:18:40 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:18:40 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:19:04 root] (main_tome.py 216): INFO Namespace(batch_size=103, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:19:09 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:19:12 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:19:13 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:19:13 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:19:35 root] (main_tome.py 216): INFO Namespace(batch_size=259, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:19:40 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:19:42 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:19:47 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:19:47 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:22:09 root] (main_tome.py 216): INFO Namespace(batch_size=259, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:22:14 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:22:17 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:22:22 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:22:22 root] (main_tome.py 416): INFO Start training for 2 epochs
[2024-01-21 14:23:48 root] (main_tome.py 213): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:23:53 root] (main_tome.py 283): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:23:54 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:23:55 root] (main_tome.py 367): INFO number of params: 86567656
[2024-01-21 14:23:55 root] (main_tome.py 413): INFO Start training for 2 epochs
[2024-01-21 14:35:29 root] (main_tome.py 213): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:35:34 root] (main_tome.py 283): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:35:36 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:35:36 root] (main_tome.py 367): INFO number of params: 86567656
[2024-01-21 14:35:36 root] (main_tome.py 413): INFO Start training for 2 epochs
[2024-01-21 14:40:00 root] (main_tome.py 213): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 14:40:04 root] (main_tome.py 283): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:40:06 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:40:07 root] (main_tome.py 367): INFO number of params: 86567656
[2024-01-21 14:40:07 root] (main_tome.py 413): INFO Start training for 2 epochs
[2024-01-21 14:43:13 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 15:48:46 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 15:48:53 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:48:54 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:49:00 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:49:01 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:49:01 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:49:25 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 15:49:40 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:49:41 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:49:44 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:49:45 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:49:45 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:50:55 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 15:50:58 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:50:59 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:51:02 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:51:03 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:51:03 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:51:06 root] (utils.py 293): INFO Epoch: [0]  [   0/3152]  eta: 2:14:04  lr_architecture: 0.010000  loss_cls: 3.1339 (3.1339)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 4.2487 (4.2487)  time: 2.5521  data: 0.0008  max mem: 8707
[2024-01-21 15:51:09 root] (utils.py 293): INFO Epoch: [0]  [  10/3152]  eta: 0:26:55  lr_architecture: 0.010000  loss_cls: 8.6409 (8.3651)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 3.0166 (inf)  time: 0.5140  data: 0.0002  max mem: 9646
[2024-01-21 15:51:12 root] (utils.py 293): INFO Epoch: [0]  [  20/3152]  eta: 0:21:19  lr_architecture: 0.010000  loss_cls: 8.0173 (8.1308)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.9231 (inf)  time: 0.3012  data: 0.0001  max mem: 9646
[2024-01-21 15:51:15 root] (utils.py 293): INFO Epoch: [0]  [  30/3152]  eta: 0:19:19  lr_architecture: 0.010000  loss_cls: 7.4798 (7.8869)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.0680 (inf)  time: 0.2929  data: 0.0001  max mem: 9646
[2024-01-21 15:51:35 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 15:51:39 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:51:40 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:51:43 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:51:44 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:51:44 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:51:46 root] (utils.py 293): INFO Epoch: [0]  [   0/3152]  eta: 2:16:36  lr_architecture: 0.010000  loss_cls: 3.1333 (3.1333)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 4.2495 (4.2495)  time: 2.6005  data: 0.0009  max mem: 8707
[2024-01-21 15:51:49 root] (utils.py 293): INFO Epoch: [0]  [  10/3152]  eta: 0:26:50  lr_architecture: 0.010000  loss_cls: 8.5083 (8.3858)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 2.9235 (inf)  time: 0.5126  data: 0.0002  max mem: 9646
[2024-01-21 15:51:52 root] (utils.py 293): INFO Epoch: [0]  [  20/3152]  eta: 0:21:18  lr_architecture: 0.010000  loss_cls: 7.9503 (8.1368)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.9709 (inf)  time: 0.2987  data: 0.0001  max mem: 9646
[2024-01-21 15:51:55 root] (utils.py 293): INFO Epoch: [0]  [  30/3152]  eta: 0:19:18  lr_architecture: 0.010000  loss_cls: 7.5771 (7.9190)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.0921 (inf)  time: 0.2930  data: 0.0001  max mem: 9646
[2024-01-21 15:51:58 root] (utils.py 293): INFO Epoch: [0]  [  40/3152]  eta: 0:18:14  lr_architecture: 0.010000  loss_cls: 7.3191 (7.7447)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.7031 (inf)  time: 0.2920  data: 0.0001  max mem: 9646
[2024-01-21 15:52:01 root] (utils.py 293): INFO Epoch: [0]  [  50/3152]  eta: 0:17:34  lr_architecture: 0.010000  loss_cls: 7.1679 (7.6277)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.5822 (inf)  time: 0.2919  data: 0.0001  max mem: 9646
[2024-01-21 15:52:04 root] (utils.py 293): INFO Epoch: [0]  [  60/3152]  eta: 0:17:07  lr_architecture: 0.010000  loss_cls: 7.1102 (7.5445)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.5522 (inf)  time: 0.2923  data: 0.0001  max mem: 9646
[2024-01-21 15:52:07 root] (utils.py 293): INFO Epoch: [0]  [  70/3152]  eta: 0:16:46  lr_architecture: 0.010000  loss_cls: 7.0828 (7.4794)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4694 (inf)  time: 0.2929  data: 0.0001  max mem: 9646
[2024-01-21 15:52:11 root] (utils.py 293): INFO Epoch: [0]  [  80/3152]  eta: 0:17:26  lr_architecture: 0.010000  loss_cls: 7.0733 (7.4274)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4471 (inf)  time: 0.3671  data: 0.0001  max mem: 9647
[2024-01-21 15:52:14 root] (utils.py 293): INFO Epoch: [0]  [  90/3152]  eta: 0:17:07  lr_architecture: 0.010000  loss_cls: 7.0372 (7.3867)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4074 (inf)  time: 0.3668  data: 0.0001  max mem: 9647
[2024-01-21 15:52:17 root] (utils.py 293): INFO Epoch: [0]  [ 100/3152]  eta: 0:16:50  lr_architecture: 0.010000  loss_cls: 7.0360 (7.3523)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3984 (inf)  time: 0.2924  data: 0.0001  max mem: 9647
[2024-01-21 15:52:20 root] (utils.py 293): INFO Epoch: [0]  [ 110/3152]  eta: 0:16:36  lr_architecture: 0.010000  loss_cls: 7.0137 (7.3223)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3984 (inf)  time: 0.2922  data: 0.0001  max mem: 9647
[2024-01-21 15:52:25 root] (utils.py 293): INFO Epoch: [0]  [ 120/3152]  eta: 0:17:13  lr_architecture: 0.010000  loss_cls: 7.0148 (7.3006)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4177 (inf)  time: 0.3902  data: 0.0742  max mem: 9647
[2024-01-21 15:52:28 root] (utils.py 293): INFO Epoch: [0]  [ 130/3152]  eta: 0:17:11  lr_architecture: 0.010000  loss_cls: 7.0302 (7.2791)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4432 (inf)  time: 0.4177  data: 0.0742  max mem: 9647
[2024-01-21 15:52:31 root] (utils.py 293): INFO Epoch: [0]  [ 140/3152]  eta: 0:16:58  lr_architecture: 0.010000  loss_cls: 7.0082 (7.2594)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4064 (inf)  time: 0.3202  data: 0.0001  max mem: 9647
[2024-01-21 15:52:34 root] (utils.py 293): INFO Epoch: [0]  [ 150/3152]  eta: 0:16:45  lr_architecture: 0.010000  loss_cls: 7.0136 (7.2424)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3963 (inf)  time: 0.2931  data: 0.0001  max mem: 9647
[2024-01-21 15:52:37 root] (utils.py 293): INFO Epoch: [0]  [ 160/3152]  eta: 0:16:34  lr_architecture: 0.010000  loss_cls: 7.0208 (7.2293)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4188 (inf)  time: 0.2932  data: 0.0001  max mem: 9647
[2024-01-21 15:52:40 root] (utils.py 293): INFO Epoch: [0]  [ 170/3152]  eta: 0:16:24  lr_architecture: 0.010000  loss_cls: 7.0238 (7.2173)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4107 (inf)  time: 0.2930  data: 0.0001  max mem: 9647
[2024-01-21 15:52:44 root] (utils.py 293): INFO Epoch: [0]  [ 180/3152]  eta: 0:16:23  lr_architecture: 0.010000  loss_cls: 7.0397 (7.2063)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3855 (inf)  time: 0.3181  data: 0.0001  max mem: 9647
[2024-01-21 15:52:48 root] (utils.py 293): INFO Epoch: [0]  [ 190/3152]  eta: 0:16:40  lr_architecture: 0.010000  loss_cls: 6.9915 (7.1953)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3616 (inf)  time: 0.4044  data: 0.0001  max mem: 9647
[2024-01-21 15:52:51 root] (utils.py 293): INFO Epoch: [0]  [ 200/3152]  eta: 0:16:30  lr_architecture: 0.010000  loss_cls: 6.9872 (7.1845)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3629 (inf)  time: 0.3792  data: 0.0001  max mem: 9647
[2024-01-21 15:52:54 root] (utils.py 293): INFO Epoch: [0]  [ 210/3152]  eta: 0:16:21  lr_architecture: 0.010000  loss_cls: 6.9905 (7.1754)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3629 (inf)  time: 0.2930  data: 0.0001  max mem: 9647
[2024-01-21 15:52:59 root] (utils.py 293): INFO Epoch: [0]  [ 220/3152]  eta: 0:16:32  lr_architecture: 0.010000  loss_cls: 6.9908 (7.1670)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3753 (inf)  time: 0.3676  data: 0.0684  max mem: 9647
[2024-01-21 15:53:02 root] (utils.py 293): INFO Epoch: [0]  [ 230/3152]  eta: 0:16:23  lr_architecture: 0.010000  loss_cls: 6.9715 (7.1582)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3753 (inf)  time: 0.3679  data: 0.0684  max mem: 9647
[2024-01-21 15:53:05 root] (utils.py 293): INFO Epoch: [0]  [ 240/3152]  eta: 0:16:14  lr_architecture: 0.010000  loss_cls: 6.9591 (7.1503)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3439 (inf)  time: 0.2935  data: 0.0001  max mem: 9647
[2024-01-21 15:53:08 root] (utils.py 293): INFO Epoch: [0]  [ 250/3152]  eta: 0:16:06  lr_architecture: 0.010000  loss_cls: 6.9609 (7.1435)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3414 (inf)  time: 0.2936  data: 0.0001  max mem: 9647
[2024-01-21 15:53:13 root] (utils.py 293): INFO Epoch: [0]  [ 260/3152]  eta: 0:16:28  lr_architecture: 0.010000  loss_cls: 6.9942 (7.1385)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3664 (inf)  time: 0.4256  data: 0.1282  max mem: 9647
[2024-01-21 15:53:16 root] (utils.py 293): INFO Epoch: [0]  [ 270/3152]  eta: 0:16:19  lr_architecture: 0.010000  loss_cls: 6.9924 (7.1326)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3944 (inf)  time: 0.4257  data: 0.1282  max mem: 9647
[2024-01-21 15:53:19 root] (utils.py 293): INFO Epoch: [0]  [ 280/3152]  eta: 0:16:11  lr_architecture: 0.010000  loss_cls: 6.9889 (7.1282)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3886 (inf)  time: 0.2938  data: 0.0001  max mem: 9647
[2024-01-21 15:53:22 root] (utils.py 293): INFO Epoch: [0]  [ 290/3152]  eta: 0:16:03  lr_architecture: 0.010000  loss_cls: 6.9929 (7.1233)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3795 (inf)  time: 0.2932  data: 0.0001  max mem: 9647
[2024-01-21 15:53:25 root] (utils.py 293): INFO Epoch: [0]  [ 300/3152]  eta: 0:16:01  lr_architecture: 0.010000  loss_cls: 6.9738 (7.1185)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3717 (inf)  time: 0.3189  data: 0.0260  max mem: 9647
[2024-01-21 15:53:29 root] (utils.py 293): INFO Epoch: [0]  [ 310/3152]  eta: 0:15:59  lr_architecture: 0.010000  loss_cls: 6.9729 (7.1136)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3426 (inf)  time: 0.3498  data: 0.0548  max mem: 9647
[2024-01-21 15:53:52 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 15:53:56 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:53:57 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:55:48 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:55:48 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:56:06 root] (main_tome.py 217): INFO Namespace(batch_size=64, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 15:56:15 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:56:17 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:56:29 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:56:29 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:56:49 root] (main_tome.py 217): INFO Namespace(batch_size=32, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 15:56:53 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:56:54 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:57:05 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:57:05 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:57:10 root] (utils.py 293): INFO Epoch: [0]  [   0/9852]  eta: 13:27:09  lr_architecture: 0.010000  loss_cls: 3.4989 (3.4989)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 11.2909 (11.2909)  time: 4.9157  data: 0.0006  max mem: 15215
[2024-01-21 15:57:26 root] (main_tome.py 217): INFO Namespace(batch_size=24, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 15:57:30 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:57:31 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:57:41 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:57:41 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:57:46 root] (utils.py 293): INFO Epoch: [0]  [    0/13136]  eta: 17:22:57  lr_architecture: 0.010000  loss_cls: 3.5531 (3.5531)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 15.4189 (15.4189)  time: 4.7638  data: 0.0008  max mem: 14771
[2024-01-21 15:57:56 root] (utils.py 293): INFO Epoch: [0]  [   10/13136]  eta: 4:58:37  lr_architecture: 0.010000  loss_cls: 7.8793 (7.6530)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 2.2490 (3.4921)  time: 1.3650  data: 0.0002  max mem: 20364
[2024-01-21 15:58:06 root] (utils.py 293): INFO Epoch: [0]  [   20/13136]  eta: 4:20:00  lr_architecture: 0.010000  loss_cls: 8.0609 (7.8491)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.7437 (2.6253)  time: 1.0107  data: 0.0001  max mem: 20364
[2024-01-21 15:58:16 root] (utils.py 293): INFO Epoch: [0]  [   30/13136]  eta: 4:06:12  lr_architecture: 0.010000  loss_cls: 7.7930 (7.7937)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.5196 (2.2081)  time: 0.9964  data: 0.0001  max mem: 20364
[2024-01-21 15:58:26 root] (utils.py 293): INFO Epoch: [0]  [   40/13136]  eta: 3:59:02  lr_architecture: 0.010000  loss_cls: 7.5193 (7.6977)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.2482 (1.9753)  time: 0.9962  data: 0.0001  max mem: 20364
[2024-01-21 15:58:36 root] (utils.py 293): INFO Epoch: [0]  [   50/13136]  eta: 3:54:31  lr_architecture: 0.010000  loss_cls: 7.2656 (7.5924)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1611 (1.8107)  time: 0.9949  data: 0.0001  max mem: 20364
[2024-01-21 15:58:46 root] (utils.py 293): INFO Epoch: [0]  [   60/13136]  eta: 3:51:26  lr_architecture: 0.010000  loss_cls: 7.1751 (7.5250)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1580 (1.7199)  time: 0.9939  data: 0.0001  max mem: 20364
[2024-01-21 15:58:56 root] (utils.py 293): INFO Epoch: [0]  [   70/13136]  eta: 3:49:10  lr_architecture: 0.010000  loss_cls: 7.1347 (7.4654)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1705 (1.6395)  time: 0.9939  data: 0.0001  max mem: 20364
[2024-01-21 15:59:06 root] (utils.py 293): INFO Epoch: [0]  [   80/13136]  eta: 3:47:26  lr_architecture: 0.010000  loss_cls: 7.0988 (7.4171)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1332 (1.5799)  time: 0.9942  data: 0.0001  max mem: 20364
[2024-01-21 15:59:16 root] (utils.py 293): INFO Epoch: [0]  [   90/13136]  eta: 3:46:04  lr_architecture: 0.010000  loss_cls: 7.0988 (7.3816)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1230 (1.5309)  time: 0.9948  data: 0.0001  max mem: 20364
[2024-01-21 15:59:26 root] (utils.py 293): INFO Epoch: [0]  [  100/13136]  eta: 3:44:54  lr_architecture: 0.010000  loss_cls: 7.1115 (7.3548)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1081 (1.4941)  time: 0.9944  data: 0.0001  max mem: 20364
[2024-01-21 15:59:36 root] (utils.py 293): INFO Epoch: [0]  [  110/13136]  eta: 3:43:56  lr_architecture: 0.010000  loss_cls: 7.0972 (7.3275)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1869 (1.4695)  time: 0.9942  data: 0.0001  max mem: 20364
[2024-01-21 16:00:00 root] (main_pitome.py 217): INFO Namespace(batch_size=24, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:14:25 root] (main_pitome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:14:26 root] (main_pitome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:14:38 root] (main_pitome.py 389): INFO number of params: 632045800
[2024-01-21 16:14:38 root] (main_pitome.py 435): INFO Start training for 2 epochs
[2024-01-21 16:14:43 root] (utils.py 293): INFO Epoch: [0]  [    0/13136]  eta: 16:55:46  lr_architecture: 0.010000  loss_cls: 3.4210 (3.4210)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 15.5667 (15.5667)  time: 4.6396  data: 0.0009  max mem: 14768
[2024-01-21 16:14:53 root] (utils.py 293): INFO Epoch: [0]  [   10/13136]  eta: 5:01:06  lr_architecture: 0.010000  loss_cls: 8.0990 (7.6581)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 2.0428 (5.1708)  time: 1.3764  data: 0.0001  max mem: 19643
[2024-01-21 16:15:03 root] (utils.py 293): INFO Epoch: [0]  [   20/13136]  eta: 4:24:50  lr_architecture: 0.010000  loss_cls: 7.8175 (7.6678)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.6930 (3.4404)  time: 1.0402  data: 0.0001  max mem: 19643
[2024-01-21 16:15:14 root] (utils.py 293): INFO Epoch: [0]  [   30/13136]  eta: 4:11:48  lr_architecture: 0.010000  loss_cls: 7.5574 (7.6472)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.3881 (2.7646)  time: 1.0298  data: 0.0001  max mem: 19643
[2024-01-21 16:15:24 root] (utils.py 293): INFO Epoch: [0]  [   40/13136]  eta: 4:04:53  lr_architecture: 0.010000  loss_cls: 7.4172 (7.5646)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.2956 (2.3996)  time: 1.0279  data: 0.0001  max mem: 19643
[2024-01-21 16:15:34 root] (utils.py 293): INFO Epoch: [0]  [   50/13136]  eta: 4:00:34  lr_architecture: 0.010000  loss_cls: 7.2202 (7.4824)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.0988 (2.1438)  time: 1.0260  data: 0.0001  max mem: 19643
[2024-01-21 16:16:03 root] (main_pitome.py 218): INFO Namespace(batch_size=20, epochs=2, ratio=0.9625, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:16:07 root] (main_pitome.py 263): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:16:09 root] (main_pitome.py 303): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:16:19 root] (main_pitome.py 390): INFO number of params: 632045800
[2024-01-21 16:16:28 root] (main_pitome.py 444): INFO Start training for 2 epochs
[2024-01-21 16:16:32 root] (utils.py 293): INFO Epoch: [0]  [    0/15763]  eta: 17:35:13  lr_architecture: 0.010000  loss_cls: 3.3981 (3.3981)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 14.2260 (14.2260)  time: 4.0166  data: 0.0006  max mem: 14771
[2024-01-21 16:16:43 root] (utils.py 293): INFO Epoch: [0]  [   10/15763]  eta: 5:45:53  lr_architecture: 0.010000  loss_cls: 7.6641 (7.5778)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 2.0620 (4.1439)  time: 1.3174  data: 0.0001  max mem: 19644
[2024-01-21 16:16:53 root] (utils.py 293): INFO Epoch: [0]  [   20/15763]  eta: 5:08:59  lr_architecture: 0.010000  loss_cls: 7.8419 (7.8659)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.9112 (3.1351)  time: 1.0357  data: 0.0001  max mem: 19644
[2024-01-21 16:17:03 root] (utils.py 293): INFO Epoch: [0]  [   30/15763]  eta: 4:55:43  lr_architecture: 0.010000  loss_cls: 7.7946 (7.7971)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.7437 (2.6213)  time: 1.0235  data: 0.0001  max mem: 19644
[2024-01-21 16:17:14 root] (utils.py 293): INFO Epoch: [0]  [   40/15763]  eta: 4:48:40  lr_architecture: 0.010000  loss_cls: 7.4591 (7.6813)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.3810 (2.2934)  time: 1.0217  data: 0.0001  max mem: 19644
[2024-01-21 16:17:24 root] (utils.py 293): INFO Epoch: [0]  [   50/15763]  eta: 4:44:22  lr_architecture: 0.010000  loss_cls: 7.2144 (7.5850)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1941 (2.0709)  time: 1.0209  data: 0.0001  max mem: 19644
[2024-01-21 16:17:34 root] (utils.py 293): INFO Epoch: [0]  [   60/15763]  eta: 4:41:26  lr_architecture: 0.010000  loss_cls: 7.1630 (7.5106)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1506 (1.9253)  time: 1.0217  data: 0.0001  max mem: 19644
[2024-01-21 16:17:44 root] (utils.py 293): INFO Epoch: [0]  [   70/15763]  eta: 4:39:20  lr_architecture: 0.010000  loss_cls: 7.1388 (7.4639)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1889 (1.8310)  time: 1.0225  data: 0.0001  max mem: 19644
[2024-01-21 16:17:54 root] (utils.py 293): INFO Epoch: [0]  [   80/15763]  eta: 4:37:39  lr_architecture: 0.010000  loss_cls: 7.1015 (7.4127)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2203 (1.7634)  time: 1.0223  data: 0.0001  max mem: 19644
[2024-01-21 16:18:05 root] (utils.py 293): INFO Epoch: [0]  [   90/15763]  eta: 4:36:17  lr_architecture: 0.010000  loss_cls: 7.0872 (7.3818)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2299 (1.7114)  time: 1.0211  data: 0.0001  max mem: 19644
[2024-01-21 16:18:15 root] (utils.py 293): INFO Epoch: [0]  [  100/15763]  eta: 4:35:10  lr_architecture: 0.010000  loss_cls: 7.0872 (7.3507)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2299 (1.6690)  time: 1.0211  data: 0.0001  max mem: 19644
[2024-01-21 16:18:25 root] (utils.py 293): INFO Epoch: [0]  [  110/15763]  eta: 4:34:14  lr_architecture: 0.010000  loss_cls: 7.0211 (7.3291)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2668 (1.6363)  time: 1.0215  data: 0.0001  max mem: 19644
[2024-01-21 16:18:59 root] (main_pitome.py 218): INFO Namespace(batch_size=20, epochs=2, ratio=0.9625, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:19:03 root] (main_pitome.py 263): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:19:04 root] (main_pitome.py 303): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:19:15 root] (main_pitome.py 390): INFO number of params: 632045800
[2024-01-21 16:19:15 root] (main_pitome.py 445): INFO Start training for 2 epochs
[2024-01-21 16:19:25 root] (utils.py 293): INFO Epoch: [0]  [    0/15763]  eta: 1 day, 19:14:54  lr_architecture: 0.010000  loss_cls: 3.3360 (3.3360)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 14.0616 (14.0616)  time: 9.8772  data: 0.0009  max mem: 14768
[2024-01-21 16:19:35 root] (utils.py 293): INFO Epoch: [0]  [   10/15763]  eta: 8:07:25  lr_architecture: 0.010000  loss_cls: 7.7346 (7.6372)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 2.0485 (3.9325)  time: 1.8565  data: 0.0002  max mem: 19639
[2024-01-21 16:19:46 root] (utils.py 293): INFO Epoch: [0]  [   20/15763]  eta: 6:24:12  lr_architecture: 0.010000  loss_cls: 7.7346 (7.6720)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.8645 (2.8990)  time: 1.0436  data: 0.0001  max mem: 19639
[2024-01-21 16:19:56 root] (utils.py 293): INFO Epoch: [0]  [   30/15763]  eta: 5:47:29  lr_architecture: 0.010000  loss_cls: 7.6303 (7.6643)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.5786 (2.4501)  time: 1.0330  data: 0.0001  max mem: 19639
[2024-01-21 16:20:06 root] (utils.py 293): INFO Epoch: [0]  [   40/15763]  eta: 5:28:23  lr_architecture: 0.010000  loss_cls: 7.5312 (7.6167)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.4175 (2.1774)  time: 1.0315  data: 0.0001  max mem: 19639
[2024-01-21 16:20:17 root] (utils.py 293): INFO Epoch: [0]  [   50/15763]  eta: 5:16:40  lr_architecture: 0.010000  loss_cls: 7.2906 (7.5366)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2673 (1.9831)  time: 1.0295  data: 0.0001  max mem: 19639
[2024-01-21 16:20:27 root] (utils.py 293): INFO Epoch: [0]  [   60/15763]  eta: 5:08:42  lr_architecture: 0.010000  loss_cls: 7.2106 (7.4670)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1838 (1.8522)  time: 1.0287  data: 0.0001  max mem: 19639
[2024-01-21 16:20:37 root] (utils.py 293): INFO Epoch: [0]  [   70/15763]  eta: 5:02:59  lr_architecture: 0.010000  loss_cls: 7.1425 (7.4293)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2471 (1.7765)  time: 1.0289  data: 0.0001  max mem: 19639
[2024-01-21 16:20:47 root] (utils.py 293): INFO Epoch: [0]  [   80/15763]  eta: 4:58:32  lr_architecture: 0.010000  loss_cls: 7.0960 (7.3844)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.3396 (1.7197)  time: 1.0280  data: 0.0001  max mem: 19639
[2024-01-21 16:22:23 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.9625, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:22:26 root] (main_pitome.py 263): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:22:28 root] (main_pitome.py 303): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:22:38 root] (main_pitome.py 390): INFO number of params: 632045800
[2024-01-21 16:22:38 root] (main_pitome.py 445): INFO Start training for 2 epochs
[2024-01-21 16:22:48 root] (utils.py 293): INFO Epoch: [0]  [    0/19704]  eta: 2 days, 6:18:14  lr_architecture: 0.001000  loss_cls: 3.0876 (3.0876)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 14.7789 (14.7789)  time: 9.9216  data: 0.0009  max mem: 14758
[2024-01-21 16:22:58 root] (utils.py 293): INFO Epoch: [0]  [   10/19704]  eta: 10:01:19  lr_architecture: 0.001000  loss_cls: 7.2725 (6.9549)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 2.3027 (4.6950)  time: 1.8320  data: 0.0001  max mem: 18411
[2024-01-21 16:23:08 root] (utils.py 293): INFO Epoch: [0]  [   20/19704]  eta: 7:51:48  lr_architecture: 0.001000  loss_cls: 7.1547 (7.0353)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.6159 (3.1720)  time: 1.0140  data: 0.0001  max mem: 18411
[2024-01-21 16:23:19 root] (utils.py 293): INFO Epoch: [0]  [   30/19704]  eta: 7:05:39  lr_architecture: 0.001000  loss_cls: 7.1436 (7.0573)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.3119 (2.5580)  time: 1.0045  data: 0.0001  max mem: 18411
[2024-01-21 16:23:29 root] (utils.py 293): INFO Epoch: [0]  [   40/19704]  eta: 6:41:59  lr_architecture: 0.001000  loss_cls: 7.1344 (7.0899)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2424 (2.2288)  time: 1.0044  data: 0.0001  max mem: 18411
[2024-01-21 16:23:39 root] (utils.py 293): INFO Epoch: [0]  [   50/19704]  eta: 6:27:27  lr_architecture: 0.001000  loss_cls: 7.1234 (7.0844)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1594 (2.0134)  time: 1.0041  data: 0.0001  max mem: 18411
[2024-01-21 16:23:49 root] (utils.py 293): INFO Epoch: [0]  [   60/19704]  eta: 6:17:42  lr_architecture: 0.001000  loss_cls: 7.0514 (7.0760)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0767 (1.8573)  time: 1.0042  data: 0.0001  max mem: 18411
[2024-01-21 16:23:59 root] (utils.py 293): INFO Epoch: [0]  [   70/19704]  eta: 6:10:41  lr_architecture: 0.001000  loss_cls: 7.0214 (7.0710)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0410 (1.7443)  time: 1.0052  data: 0.0001  max mem: 18411
[2024-01-21 16:24:09 root] (utils.py 293): INFO Epoch: [0]  [   80/19704]  eta: 6:05:20  lr_architecture: 0.001000  loss_cls: 7.0214 (7.0613)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0112 (1.6537)  time: 1.0052  data: 0.0001  max mem: 18411
[2024-01-21 16:24:19 root] (utils.py 293): INFO Epoch: [0]  [   90/19704]  eta: 6:01:06  lr_architecture: 0.001000  loss_cls: 7.0343 (7.0609)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0026 (1.5835)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:24:29 root] (utils.py 293): INFO Epoch: [0]  [  100/19704]  eta: 5:57:41  lr_architecture: 0.001000  loss_cls: 7.0343 (7.0536)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9914 (1.5245)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:24:39 root] (utils.py 293): INFO Epoch: [0]  [  110/19704]  eta: 5:54:51  lr_architecture: 0.001000  loss_cls: 6.9907 (7.0502)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9908 (1.4786)  time: 1.0046  data: 0.0001  max mem: 18411
[2024-01-21 16:24:49 root] (utils.py 293): INFO Epoch: [0]  [  120/19704]  eta: 5:52:28  lr_architecture: 0.001000  loss_cls: 7.0255 (7.0446)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9929 (1.4415)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:24:59 root] (utils.py 293): INFO Epoch: [0]  [  130/19704]  eta: 5:50:24  lr_architecture: 0.001000  loss_cls: 7.0374 (7.0454)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0061 (1.4093)  time: 1.0045  data: 0.0001  max mem: 18411
[2024-01-21 16:25:09 root] (utils.py 293): INFO Epoch: [0]  [  140/19704]  eta: 5:48:38  lr_architecture: 0.001000  loss_cls: 7.0209 (7.0406)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9721 (1.3761)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:25:19 root] (utils.py 293): INFO Epoch: [0]  [  150/19704]  eta: 5:47:04  lr_architecture: 0.001000  loss_cls: 6.9743 (7.0383)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9455 (1.3494)  time: 1.0053  data: 0.0001  max mem: 18411
[2024-01-21 16:25:29 root] (utils.py 293): INFO Epoch: [0]  [  160/19704]  eta: 5:45:42  lr_architecture: 0.001000  loss_cls: 6.9694 (7.0341)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9480 (1.3244)  time: 1.0055  data: 0.0001  max mem: 18411
[2024-01-21 16:25:39 root] (utils.py 293): INFO Epoch: [0]  [  170/19704]  eta: 5:44:28  lr_architecture: 0.001000  loss_cls: 6.9533 (7.0317)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9368 (1.3022)  time: 1.0061  data: 0.0001  max mem: 18411
[2024-01-21 16:25:49 root] (utils.py 293): INFO Epoch: [0]  [  180/19704]  eta: 5:43:21  lr_architecture: 0.001000  loss_cls: 7.0146 (7.0300)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9154 (1.2807)  time: 1.0062  data: 0.0001  max mem: 18411
[2024-01-21 16:25:59 root] (utils.py 293): INFO Epoch: [0]  [  190/19704]  eta: 5:42:21  lr_architecture: 0.001000  loss_cls: 6.9916 (7.0272)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9069 (1.2611)  time: 1.0062  data: 0.0001  max mem: 18411
[2024-01-21 16:26:09 root] (utils.py 293): INFO Epoch: [0]  [  200/19704]  eta: 5:41:26  lr_architecture: 0.001000  loss_cls: 6.9768 (7.0249)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9136 (1.2445)  time: 1.0065  data: 0.0001  max mem: 18411
[2024-01-21 16:26:19 root] (utils.py 293): INFO Epoch: [0]  [  210/19704]  eta: 5:40:34  lr_architecture: 0.001000  loss_cls: 6.9720 (7.0233)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9213 (1.2297)  time: 1.0063  data: 0.0001  max mem: 18411
[2024-01-21 16:26:30 root] (utils.py 293): INFO Epoch: [0]  [  220/19704]  eta: 5:39:47  lr_architecture: 0.001000  loss_cls: 6.9686 (7.0220)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9386 (1.2189)  time: 1.0062  data: 0.0001  max mem: 18411
[2024-01-21 16:26:40 root] (utils.py 293): INFO Epoch: [0]  [  230/19704]  eta: 5:39:02  lr_architecture: 0.001000  loss_cls: 6.9972 (7.0199)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9483 (1.2068)  time: 1.0059  data: 0.0001  max mem: 18411
[2024-01-21 16:26:50 root] (utils.py 293): INFO Epoch: [0]  [  240/19704]  eta: 5:38:20  lr_architecture: 0.001000  loss_cls: 6.9683 (7.0184)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9445 (1.1969)  time: 1.0055  data: 0.0001  max mem: 18411
[2024-01-21 16:27:00 root] (utils.py 293): INFO Epoch: [0]  [  250/19704]  eta: 5:37:41  lr_architecture: 0.001000  loss_cls: 6.9707 (7.0177)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9109 (1.1859)  time: 1.0057  data: 0.0001  max mem: 18411
[2024-01-21 16:28:14 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:28:46 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:29:49 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:30:45 root] (main_pitome.py 219): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:30:48 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:30:49 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:30:55 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:30:55 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:37:11 root] (main_pitome.py 219): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:37:14 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:37:15 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:37:21 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:37:21 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:39:33 root] (main_pitome.py 219): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:39:36 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:39:37 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:39:43 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:39:43 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:39:53 root] (utils.py 302): INFO Epoch: [0]  [    0/19704]  eta: 2 days, 6:56:10  lr_architecture: 0.001000  loss_cls: 3.2183 (3.2183)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 14.5901 (14.5901)  time: 10.0371  data: 0.0009  max mem: 6996
[2024-01-21 16:39:58 root] (utils.py 302): INFO Epoch: [0]  [   10/19704]  eta: 7:29:17  lr_architecture: 0.001000  loss_cls: 7.2594 (6.9707)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 2.4817 (5.9157)  time: 1.3688  data: 0.0001  max mem: 8651
[2024-01-21 16:40:03 root] (utils.py 302): INFO Epoch: [0]  [   20/19704]  eta: 5:11:40  lr_architecture: 0.001000  loss_cls: 7.2090 (7.0522)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.6502 (3.8500)  time: 0.4957  data: 0.0001  max mem: 8651
[2024-01-21 16:40:08 root] (utils.py 302): INFO Epoch: [0]  [   30/19704]  eta: 4:22:53  lr_architecture: 0.001000  loss_cls: 7.0971 (7.0752)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.4636 (3.0519)  time: 0.4899  data: 0.0001  max mem: 8651
[2024-01-21 16:40:13 root] (utils.py 302): INFO Epoch: [0]  [   40/19704]  eta: 3:57:55  lr_architecture: 0.001000  loss_cls: 7.1514 (7.0943)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.3467 (2.6464)  time: 0.4907  data: 0.0001  max mem: 8652
[2024-01-21 16:40:18 root] (utils.py 302): INFO Epoch: [0]  [   50/19704]  eta: 3:42:39  lr_architecture: 0.001000  loss_cls: 7.1237 (7.0949)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.1890 (2.3518)  time: 0.4906  data: 0.0001  max mem: 8652
[2024-01-21 16:40:23 root] (utils.py 302): INFO Epoch: [0]  [   60/19704]  eta: 3:32:24  lr_architecture: 0.001000  loss_cls: 7.0675 (7.0816)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0877 (2.1393)  time: 0.4905  data: 0.0001  max mem: 8652
[2024-01-21 16:40:28 root] (utils.py 302): INFO Epoch: [0]  [   70/19704]  eta: 3:24:58  lr_architecture: 0.001000  loss_cls: 7.0282 (7.0780)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0673 (1.9903)  time: 0.4904  data: 0.0001  max mem: 8652
[2024-01-21 16:40:33 root] (utils.py 302): INFO Epoch: [0]  [   80/19704]  eta: 3:19:24  lr_architecture: 0.001000  loss_cls: 7.0196 (7.0701)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0308 (1.8719)  time: 0.4905  data: 0.0001  max mem: 8652
[2024-01-21 16:40:37 root] (utils.py 302): INFO Epoch: [0]  [   90/19704]  eta: 3:15:01  lr_architecture: 0.001000  loss_cls: 7.0410 (7.0658)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0060 (1.7741)  time: 0.4907  data: 0.0001  max mem: 8652
[2024-01-21 16:40:42 root] (utils.py 302): INFO Epoch: [0]  [  100/19704]  eta: 3:11:29  lr_architecture: 0.001000  loss_cls: 7.0410 (7.0576)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 0.9568 (1.6908)  time: 0.4904  data: 0.0001  max mem: 8652
[2024-01-21 16:40:47 root] (utils.py 302): INFO Epoch: [0]  [  110/19704]  eta: 3:08:35  lr_architecture: 0.001000  loss_cls: 6.9918 (7.0527)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 0.9513 (1.6245)  time: 0.4907  data: 0.0001  max mem: 8652
[2024-01-21 16:40:52 root] (utils.py 302): INFO Epoch: [0]  [  120/19704]  eta: 3:06:08  lr_architecture: 0.001000  loss_cls: 6.9494 (7.0436)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 0.9818 (1.5726)  time: 0.4906  data: 0.0001  max mem: 8652
[2024-01-21 16:41:33 root] (main_pitome.py 219): INFO Namespace(batch_size=32, epochs=2, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:41:36 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:41:37 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:41:43 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:41:43 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:41:53 root] (utils.py 302): INFO Epoch: [0]  [   0/9852]  eta: 1 day, 2:42:21  lr_architecture: 0.010000  loss_cls: 3.3826 (3.3826)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 12.0292 (12.0292)  time: 9.7586  data: 0.0009  max mem: 7078
[2024-01-21 16:41:58 root] (utils.py 302): INFO Epoch: [0]  [  10/9852]  eta: 3:46:18  lr_architecture: 0.010000  loss_cls: 7.8247 (7.4934)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.8356 (3.0640)  time: 1.3797  data: 0.0001  max mem: 10516
[2024-01-21 16:42:03 root] (utils.py 302): INFO Epoch: [0]  [  20/9852]  eta: 2:39:35  lr_architecture: 0.010000  loss_cls: 7.7729 (7.6684)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.5667 (2.2947)  time: 0.5347  data: 0.0001  max mem: 10516
[2024-01-21 16:42:09 root] (utils.py 302): INFO Epoch: [0]  [  30/9852]  eta: 2:15:49  lr_architecture: 0.010000  loss_cls: 7.6246 (7.5799)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.1799 (1.8932)  time: 0.5273  data: 0.0001  max mem: 10516
[2024-01-21 16:42:14 root] (utils.py 302): INFO Epoch: [0]  [  40/9852]  eta: 2:03:36  lr_architecture: 0.010000  loss_cls: 7.1909 (7.4735)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.9142 (1.6448)  time: 0.5270  data: 0.0001  max mem: 10516
[2024-01-21 16:42:19 root] (utils.py 302): INFO Epoch: [0]  [  50/9852]  eta: 1:56:07  lr_architecture: 0.010000  loss_cls: 7.1179 (7.3981)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8550 (1.4894)  time: 0.5265  data: 0.0001  max mem: 10516
[2024-01-21 16:42:24 root] (utils.py 302): INFO Epoch: [0]  [  60/9852]  eta: 1:51:04  lr_architecture: 0.010000  loss_cls: 7.0820 (7.3397)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8443 (1.3881)  time: 0.5264  data: 0.0001  max mem: 10516
[2024-01-21 16:42:30 root] (utils.py 302): INFO Epoch: [0]  [  70/9852]  eta: 1:47:24  lr_architecture: 0.010000  loss_cls: 7.0761 (7.2997)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8859 (1.3188)  time: 0.5263  data: 0.0001  max mem: 10516
[2024-01-21 16:42:35 root] (utils.py 302): INFO Epoch: [0]  [  80/9852]  eta: 1:44:37  lr_architecture: 0.010000  loss_cls: 7.0761 (7.2734)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8728 (1.2655)  time: 0.5258  data: 0.0001  max mem: 10516
[2024-01-21 16:42:40 root] (utils.py 302): INFO Epoch: [0]  [  90/9852]  eta: 1:42:25  lr_architecture: 0.010000  loss_cls: 7.0447 (7.2460)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8262 (1.2171)  time: 0.5257  data: 0.0001  max mem: 10516
[2024-01-21 16:42:45 root] (utils.py 302): INFO Epoch: [0]  [ 100/9852]  eta: 1:40:39  lr_architecture: 0.010000  loss_cls: 7.0251 (7.2248)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8447 (1.1850)  time: 0.5257  data: 0.0001  max mem: 10516
[2024-01-21 16:43:23 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=3, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:43:27 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:43:29 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:43:35 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:43:35 root] (main_pitome.py 446): INFO Start training for 3 epochs
[2024-01-21 16:43:44 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:57:02  lr_architecture: 0.010000  loss_cls: 3.2787 (3.2787)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 9.5476 (9.5476)  time: 9.4645  data: 0.0008  max mem: 11145
[2024-01-21 16:43:50 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:57:27  lr_architecture: 0.010000  loss_cls: 7.7467 (7.3661)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.2086 (2.0786)  time: 1.4336  data: 0.0001  max mem: 14587
[2024-01-21 16:43:57 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:12  lr_architecture: 0.010000  loss_cls: 7.6531 (7.3716)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.0176 (1.4736)  time: 0.6211  data: 0.0001  max mem: 14587
[2024-01-21 16:44:03 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:13:43  lr_architecture: 0.010000  loss_cls: 7.1483 (7.2774)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6878 (1.1953)  time: 0.6120  data: 0.0001  max mem: 14587
[2024-01-21 16:44:09 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:07:44  lr_architecture: 0.010000  loss_cls: 7.0769 (7.2295)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5932 (1.0519)  time: 0.6112  data: 0.0001  max mem: 14587
[2024-01-21 16:44:15 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:04  lr_architecture: 0.010000  loss_cls: 7.0719 (7.1948)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6034 (0.9680)  time: 0.6099  data: 0.0001  max mem: 14587
[2024-01-21 16:44:21 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:01:33  lr_architecture: 0.010000  loss_cls: 7.0165 (7.1625)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5947 (0.9048)  time: 0.6098  data: 0.0001  max mem: 14587
[2024-01-21 16:44:27 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 0:59:43  lr_architecture: 0.010000  loss_cls: 7.0040 (7.1422)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5919 (0.8626)  time: 0.6096  data: 0.0001  max mem: 14587
[2024-01-21 16:44:33 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:20  lr_architecture: 0.010000  loss_cls: 7.0094 (7.1259)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5755 (0.8258)  time: 0.6103  data: 0.0001  max mem: 14587
[2024-01-21 16:44:39 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:13  lr_architecture: 0.010000  loss_cls: 7.0360 (7.1169)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5978 (0.8059)  time: 0.6100  data: 0.0001  max mem: 14587
[2024-01-21 16:44:45 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:18  lr_architecture: 0.010000  loss_cls: 7.0294 (7.1045)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6228 (0.7881)  time: 0.6103  data: 0.0001  max mem: 14587
[2024-01-21 16:44:51 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:55:32  lr_architecture: 0.010000  loss_cls: 6.9780 (7.0956)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6215 (0.7723)  time: 0.6105  data: 0.0001  max mem: 14587
[2024-01-21 16:44:58 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:54:53  lr_architecture: 0.010000  loss_cls: 7.0013 (7.0879)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5993 (0.7575)  time: 0.6099  data: 0.0001  max mem: 14587
[2024-01-21 16:45:04 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:18  lr_architecture: 0.010000  loss_cls: 7.0008 (7.0803)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5901 (0.7444)  time: 0.6100  data: 0.0001  max mem: 14587
[2024-01-21 16:45:10 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:53:48  lr_architecture: 0.010000  loss_cls: 7.0008 (7.0762)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6096 (0.7388)  time: 0.6093  data: 0.0001  max mem: 14587
[2024-01-21 16:45:16 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:21  lr_architecture: 0.010000  loss_cls: 7.0152 (7.0710)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6405 (0.7313)  time: 0.6098  data: 0.0001  max mem: 14587
[2024-01-21 16:45:22 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:52:56  lr_architecture: 0.010000  loss_cls: 7.0116 (7.0680)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6468 (0.7292)  time: 0.6110  data: 0.0001  max mem: 14587
[2024-01-21 16:45:28 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:52:34  lr_architecture: 0.010000  loss_cls: 6.9947 (7.0631)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6568 (0.7253)  time: 0.6111  data: 0.0001  max mem: 14587
[2024-01-21 16:45:34 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:14  lr_architecture: 0.010000  loss_cls: 6.9903 (7.0598)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6547 (0.7218)  time: 0.6102  data: 0.0001  max mem: 14587
[2024-01-21 16:45:40 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:51:55  lr_architecture: 0.010000  loss_cls: 6.9951 (7.0570)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6482 (0.7182)  time: 0.6100  data: 0.0001  max mem: 14587
[2024-01-21 16:45:46 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:51:37  lr_architecture: 0.010000  loss_cls: 7.0058 (7.0545)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6583 (0.7165)  time: 0.6103  data: 0.0001  max mem: 14587
[2024-01-21 16:45:53 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:20  lr_architecture: 0.010000  loss_cls: 6.9971 (7.0502)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6292 (0.7114)  time: 0.6105  data: 0.0001  max mem: 14587
[2024-01-21 16:45:59 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:05  lr_architecture: 0.010000  loss_cls: 6.9833 (7.0473)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6292 (0.7089)  time: 0.6104  data: 0.0001  max mem: 14587
[2024-01-21 16:46:05 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:50:50  lr_architecture: 0.010000  loss_cls: 6.9833 (7.0444)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6738 (0.7082)  time: 0.6106  data: 0.0001  max mem: 14587
[2024-01-21 16:46:11 root] (utils.py 302): INFO Epoch: [0]  [ 240/4926]  eta: 0:50:36  lr_architecture: 0.010000  loss_cls: 6.9848 (7.0424)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6858 (0.7072)  time: 0.6112  data: 0.0001  max mem: 14587
[2024-01-21 16:46:50 root] (main_pitome.py 219): INFO Namespace(batch_size=128, epochs=5, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:46:54 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:46:55 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:47:03 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:47:03 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:47:13 root] (utils.py 302): INFO Epoch: [0]  [   0/2463]  eta: 6:43:11  lr_architecture: 0.010000  loss_cls: 3.0982 (3.0982)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 5.5643 (5.5643)  time: 9.8218  data: 0.0009  max mem: 19309
[2024-01-21 16:47:41 root] (main_pitome.py 219): INFO Namespace(batch_size=80, epochs=5, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:47:44 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:47:45 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:47:53 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:47:53 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:48:02 root] (utils.py 302): INFO Epoch: [0]  [   0/3940]  eta: 10:18:22  lr_architecture: 0.010000  loss_cls: 3.0289 (3.0289)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 7.8372 (7.8372)  time: 9.4170  data: 0.0009  max mem: 13172
[2024-01-21 16:48:09 root] (utils.py 302): INFO Epoch: [0]  [  10/3940]  eta: 1:37:09  lr_architecture: 0.010000  loss_cls: 7.5547 (7.2544)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.1815 (1.9207)  time: 1.4833  data: 0.0002  max mem: 16611
[2024-01-21 16:48:16 root] (utils.py 302): INFO Epoch: [0]  [  20/3940]  eta: 1:11:24  lr_architecture: 0.010000  loss_cls: 7.4872 (7.3306)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.9072 (1.3803)  time: 0.6769  data: 0.0001  max mem: 16611
[2024-01-21 16:48:22 root] (utils.py 302): INFO Epoch: [0]  [  30/3940]  eta: 1:02:13  lr_architecture: 0.010000  loss_cls: 7.1890 (7.2612)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6089 (1.1205)  time: 0.6642  data: 0.0001  max mem: 16611
[2024-01-21 16:48:29 root] (utils.py 302): INFO Epoch: [0]  [  40/3940]  eta: 0:57:29  lr_architecture: 0.010000  loss_cls: 7.0728 (7.2129)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5684 (0.9822)  time: 0.6655  data: 0.0001  max mem: 16611
[2024-01-21 16:48:35 root] (utils.py 302): INFO Epoch: [0]  [  50/3940]  eta: 0:54:33  lr_architecture: 0.010000  loss_cls: 7.0447 (7.1717)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5125 (0.8878)  time: 0.6656  data: 0.0001  max mem: 16611
[2024-01-21 16:48:42 root] (utils.py 302): INFO Epoch: [0]  [  60/3940]  eta: 0:52:32  lr_architecture: 0.010000  loss_cls: 7.0025 (7.1425)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5004 (0.8275)  time: 0.6646  data: 0.0001  max mem: 16611
[2024-01-21 16:48:49 root] (utils.py 302): INFO Epoch: [0]  [  70/3940]  eta: 0:51:03  lr_architecture: 0.010000  loss_cls: 7.0087 (7.1260)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5141 (0.7836)  time: 0.6650  data: 0.0001  max mem: 16611
[2024-01-21 16:48:55 root] (utils.py 302): INFO Epoch: [0]  [  80/3940]  eta: 0:49:55  lr_architecture: 0.010000  loss_cls: 7.0068 (7.1112)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5245 (0.7501)  time: 0.6652  data: 0.0001  max mem: 16611
[2024-01-21 16:49:02 root] (utils.py 302): INFO Epoch: [0]  [  90/3940]  eta: 0:49:01  lr_architecture: 0.010000  loss_cls: 6.9798 (7.0973)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5218 (0.7254)  time: 0.6657  data: 0.0001  max mem: 16611
[2024-01-21 16:49:09 root] (utils.py 302): INFO Epoch: [0]  [ 100/3940]  eta: 0:48:16  lr_architecture: 0.010000  loss_cls: 6.9786 (7.0866)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5230 (0.7064)  time: 0.6660  data: 0.0001  max mem: 16611
[2024-01-21 16:49:15 root] (utils.py 302): INFO Epoch: [0]  [ 110/3940]  eta: 0:47:38  lr_architecture: 0.010000  loss_cls: 6.9748 (7.0776)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5271 (0.6889)  time: 0.6659  data: 0.0001  max mem: 16611
[2024-01-21 16:49:22 root] (utils.py 302): INFO Epoch: [0]  [ 120/3940]  eta: 0:47:05  lr_architecture: 0.010000  loss_cls: 7.0006 (7.0720)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5311 (0.6777)  time: 0.6658  data: 0.0001  max mem: 16611
[2024-01-21 16:49:29 root] (utils.py 302): INFO Epoch: [0]  [ 130/3940]  eta: 0:46:36  lr_architecture: 0.010000  loss_cls: 6.9939 (7.0645)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5357 (0.6670)  time: 0.6651  data: 0.0001  max mem: 16611
[2024-01-21 16:49:35 root] (utils.py 302): INFO Epoch: [0]  [ 140/3940]  eta: 0:46:10  lr_architecture: 0.010000  loss_cls: 6.9656 (7.0591)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5181 (0.6600)  time: 0.6655  data: 0.0001  max mem: 16611
[2024-01-21 16:49:42 root] (utils.py 302): INFO Epoch: [0]  [ 150/3940]  eta: 0:45:47  lr_architecture: 0.010000  loss_cls: 6.9781 (7.0552)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5398 (0.6537)  time: 0.6664  data: 0.0001  max mem: 16611
[2024-01-21 16:49:49 root] (utils.py 302): INFO Epoch: [0]  [ 160/3940]  eta: 0:45:26  lr_architecture: 0.010000  loss_cls: 6.9718 (7.0508)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5398 (0.6481)  time: 0.6671  data: 0.0001  max mem: 16611
[2024-01-21 16:49:55 root] (utils.py 302): INFO Epoch: [0]  [ 170/3940]  eta: 0:45:07  lr_architecture: 0.010000  loss_cls: 6.9771 (7.0465)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5600 (0.6432)  time: 0.6674  data: 0.0001  max mem: 16611
[2024-01-21 16:50:02 root] (utils.py 302): INFO Epoch: [0]  [ 180/3940]  eta: 0:44:50  lr_architecture: 0.010000  loss_cls: 6.9903 (7.0432)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5694 (0.6411)  time: 0.6674  data: 0.0001  max mem: 16611
[2024-01-21 16:50:09 root] (utils.py 302): INFO Epoch: [0]  [ 190/3940]  eta: 0:44:33  lr_architecture: 0.010000  loss_cls: 6.9985 (7.0406)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5969 (0.6390)  time: 0.6666  data: 0.0001  max mem: 16611
[2024-01-21 16:50:09 root] (engine.py 60): INFO Loss is nan, stopping training
[2024-01-21 16:55:52 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:55:56 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:55:57 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:56:04 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:56:04 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:56:13 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:52:49  lr_architecture: 0.010000  loss_cls: 3.2694 (3.2694)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 8.7332 (8.7332)  time: 9.4132  data: 0.0009  max mem: 11145
[2024-01-21 16:56:20 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:58:22  lr_architecture: 0.010000  loss_cls: 7.6422 (7.3120)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.2290 (2.5742)  time: 1.4448  data: 0.0002  max mem: 14587
[2024-01-21 16:56:26 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:58  lr_architecture: 0.010000  loss_cls: 7.5052 (7.3856)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.0066 (1.7664)  time: 0.6333  data: 0.0001  max mem: 14587
[2024-01-21 16:56:32 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:20  lr_architecture: 0.010000  loss_cls: 7.2735 (7.3056)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.7483 (1.4092)  time: 0.6174  data: 0.0001  max mem: 14587
[2024-01-21 16:56:38 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:18  lr_architecture: 0.010000  loss_cls: 7.0997 (7.2521)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6479 (1.2223)  time: 0.6157  data: 0.0001  max mem: 14587
[2024-01-21 16:56:44 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:35  lr_architecture: 0.010000  loss_cls: 7.0577 (7.2107)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6165 (1.1017)  time: 0.6148  data: 0.0001  max mem: 14587
[2024-01-21 16:56:51 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:04  lr_architecture: 0.010000  loss_cls: 7.0123 (7.1750)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5763 (1.0137)  time: 0.6149  data: 0.0001  max mem: 14587
[2024-01-21 16:56:57 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:15  lr_architecture: 0.010000  loss_cls: 6.9976 (7.1512)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5724 (0.9545)  time: 0.6160  data: 0.0001  max mem: 14587
[2024-01-21 16:57:03 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:51  lr_architecture: 0.010000  loss_cls: 7.0026 (7.1355)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5724 (0.9083)  time: 0.6165  data: 0.0001  max mem: 14587
[2024-01-21 16:57:09 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:44  lr_architecture: 0.010000  loss_cls: 7.0331 (7.1253)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5978 (0.8753)  time: 0.6169  data: 0.0001  max mem: 14587
[2024-01-21 16:57:15 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:50  lr_architecture: 0.010000  loss_cls: 7.0213 (7.1131)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6027 (0.8503)  time: 0.6182  data: 0.0001  max mem: 14587
[2024-01-21 16:57:22 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:56:05  lr_architecture: 0.010000  loss_cls: 6.9739 (7.1021)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5893 (0.8274)  time: 0.6190  data: 0.0001  max mem: 14587
[2024-01-21 16:57:28 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:27  lr_architecture: 0.010000  loss_cls: 6.9848 (7.0943)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6008 (0.8094)  time: 0.6190  data: 0.0001  max mem: 14587
[2024-01-21 16:57:34 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:52  lr_architecture: 0.010000  loss_cls: 7.0019 (7.0853)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6098 (0.7932)  time: 0.6179  data: 0.0001  max mem: 14587
[2024-01-21 16:57:40 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:22  lr_architecture: 0.010000  loss_cls: 7.0019 (7.0807)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6150 (0.7830)  time: 0.6176  data: 0.0001  max mem: 14587
[2024-01-21 16:57:46 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:55  lr_architecture: 0.010000  loss_cls: 7.0102 (7.0756)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6277 (0.7720)  time: 0.6185  data: 0.0001  max mem: 14587
[2024-01-21 16:57:52 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:31  lr_architecture: 0.010000  loss_cls: 7.0050 (7.0716)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6486 (0.7663)  time: 0.6186  data: 0.0001  max mem: 14587
[2024-01-21 16:57:59 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:53:09  lr_architecture: 0.010000  loss_cls: 6.9840 (7.0661)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6558 (0.7601)  time: 0.6183  data: 0.0001  max mem: 14587
[2024-01-21 16:58:05 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:48  lr_architecture: 0.010000  loss_cls: 6.9805 (7.0624)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6606 (0.7542)  time: 0.6177  data: 0.0001  max mem: 14587
[2024-01-21 16:58:11 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:29  lr_architecture: 0.010000  loss_cls: 6.9911 (7.0596)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6469 (0.7486)  time: 0.6183  data: 0.0001  max mem: 14587
[2024-01-21 16:58:17 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:52:12  lr_architecture: 0.010000  loss_cls: 7.0026 (7.0563)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6659 (0.7464)  time: 0.6182  data: 0.0001  max mem: 14587
[2024-01-21 16:58:23 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:55  lr_architecture: 0.010000  loss_cls: 6.9890 (7.0519)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6335 (0.7410)  time: 0.6172  data: 0.0001  max mem: 14587
[2024-01-21 16:58:30 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:39  lr_architecture: 0.010000  loss_cls: 6.9806 (7.0486)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6337 (0.7367)  time: 0.6180  data: 0.0001  max mem: 14587
[2024-01-21 16:58:36 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:51:25  lr_architecture: 0.010000  loss_cls: 6.9790 (7.0455)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6513 (0.7340)  time: 0.6188  data: 0.0001  max mem: 14587
[2024-01-21 16:59:18 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 16:59:21 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:59:23 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:59:29 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:59:29 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:59:39 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:59:59  lr_architecture: 0.010000  loss_cls: 3.2943 (3.2943)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.4229 (9.4229)  time: 9.5005  data: 0.0008  max mem: 10987
[2024-01-21 16:59:45 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:59:06  lr_architecture: 0.010000  loss_cls: 7.8339 (7.3313)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 1.1803 (2.0192)  time: 1.4537  data: 0.0002  max mem: 14435
[2024-01-21 16:59:51 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:26:11  lr_architecture: 0.010000  loss_cls: 7.4570 (7.3351)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.9764 (1.4387)  time: 0.6318  data: 0.0001  max mem: 14435
[2024-01-21 16:59:58 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:24  lr_architecture: 0.010000  loss_cls: 7.1538 (7.2511)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6895 (1.1816)  time: 0.6138  data: 0.0001  max mem: 14435
[2024-01-21 17:00:04 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:20  lr_architecture: 0.010000  loss_cls: 7.0535 (7.2034)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6252 (1.0374)  time: 0.6137  data: 0.0001  max mem: 14435
[2024-01-21 17:00:10 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:36  lr_architecture: 0.010000  loss_cls: 7.0527 (7.1712)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5731 (0.9519)  time: 0.6140  data: 0.0001  max mem: 14435
[2024-01-21 17:00:16 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:03  lr_architecture: 0.010000  loss_cls: 7.0111 (7.1427)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5753 (0.8909)  time: 0.6135  data: 0.0001  max mem: 14435
[2024-01-21 17:00:22 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:13  lr_architecture: 0.010000  loss_cls: 6.9904 (7.1226)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5796 (0.8483)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:00:28 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:48  lr_architecture: 0.010000  loss_cls: 7.0091 (7.1119)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5887 (0.8158)  time: 0.6151  data: 0.0001  max mem: 14435
[2024-01-21 17:00:34 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:40  lr_architecture: 0.010000  loss_cls: 7.0292 (7.1035)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5943 (0.7935)  time: 0.6141  data: 0.0001  max mem: 14435
[2024-01-21 17:00:41 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:45  lr_architecture: 0.010000  loss_cls: 7.0168 (7.0940)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6133 (0.7774)  time: 0.6146  data: 0.0001  max mem: 14435
[2024-01-21 17:00:47 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:55:58  lr_architecture: 0.010000  loss_cls: 7.0017 (7.0851)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6184 (0.7631)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:00:53 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:18  lr_architecture: 0.010000  loss_cls: 7.0138 (7.0795)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6203 (0.7528)  time: 0.6132  data: 0.0001  max mem: 14435
[2024-01-21 17:00:59 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:42  lr_architecture: 0.010000  loss_cls: 7.0022 (7.0714)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5944 (0.7393)  time: 0.6124  data: 0.0001  max mem: 14435
[2024-01-21 17:01:05 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:11  lr_architecture: 0.010000  loss_cls: 7.0022 (7.0673)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5944 (0.7343)  time: 0.6124  data: 0.0001  max mem: 14435
[2024-01-21 17:01:11 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:43  lr_architecture: 0.010000  loss_cls: 7.0113 (7.0632)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6336 (0.7273)  time: 0.6132  data: 0.0001  max mem: 14435
[2024-01-21 17:01:17 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:19  lr_architecture: 0.010000  loss_cls: 7.0096 (7.0597)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6168 (0.7225)  time: 0.6143  data: 0.0001  max mem: 14435
[2024-01-21 17:01:24 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:52:56  lr_architecture: 0.010000  loss_cls: 6.9876 (7.0552)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6369 (0.7186)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:30 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:35  lr_architecture: 0.010000  loss_cls: 6.9796 (7.0514)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6385 (0.7139)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:36 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:16  lr_architecture: 0.010000  loss_cls: 6.9864 (7.0494)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6392 (0.7104)  time: 0.6148  data: 0.0001  max mem: 14435
[2024-01-21 17:01:42 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:51:58  lr_architecture: 0.010000  loss_cls: 6.9849 (7.0465)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6539 (0.7093)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:48 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:42  lr_architecture: 0.010000  loss_cls: 6.9763 (7.0428)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6626 (0.7069)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:54 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:26  lr_architecture: 0.010000  loss_cls: 6.9929 (7.0400)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6700 (0.7048)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:07:01 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 17:07:05 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:07:07 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:07:13 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 17:07:13 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 17:08:29 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 17:08:32 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:08:34 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:08:40 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 17:08:40 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 17:08:50 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:50:53  lr_architecture: 0.010000  loss_cls: 3.2934 (3.2934)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 8.7347 (8.7347)  time: 9.3896  data: 0.0006  max mem: 10987
[2024-01-21 17:08:56 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:57:04  lr_architecture: 0.010000  loss_cls: 7.8637 (7.4586)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 1.2888 (2.1405)  time: 1.4290  data: 0.0001  max mem: 14435
[2024-01-21 17:09:02 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:03  lr_architecture: 0.010000  loss_cls: 7.7180 (7.5139)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 1.0581 (1.5520)  time: 0.6228  data: 0.0001  max mem: 14435
[2024-01-21 17:09:08 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:13:38  lr_architecture: 0.010000  loss_cls: 7.3165 (7.4022)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.7337 (1.2633)  time: 0.6130  data: 0.0001  max mem: 14435
[2024-01-21 17:09:15 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:07:45  lr_architecture: 0.010000  loss_cls: 7.0842 (7.3199)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6310 (1.1065)  time: 0.6136  data: 0.0001  max mem: 14435
[2024-01-21 17:09:21 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:06  lr_architecture: 0.010000  loss_cls: 7.0518 (7.2600)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5784 (0.9989)  time: 0.6128  data: 0.0001  max mem: 14435
[2024-01-21 17:09:27 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:01:39  lr_architecture: 0.010000  loss_cls: 6.9948 (7.2137)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5475 (0.9240)  time: 0.6128  data: 0.0001  max mem: 14435
[2024-01-21 17:09:33 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 0:59:50  lr_architecture: 0.010000  loss_cls: 6.9838 (7.1834)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5477 (0.8724)  time: 0.6133  data: 0.0001  max mem: 14435
[2024-01-21 17:09:39 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:27  lr_architecture: 0.010000  loss_cls: 6.9835 (7.1598)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5538 (0.8330)  time: 0.6126  data: 0.0001  max mem: 14435
[2024-01-21 17:09:45 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:22  lr_architecture: 0.010000  loss_cls: 7.0319 (7.1465)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5538 (0.8083)  time: 0.6138  data: 0.0001  max mem: 14435
[2024-01-21 17:09:51 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:27  lr_architecture: 0.010000  loss_cls: 7.0297 (7.1322)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5798 (0.7882)  time: 0.6134  data: 0.0001  max mem: 14435
[2024-01-21 17:09:54 root] (engine.py 60): INFO Loss is nan, stopping training
[2024-01-21 17:40:55 root] (main_pitome.py 215): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 17:40:59 root] (main_pitome.py 260): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:41:00 root] (main_pitome.py 300): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:41:07 root] (main_pitome.py 386): INFO number of params: 304326632
[2024-01-21 17:41:07 root] (main_pitome.py 441): INFO Start training for 5 epochs
[2024-01-21 17:41:16 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:59:18  lr_architecture: 0.000250  loss_cls: 3.2848 (3.2848)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.1179 (9.1179)  time: 9.4922  data: 0.0007  max mem: 10987
[2024-01-21 17:41:23 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:58:34  lr_architecture: 0.000250  loss_cls: 3.6209 (3.7293)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.0908 (6.7913)  time: 1.4472  data: 0.0002  max mem: 14435
[2024-01-21 17:41:29 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:26:03  lr_architecture: 0.000250  loss_cls: 3.6209 (3.5764)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2937 (6.0860)  time: 0.6305  data: 0.0001  max mem: 14435
[2024-01-21 17:41:35 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:26  lr_architecture: 0.000250  loss_cls: 3.2285 (3.3940)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9420 (5.7617)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:41:41 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:25  lr_architecture: 0.000250  loss_cls: 3.3246 (3.3629)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2213 (5.6275)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:41:48 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:43  lr_architecture: 0.000250  loss_cls: 3.3683 (3.3438)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8586 (5.4696)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:41:54 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:11  lr_architecture: 0.000250  loss_cls: 3.2267 (3.3219)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8267 (5.3690)  time: 0.6162  data: 0.0001  max mem: 14435
[2024-01-21 17:42:00 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:20  lr_architecture: 0.000250  loss_cls: 3.3166 (3.3268)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8266 (5.2887)  time: 0.6163  data: 0.0001  max mem: 14435
[2024-01-21 17:42:06 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:56  lr_architecture: 0.000250  loss_cls: 3.4074 (3.3394)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8266 (5.2455)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:42:12 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:49  lr_architecture: 0.000250  loss_cls: 3.3908 (3.3590)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6682 (5.1779)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:42:18 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:54  lr_architecture: 0.000250  loss_cls: 3.2408 (3.3458)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6899 (5.1399)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:42:25 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:56:08  lr_architecture: 0.000250  loss_cls: 3.4080 (3.3859)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7572 (5.0956)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:42:31 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:29  lr_architecture: 0.000250  loss_cls: 3.5040 (3.3614)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7733 (5.0728)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:42:37 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:55  lr_architecture: 0.000250  loss_cls: 3.2129 (3.3568)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7034 (5.0340)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:42:43 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:24  lr_architecture: 0.000250  loss_cls: 3.3487 (3.3618)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.4624 (4.9954)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:42:49 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:57  lr_architecture: 0.000250  loss_cls: 3.4735 (3.3753)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.4797 (4.9662)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 17:42:55 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:33  lr_architecture: 0.000250  loss_cls: 3.6606 (3.3831)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.4463 (4.9276)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:43:02 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:53:11  lr_architecture: 0.000250  loss_cls: 3.6606 (3.3943)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2543 (4.8925)  time: 0.6191  data: 0.0001  max mem: 14435
[2024-01-21 17:43:08 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:50  lr_architecture: 0.000250  loss_cls: 3.4124 (3.3813)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2543 (4.8659)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:43:14 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:31  lr_architecture: 0.000250  loss_cls: 3.1956 (3.3778)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3014 (4.8381)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:43:20 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:52:13  lr_architecture: 0.000250  loss_cls: 3.1837 (3.3728)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3791 (4.8190)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:43:26 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:57  lr_architecture: 0.000250  loss_cls: 3.0657 (3.3671)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3336 (4.7891)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:43:33 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:41  lr_architecture: 0.000250  loss_cls: 3.3158 (3.3685)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1175 (4.7570)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:43:39 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:51:26  lr_architecture: 0.000250  loss_cls: 3.3961 (3.3763)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0804 (4.7331)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:43:45 root] (utils.py 302): INFO Epoch: [0]  [ 240/4926]  eta: 0:51:12  lr_architecture: 0.000250  loss_cls: 3.6595 (3.3881)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2741 (4.7161)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:43:51 root] (utils.py 302): INFO Epoch: [0]  [ 250/4926]  eta: 0:50:59  lr_architecture: 0.000250  loss_cls: 3.3307 (3.3722)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3100 (4.7060)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:43:57 root] (utils.py 302): INFO Epoch: [0]  [ 260/4926]  eta: 0:50:46  lr_architecture: 0.000250  loss_cls: 2.9614 (3.3693)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2500 (4.6867)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:44:03 root] (utils.py 302): INFO Epoch: [0]  [ 270/4926]  eta: 0:50:33  lr_architecture: 0.000250  loss_cls: 3.6868 (3.3770)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1848 (4.6718)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:44:10 root] (utils.py 302): INFO Epoch: [0]  [ 280/4926]  eta: 0:50:21  lr_architecture: 0.000250  loss_cls: 3.4438 (3.3758)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1996 (4.6519)  time: 0.6191  data: 0.0001  max mem: 14435
[2024-01-21 17:44:16 root] (utils.py 302): INFO Epoch: [0]  [ 290/4926]  eta: 0:50:10  lr_architecture: 0.000250  loss_cls: 3.4438 (3.3814)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1996 (4.6436)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:44:22 root] (utils.py 302): INFO Epoch: [0]  [ 300/4926]  eta: 0:49:58  lr_architecture: 0.000250  loss_cls: 3.6806 (3.3857)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1383 (4.6274)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:44:28 root] (utils.py 302): INFO Epoch: [0]  [ 310/4926]  eta: 0:49:48  lr_architecture: 0.000250  loss_cls: 3.6900 (3.3920)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1383 (4.6173)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:44:34 root] (utils.py 302): INFO Epoch: [0]  [ 320/4926]  eta: 0:49:37  lr_architecture: 0.000250  loss_cls: 3.7108 (3.3992)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3412 (4.6076)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:44:41 root] (utils.py 302): INFO Epoch: [0]  [ 330/4926]  eta: 0:49:27  lr_architecture: 0.000250  loss_cls: 3.7466 (3.4031)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3926 (4.5995)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:44:47 root] (utils.py 302): INFO Epoch: [0]  [ 340/4926]  eta: 0:49:16  lr_architecture: 0.000250  loss_cls: 3.3773 (3.4031)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0816 (4.5827)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:44:53 root] (utils.py 302): INFO Epoch: [0]  [ 350/4926]  eta: 0:49:07  lr_architecture: 0.000250  loss_cls: 3.6037 (3.4137)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0780 (4.5714)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:44:59 root] (utils.py 302): INFO Epoch: [0]  [ 360/4926]  eta: 0:48:57  lr_architecture: 0.000250  loss_cls: 3.4359 (3.4105)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2127 (4.5629)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:45:05 root] (utils.py 302): INFO Epoch: [0]  [ 370/4926]  eta: 0:48:47  lr_architecture: 0.000250  loss_cls: 3.2285 (3.4023)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1301 (4.5478)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:45:12 root] (utils.py 302): INFO Epoch: [0]  [ 380/4926]  eta: 0:48:38  lr_architecture: 0.000250  loss_cls: 3.4582 (3.4041)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0326 (4.5363)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:45:18 root] (utils.py 302): INFO Epoch: [0]  [ 390/4926]  eta: 0:48:29  lr_architecture: 0.000250  loss_cls: 3.5244 (3.4020)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0326 (4.5250)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:45:24 root] (utils.py 302): INFO Epoch: [0]  [ 400/4926]  eta: 0:48:20  lr_architecture: 0.000250  loss_cls: 3.3990 (3.3978)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0574 (4.5138)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:45:30 root] (utils.py 302): INFO Epoch: [0]  [ 410/4926]  eta: 0:48:11  lr_architecture: 0.000250  loss_cls: 3.5053 (3.4040)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1395 (4.5044)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:45:36 root] (utils.py 302): INFO Epoch: [0]  [ 420/4926]  eta: 0:48:02  lr_architecture: 0.000250  loss_cls: 3.6494 (3.4058)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0826 (4.4948)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:45:42 root] (utils.py 302): INFO Epoch: [0]  [ 430/4926]  eta: 0:47:54  lr_architecture: 0.000250  loss_cls: 3.5748 (3.4086)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0192 (4.4848)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:45:49 root] (utils.py 302): INFO Epoch: [0]  [ 440/4926]  eta: 0:47:45  lr_architecture: 0.000250  loss_cls: 3.4986 (3.4100)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9928 (4.4721)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:45:55 root] (utils.py 302): INFO Epoch: [0]  [ 450/4926]  eta: 0:47:37  lr_architecture: 0.000250  loss_cls: 3.4642 (3.4114)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7533 (4.4552)  time: 0.6194  data: 0.0001  max mem: 14435
[2024-01-21 17:46:01 root] (utils.py 302): INFO Epoch: [0]  [ 460/4926]  eta: 0:47:29  lr_architecture: 0.000250  loss_cls: 3.5576 (3.4157)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7370 (4.4458)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:46:07 root] (utils.py 302): INFO Epoch: [0]  [ 470/4926]  eta: 0:47:20  lr_architecture: 0.000250  loss_cls: 3.6391 (3.4203)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9635 (4.4350)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:46:13 root] (utils.py 302): INFO Epoch: [0]  [ 480/4926]  eta: 0:47:12  lr_architecture: 0.000250  loss_cls: 3.6139 (3.4212)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9889 (4.4305)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:46:20 root] (utils.py 302): INFO Epoch: [0]  [ 490/4926]  eta: 0:47:04  lr_architecture: 0.000250  loss_cls: 3.6139 (3.4221)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1521 (4.4282)  time: 0.6195  data: 0.0001  max mem: 14435
[2024-01-21 17:46:26 root] (utils.py 302): INFO Epoch: [0]  [ 500/4926]  eta: 0:46:56  lr_architecture: 0.000250  loss_cls: 3.5766 (3.4201)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0871 (4.4180)  time: 0.6190  data: 0.0001  max mem: 14435
[2024-01-21 17:46:32 root] (utils.py 302): INFO Epoch: [0]  [ 510/4926]  eta: 0:46:48  lr_architecture: 0.000250  loss_cls: 3.5766 (3.4256)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7761 (4.4069)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:46:38 root] (utils.py 302): INFO Epoch: [0]  [ 520/4926]  eta: 0:46:41  lr_architecture: 0.000250  loss_cls: 3.5082 (3.4192)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8723 (4.3979)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:46:44 root] (utils.py 302): INFO Epoch: [0]  [ 530/4926]  eta: 0:46:34  lr_architecture: 0.000250  loss_cls: 3.4529 (3.4209)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8621 (4.3888)  time: 0.6249  data: 0.0001  max mem: 14435
[2024-01-21 17:46:51 root] (utils.py 302): INFO Epoch: [0]  [ 540/4926]  eta: 0:46:26  lr_architecture: 0.000250  loss_cls: 3.7133 (3.4227)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9374 (4.3807)  time: 0.6248  data: 0.0001  max mem: 14435
[2024-01-21 17:46:57 root] (utils.py 302): INFO Epoch: [0]  [ 550/4926]  eta: 0:46:18  lr_architecture: 0.000250  loss_cls: 3.6149 (3.4202)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9464 (4.3732)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:47:03 root] (utils.py 302): INFO Epoch: [0]  [ 560/4926]  eta: 0:46:11  lr_architecture: 0.000250  loss_cls: 3.5642 (3.4220)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9717 (4.3677)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:47:09 root] (utils.py 302): INFO Epoch: [0]  [ 570/4926]  eta: 0:46:03  lr_architecture: 0.000250  loss_cls: 3.5642 (3.4211)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7966 (4.3585)  time: 0.6194  data: 0.0001  max mem: 14435
[2024-01-21 17:47:15 root] (utils.py 302): INFO Epoch: [0]  [ 580/4926]  eta: 0:45:56  lr_architecture: 0.000250  loss_cls: 3.2982 (3.4187)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7333 (4.3502)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:47:22 root] (utils.py 302): INFO Epoch: [0]  [ 590/4926]  eta: 0:45:48  lr_architecture: 0.000250  loss_cls: 3.3545 (3.4218)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8094 (4.3409)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:47:28 root] (utils.py 302): INFO Epoch: [0]  [ 600/4926]  eta: 0:45:42  lr_architecture: 0.000250  loss_cls: 3.3545 (3.4188)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7948 (4.3303)  time: 0.6261  data: 0.0001  max mem: 14435
[2024-01-21 17:47:34 root] (utils.py 302): INFO Epoch: [0]  [ 610/4926]  eta: 0:45:34  lr_architecture: 0.000250  loss_cls: 3.0864 (3.4167)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8225 (4.3231)  time: 0.6255  data: 0.0001  max mem: 14435
[2024-01-21 17:47:40 root] (utils.py 302): INFO Epoch: [0]  [ 620/4926]  eta: 0:45:27  lr_architecture: 0.000250  loss_cls: 3.5004 (3.4207)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8725 (4.3156)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:47:46 root] (utils.py 302): INFO Epoch: [0]  [ 630/4926]  eta: 0:45:20  lr_architecture: 0.000250  loss_cls: 3.7298 (3.4257)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8833 (4.3100)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:47:53 root] (utils.py 302): INFO Epoch: [0]  [ 640/4926]  eta: 0:45:12  lr_architecture: 0.000250  loss_cls: 3.7149 (3.4292)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9091 (4.3019)  time: 0.6191  data: 0.0001  max mem: 14435
[2024-01-21 17:47:59 root] (utils.py 302): INFO Epoch: [0]  [ 650/4926]  eta: 0:45:05  lr_architecture: 0.000250  loss_cls: 3.5120 (3.4259)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7774 (4.2962)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:48:05 root] (utils.py 302): INFO Epoch: [0]  [ 660/4926]  eta: 0:44:58  lr_architecture: 0.000250  loss_cls: 3.7084 (3.4306)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7880 (4.2907)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:48:11 root] (utils.py 302): INFO Epoch: [0]  [ 670/4926]  eta: 0:44:51  lr_architecture: 0.000250  loss_cls: 3.7836 (3.4335)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9316 (4.2878)  time: 0.6190  data: 0.0001  max mem: 14435
[2024-01-21 17:48:17 root] (utils.py 302): INFO Epoch: [0]  [ 680/4926]  eta: 0:44:44  lr_architecture: 0.000250  loss_cls: 3.4807 (3.4302)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8267 (4.2795)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:48:24 root] (utils.py 302): INFO Epoch: [0]  [ 690/4926]  eta: 0:44:36  lr_architecture: 0.000250  loss_cls: 3.2941 (3.4291)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7496 (4.2732)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:48:30 root] (utils.py 302): INFO Epoch: [0]  [ 700/4926]  eta: 0:44:29  lr_architecture: 0.000250  loss_cls: 3.4818 (3.4340)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.6901 (4.2645)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:48:36 root] (utils.py 302): INFO Epoch: [0]  [ 710/4926]  eta: 0:44:22  lr_architecture: 0.000250  loss_cls: 3.5618 (3.4352)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7017 (4.2592)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:48:42 root] (utils.py 302): INFO Epoch: [0]  [ 720/4926]  eta: 0:44:15  lr_architecture: 0.000250  loss_cls: 3.5900 (3.4364)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8216 (4.2531)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:48:48 root] (utils.py 302): INFO Epoch: [0]  [ 730/4926]  eta: 0:44:08  lr_architecture: 0.000250  loss_cls: 3.6445 (3.4370)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7615 (4.2469)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:49:19 root] (main_pitome.py 215): INFO Namespace(batch_size=64, epochs=10, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=3, gpu=3, distributed=True, dist_backend='nccl')
[2024-01-21 17:49:23 root] (main_pitome.py 260): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:49:24 root] (main_pitome.py 300): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:49:31 root] (main_pitome.py 386): INFO number of params: 304326632
[2024-01-21 17:49:31 root] (main_pitome.py 441): INFO Start training for 10 epochs
[2024-01-21 17:49:40 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 13:01:29  lr_architecture: 0.000050  loss_cls: 3.3134 (3.3134)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.9756 (9.9756)  time: 9.5189  data: 0.0009  max mem: 10987
[2024-01-21 17:49:47 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:58:35  lr_architecture: 0.000050  loss_cls: 3.0239 (3.0686)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.9756 (10.1166)  time: 1.4473  data: 0.0002  max mem: 14435
[2024-01-21 17:49:53 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:58  lr_architecture: 0.000050  loss_cls: 3.0064 (2.9722)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 7.8741 (8.4506)  time: 0.6280  data: 0.0001  max mem: 14435
[2024-01-21 17:49:59 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:20  lr_architecture: 0.000050  loss_cls: 2.6449 (2.8380)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.3897 (7.8338)  time: 0.6162  data: 0.0001  max mem: 14435
[2024-01-21 17:50:05 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:14  lr_architecture: 0.000050  loss_cls: 2.7073 (2.8227)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.6886 (inf)  time: 0.6141  data: 0.0001  max mem: 14435
[2024-01-21 17:50:11 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:34  lr_architecture: 0.000050  loss_cls: 2.9320 (2.8182)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.6692 (inf)  time: 0.6142  data: 0.0001  max mem: 14435
[2024-01-21 17:50:17 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:03  lr_architecture: 0.000050  loss_cls: 2.7329 (2.7946)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.5364 (inf)  time: 0.6162  data: 0.0001  max mem: 14435
[2024-01-21 17:50:23 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:14  lr_architecture: 0.000050  loss_cls: 2.7814 (2.8014)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.3946 (inf)  time: 0.6157  data: 0.0001  max mem: 14435
[2024-01-21 17:50:30 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:50  lr_architecture: 0.000050  loss_cls: 2.9200 (2.7999)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.1701 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 17:50:36 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:43  lr_architecture: 0.000050  loss_cls: 2.9040 (2.8145)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.2078 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:50:42 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:49  lr_architecture: 0.000050  loss_cls: 2.7991 (2.7920)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.4734 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:50:48 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:56:03  lr_architecture: 0.000050  loss_cls: 2.9955 (2.8250)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.4934 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:50:54 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:24  lr_architecture: 0.000050  loss_cls: 2.9955 (2.8054)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.4413 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:51:00 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:49  lr_architecture: 0.000050  loss_cls: 2.6893 (2.7989)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.2094 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:51:07 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:19  lr_architecture: 0.000050  loss_cls: 2.9054 (2.8034)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7837 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 17:51:13 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:52  lr_architecture: 0.000050  loss_cls: 2.9874 (2.8173)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8021 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:51:19 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:28  lr_architecture: 0.000050  loss_cls: 3.0452 (2.8206)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8019 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:51:25 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:53:05  lr_architecture: 0.000050  loss_cls: 3.0313 (2.8282)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6494 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 17:51:31 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:45  lr_architecture: 0.000050  loss_cls: 2.8590 (2.8169)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6344 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:51:38 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:26  lr_architecture: 0.000050  loss_cls: 2.7754 (2.8105)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6800 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:51:44 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:52:08  lr_architecture: 0.000050  loss_cls: 2.6481 (2.8042)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6800 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:51:50 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:52  lr_architecture: 0.000050  loss_cls: 2.6481 (2.7953)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6706 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:51:56 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:36  lr_architecture: 0.000050  loss_cls: 2.6657 (2.7975)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7570 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:02 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:51:21  lr_architecture: 0.000050  loss_cls: 2.8002 (2.8048)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7570 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:52:08 root] (utils.py 302): INFO Epoch: [0]  [ 240/4926]  eta: 0:51:07  lr_architecture: 0.000050  loss_cls: 3.0115 (2.8144)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8120 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:15 root] (utils.py 302): INFO Epoch: [0]  [ 250/4926]  eta: 0:50:54  lr_architecture: 0.000050  loss_cls: 2.8464 (2.7980)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8120 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:21 root] (utils.py 302): INFO Epoch: [0]  [ 260/4926]  eta: 0:50:41  lr_architecture: 0.000050  loss_cls: 2.4774 (2.7918)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1731 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:52:27 root] (utils.py 302): INFO Epoch: [0]  [ 270/4926]  eta: 0:50:29  lr_architecture: 0.000050  loss_cls: 3.0218 (2.7959)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3726 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:52:33 root] (utils.py 302): INFO Epoch: [0]  [ 280/4926]  eta: 0:50:17  lr_architecture: 0.000050  loss_cls: 2.8755 (2.7940)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6691 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:52:39 root] (utils.py 302): INFO Epoch: [0]  [ 290/4926]  eta: 0:50:05  lr_architecture: 0.000050  loss_cls: 2.8389 (2.7970)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6690 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:52:45 root] (utils.py 302): INFO Epoch: [0]  [ 300/4926]  eta: 0:49:54  lr_architecture: 0.000050  loss_cls: 2.9473 (2.7983)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6340 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:52:52 root] (utils.py 302): INFO Epoch: [0]  [ 310/4926]  eta: 0:49:43  lr_architecture: 0.000050  loss_cls: 2.9606 (2.8028)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4217 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:58 root] (utils.py 302): INFO Epoch: [0]  [ 320/4926]  eta: 0:49:33  lr_architecture: 0.000050  loss_cls: 3.0230 (2.8064)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3677 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:53:04 root] (utils.py 302): INFO Epoch: [0]  [ 330/4926]  eta: 0:49:22  lr_architecture: 0.000050  loss_cls: 2.9625 (2.8087)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4007 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:53:10 root] (utils.py 302): INFO Epoch: [0]  [ 340/4926]  eta: 0:49:12  lr_architecture: 0.000050  loss_cls: 2.7936 (2.8076)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7561 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:53:16 root] (utils.py 302): INFO Epoch: [0]  [ 350/4926]  eta: 0:49:03  lr_architecture: 0.000050  loss_cls: 2.9013 (2.8158)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.9009 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:53:23 root] (utils.py 302): INFO Epoch: [0]  [ 360/4926]  eta: 0:48:53  lr_architecture: 0.000050  loss_cls: 2.8024 (2.8106)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.9009 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:53:29 root] (utils.py 302): INFO Epoch: [0]  [ 370/4926]  eta: 0:48:43  lr_architecture: 0.000050  loss_cls: 2.6301 (2.8017)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4450 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:53:35 root] (utils.py 302): INFO Epoch: [0]  [ 380/4926]  eta: 0:48:34  lr_architecture: 0.000050  loss_cls: 2.7785 (2.8035)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2555 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:53:41 root] (utils.py 302): INFO Epoch: [0]  [ 390/4926]  eta: 0:48:25  lr_architecture: 0.000050  loss_cls: 2.8608 (2.8021)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2525 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:53:47 root] (utils.py 302): INFO Epoch: [0]  [ 400/4926]  eta: 0:48:16  lr_architecture: 0.000050  loss_cls: 2.7211 (2.7977)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2586 (inf)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:53:53 root] (utils.py 302): INFO Epoch: [0]  [ 410/4926]  eta: 0:48:07  lr_architecture: 0.000050  loss_cls: 2.7577 (2.8032)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.9010 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:54:00 root] (utils.py 302): INFO Epoch: [0]  [ 420/4926]  eta: 0:47:59  lr_architecture: 0.000050  loss_cls: 2.9758 (2.8041)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7760 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:54:06 root] (utils.py 302): INFO Epoch: [0]  [ 430/4926]  eta: 0:47:50  lr_architecture: 0.000050  loss_cls: 2.9231 (2.8073)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4484 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:54:12 root] (utils.py 302): INFO Epoch: [0]  [ 440/4926]  eta: 0:47:42  lr_architecture: 0.000050  loss_cls: 2.8497 (2.8081)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4484 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:54:18 root] (utils.py 302): INFO Epoch: [0]  [ 450/4926]  eta: 0:47:33  lr_architecture: 0.000050  loss_cls: 2.7827 (2.8084)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4086 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:54:24 root] (utils.py 302): INFO Epoch: [0]  [ 460/4926]  eta: 0:47:25  lr_architecture: 0.000050  loss_cls: 2.9818 (2.8109)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4560 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:54:31 root] (utils.py 302): INFO Epoch: [0]  [ 470/4926]  eta: 0:47:17  lr_architecture: 0.000050  loss_cls: 3.0417 (2.8138)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4456 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:54:37 root] (utils.py 302): INFO Epoch: [0]  [ 480/4926]  eta: 0:47:09  lr_architecture: 0.000050  loss_cls: 2.8527 (2.8129)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2645 (inf)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:54:43 root] (utils.py 302): INFO Epoch: [0]  [ 490/4926]  eta: 0:47:01  lr_architecture: 0.000050  loss_cls: 2.8527 (2.8131)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2740 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:54:49 root] (utils.py 302): INFO Epoch: [0]  [ 500/4926]  eta: 0:46:53  lr_architecture: 0.000050  loss_cls: 2.8753 (2.8097)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2740 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:54:55 root] (utils.py 302): INFO Epoch: [0]  [ 510/4926]  eta: 0:46:45  lr_architecture: 0.000050  loss_cls: 2.9291 (2.8133)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3210 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 17:55:01 root] (utils.py 302): INFO Epoch: [0]  [ 520/4926]  eta: 0:46:37  lr_architecture: 0.000050  loss_cls: 2.9417 (2.8082)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2423 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:55:08 root] (utils.py 302): INFO Epoch: [0]  [ 530/4926]  eta: 0:46:31  lr_architecture: 0.000050  loss_cls: 2.9048 (2.8092)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0388 (inf)  time: 0.6261  data: 0.0001  max mem: 14435
[2024-01-21 17:55:14 root] (utils.py 302): INFO Epoch: [0]  [ 540/4926]  eta: 0:46:23  lr_architecture: 0.000050  loss_cls: 2.9372 (2.8096)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3543 (inf)  time: 0.6265  data: 0.0001  max mem: 14435
[2024-01-21 17:55:20 root] (utils.py 302): INFO Epoch: [0]  [ 550/4926]  eta: 0:46:15  lr_architecture: 0.000050  loss_cls: 2.9148 (2.8075)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4547 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:55:26 root] (utils.py 302): INFO Epoch: [0]  [ 560/4926]  eta: 0:46:08  lr_architecture: 0.000050  loss_cls: 2.9110 (2.8079)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4547 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:55:33 root] (utils.py 302): INFO Epoch: [0]  [ 570/4926]  eta: 0:46:00  lr_architecture: 0.000050  loss_cls: 2.7967 (2.8068)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2466 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:55:39 root] (utils.py 302): INFO Epoch: [0]  [ 580/4926]  eta: 0:45:53  lr_architecture: 0.000050  loss_cls: 2.7546 (2.8042)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2466 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:55:45 root] (utils.py 302): INFO Epoch: [0]  [ 590/4926]  eta: 0:45:45  lr_architecture: 0.000050  loss_cls: 2.8011 (2.8062)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2825 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:55:51 root] (utils.py 302): INFO Epoch: [0]  [ 600/4926]  eta: 0:45:40  lr_architecture: 0.000050  loss_cls: 2.8011 (2.8026)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3029 (inf)  time: 0.6311  data: 0.0001  max mem: 14435
[2024-01-21 17:55:58 root] (utils.py 302): INFO Epoch: [0]  [ 610/4926]  eta: 0:45:32  lr_architecture: 0.000050  loss_cls: 2.4959 (2.7997)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1365 (inf)  time: 0.6316  data: 0.0001  max mem: 14435
[2024-01-21 17:56:04 root] (utils.py 302): INFO Epoch: [0]  [ 620/4926]  eta: 0:45:25  lr_architecture: 0.000050  loss_cls: 2.7833 (2.8019)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1741 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:56:10 root] (utils.py 302): INFO Epoch: [0]  [ 630/4926]  eta: 0:45:17  lr_architecture: 0.000050  loss_cls: 3.0698 (2.8061)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3385 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:56:16 root] (utils.py 302): INFO Epoch: [0]  [ 640/4926]  eta: 0:45:10  lr_architecture: 0.000050  loss_cls: 3.0698 (2.8082)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1989 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:56:22 root] (utils.py 302): INFO Epoch: [0]  [ 650/4926]  eta: 0:45:03  lr_architecture: 0.000050  loss_cls: 2.7830 (2.8046)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3483 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:56:28 root] (utils.py 302): INFO Epoch: [0]  [ 660/4926]  eta: 0:44:56  lr_architecture: 0.000050  loss_cls: 2.9783 (2.8080)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3698 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:56:35 root] (utils.py 302): INFO Epoch: [0]  [ 670/4926]  eta: 0:44:48  lr_architecture: 0.000050  loss_cls: 3.0108 (2.8088)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6347 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 17:56:41 root] (utils.py 302): INFO Epoch: [0]  [ 680/4926]  eta: 0:44:41  lr_architecture: 0.000050  loss_cls: 2.8226 (2.8055)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6008 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:56:47 root] (utils.py 302): INFO Epoch: [0]  [ 690/4926]  eta: 0:44:34  lr_architecture: 0.000050  loss_cls: 2.6552 (2.8031)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4817 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 17:56:53 root] (utils.py 302): INFO Epoch: [0]  [ 700/4926]  eta: 0:44:27  lr_architecture: 0.000050  loss_cls: 2.8893 (2.8070)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4146 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:56:59 root] (utils.py 302): INFO Epoch: [0]  [ 710/4926]  eta: 0:44:20  lr_architecture: 0.000050  loss_cls: 2.8893 (2.8079)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2502 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:57:05 root] (utils.py 302): INFO Epoch: [0]  [ 720/4926]  eta: 0:44:13  lr_architecture: 0.000050  loss_cls: 2.8839 (2.8078)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.5115 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:57:12 root] (utils.py 302): INFO Epoch: [0]  [ 730/4926]  eta: 0:44:06  lr_architecture: 0.000050  loss_cls: 2.9810 (2.8073)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.5500 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:57:18 root] (utils.py 302): INFO Epoch: [0]  [ 740/4926]  eta: 0:43:59  lr_architecture: 0.000050  loss_cls: 2.8315 (2.8053)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1203 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:57:24 root] (utils.py 302): INFO Epoch: [0]  [ 750/4926]  eta: 0:43:52  lr_architecture: 0.000050  loss_cls: 2.7444 (2.8046)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0405 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:57:30 root] (utils.py 302): INFO Epoch: [0]  [ 760/4926]  eta: 0:43:45  lr_architecture: 0.000050  loss_cls: 2.9788 (2.8077)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3628 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:57:36 root] (utils.py 302): INFO Epoch: [0]  [ 770/4926]  eta: 0:43:38  lr_architecture: 0.000050  loss_cls: 2.9788 (2.8087)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6187 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:57:43 root] (utils.py 302): INFO Epoch: [0]  [ 780/4926]  eta: 0:43:31  lr_architecture: 0.000050  loss_cls: 3.0239 (2.8120)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4230 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:57:49 root] (utils.py 302): INFO Epoch: [0]  [ 790/4926]  eta: 0:43:24  lr_architecture: 0.000050  loss_cls: 2.8385 (2.8103)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1810 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:57:55 root] (utils.py 302): INFO Epoch: [0]  [ 800/4926]  eta: 0:43:17  lr_architecture: 0.000050  loss_cls: 2.9177 (2.8132)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1586 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:58:01 root] (utils.py 302): INFO Epoch: [0]  [ 810/4926]  eta: 0:43:10  lr_architecture: 0.000050  loss_cls: 3.0063 (2.8130)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1471 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:58:07 root] (utils.py 302): INFO Epoch: [0]  [ 820/4926]  eta: 0:43:03  lr_architecture: 0.000050  loss_cls: 2.7114 (2.8098)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9418 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:58:13 root] (utils.py 302): INFO Epoch: [0]  [ 830/4926]  eta: 0:42:56  lr_architecture: 0.000050  loss_cls: 2.7114 (2.8095)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0879 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:58:20 root] (utils.py 302): INFO Epoch: [0]  [ 840/4926]  eta: 0:42:49  lr_architecture: 0.000050  loss_cls: 2.7763 (2.8092)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2562 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:58:26 root] (utils.py 302): INFO Epoch: [0]  [ 850/4926]  eta: 0:42:43  lr_architecture: 0.000050  loss_cls: 2.7812 (2.8109)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4314 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:58:32 root] (utils.py 302): INFO Epoch: [0]  [ 860/4926]  eta: 0:42:36  lr_architecture: 0.000050  loss_cls: 2.7488 (2.8106)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0060 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 17:58:38 root] (utils.py 302): INFO Epoch: [0]  [ 870/4926]  eta: 0:42:29  lr_architecture: 0.000050  loss_cls: 2.9653 (2.8140)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9770 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:58:44 root] (utils.py 302): INFO Epoch: [0]  [ 880/4926]  eta: 0:42:22  lr_architecture: 0.000050  loss_cls: 2.9513 (2.8138)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2608 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:58:50 root] (utils.py 302): INFO Epoch: [0]  [ 890/4926]  eta: 0:42:15  lr_architecture: 0.000050  loss_cls: 2.9037 (2.8147)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1742 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:58:57 root] (utils.py 302): INFO Epoch: [0]  [ 900/4926]  eta: 0:42:09  lr_architecture: 0.000050  loss_cls: 2.9037 (2.8151)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1742 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:59:03 root] (utils.py 302): INFO Epoch: [0]  [ 910/4926]  eta: 0:42:02  lr_architecture: 0.000050  loss_cls: 2.9283 (2.8148)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2509 (inf)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:59:09 root] (utils.py 302): INFO Epoch: [0]  [ 920/4926]  eta: 0:41:55  lr_architecture: 0.000050  loss_cls: 2.9834 (2.8160)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4124 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:59:15 root] (utils.py 302): INFO Epoch: [0]  [ 930/4926]  eta: 0:41:49  lr_architecture: 0.000050  loss_cls: 2.8194 (2.8146)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4424 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:59:21 root] (utils.py 302): INFO Epoch: [0]  [ 940/4926]  eta: 0:41:42  lr_architecture: 0.000050  loss_cls: 2.5930 (2.8130)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3109 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:59:28 root] (utils.py 302): INFO Epoch: [0]  [ 950/4926]  eta: 0:41:35  lr_architecture: 0.000050  loss_cls: 2.9191 (2.8173)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2997 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:59:34 root] (utils.py 302): INFO Epoch: [0]  [ 960/4926]  eta: 0:41:28  lr_architecture: 0.000050  loss_cls: 3.1271 (2.8194)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1092 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:59:40 root] (utils.py 302): INFO Epoch: [0]  [ 970/4926]  eta: 0:41:22  lr_architecture: 0.000050  loss_cls: 3.0958 (2.8224)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1582 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:59:46 root] (utils.py 302): INFO Epoch: [0]  [ 980/4926]  eta: 0:41:15  lr_architecture: 0.000050  loss_cls: 3.0382 (2.8212)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2626 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:59:52 root] (utils.py 302): INFO Epoch: [0]  [ 990/4926]  eta: 0:41:08  lr_architecture: 0.000050  loss_cls: 2.6955 (2.8203)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4092 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 17:59:58 root] (utils.py 302): INFO Epoch: [0]  [1000/4926]  eta: 0:41:02  lr_architecture: 0.000050  loss_cls: 2.6375 (2.8174)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1340 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:00:05 root] (utils.py 302): INFO Epoch: [0]  [1010/4926]  eta: 0:40:55  lr_architecture: 0.000050  loss_cls: 2.6375 (2.8163)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0145 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:00:11 root] (utils.py 302): INFO Epoch: [0]  [1020/4926]  eta: 0:40:48  lr_architecture: 0.000050  loss_cls: 2.9539 (2.8174)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1103 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:00:17 root] (utils.py 302): INFO Epoch: [0]  [1030/4926]  eta: 0:40:42  lr_architecture: 0.000050  loss_cls: 3.2096 (2.8210)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2242 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:00:23 root] (utils.py 302): INFO Epoch: [0]  [1040/4926]  eta: 0:40:35  lr_architecture: 0.000050  loss_cls: 2.9699 (2.8191)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1670 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:00:29 root] (utils.py 302): INFO Epoch: [0]  [1050/4926]  eta: 0:40:29  lr_architecture: 0.000050  loss_cls: 2.9275 (2.8192)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1596 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:00:36 root] (utils.py 302): INFO Epoch: [0]  [1060/4926]  eta: 0:40:22  lr_architecture: 0.000050  loss_cls: 2.8779 (2.8202)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0127 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 18:00:42 root] (utils.py 302): INFO Epoch: [0]  [1070/4926]  eta: 0:40:15  lr_architecture: 0.000050  loss_cls: 2.8779 (2.8208)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9255 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:00:48 root] (utils.py 302): INFO Epoch: [0]  [1080/4926]  eta: 0:40:09  lr_architecture: 0.000050  loss_cls: 2.9176 (2.8199)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0092 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:00:54 root] (utils.py 302): INFO Epoch: [0]  [1090/4926]  eta: 0:40:02  lr_architecture: 0.000050  loss_cls: 2.9379 (2.8197)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0889 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 18:01:00 root] (utils.py 302): INFO Epoch: [0]  [1100/4926]  eta: 0:39:56  lr_architecture: 0.000050  loss_cls: 2.9485 (2.8196)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1153 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:01:06 root] (utils.py 302): INFO Epoch: [0]  [1110/4926]  eta: 0:39:49  lr_architecture: 0.000050  loss_cls: 3.1033 (2.8217)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2399 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 18:01:13 root] (utils.py 302): INFO Epoch: [0]  [1120/4926]  eta: 0:39:43  lr_architecture: 0.000050  loss_cls: 3.1033 (2.8231)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2399 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:01:19 root] (utils.py 302): INFO Epoch: [0]  [1130/4926]  eta: 0:39:36  lr_architecture: 0.000050  loss_cls: 2.8856 (2.8224)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0182 (inf)  time: 0.6182  data: 0.0002  max mem: 14435
[2024-01-21 18:01:25 root] (utils.py 302): INFO Epoch: [0]  [1140/4926]  eta: 0:39:30  lr_architecture: 0.000050  loss_cls: 2.7692 (2.8209)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8994 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 18:01:31 root] (utils.py 302): INFO Epoch: [0]  [1150/4926]  eta: 0:39:23  lr_architecture: 0.000050  loss_cls: 2.9655 (2.8214)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8269 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 18:01:37 root] (utils.py 302): INFO Epoch: [0]  [1160/4926]  eta: 0:39:17  lr_architecture: 0.000050  loss_cls: 2.9029 (2.8201)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8349 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:01:44 root] (utils.py 302): INFO Epoch: [0]  [1170/4926]  eta: 0:39:10  lr_architecture: 0.000050  loss_cls: 2.5928 (2.8171)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7490 (inf)  time: 0.6157  data: 0.0001  max mem: 14435
[2024-01-21 18:01:50 root] (utils.py 302): INFO Epoch: [0]  [1180/4926]  eta: 0:39:04  lr_architecture: 0.000050  loss_cls: 2.5139 (2.8155)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8116 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 18:01:56 root] (utils.py 302): INFO Epoch: [0]  [1190/4926]  eta: 0:38:57  lr_architecture: 0.000050  loss_cls: 2.8298 (2.8178)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8462 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:02:02 root] (utils.py 302): INFO Epoch: [0]  [1200/4926]  eta: 0:38:50  lr_architecture: 0.000050  loss_cls: 2.8432 (2.8165)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0580 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:02:08 root] (utils.py 302): INFO Epoch: [0]  [1210/4926]  eta: 0:38:44  lr_architecture: 0.000050  loss_cls: 2.8118 (2.8175)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2814 (inf)  time: 0.6166  data: 0.0001  max mem: 14435
[2024-01-21 18:02:14 root] (utils.py 302): INFO Epoch: [0]  [1220/4926]  eta: 0:38:37  lr_architecture: 0.000050  loss_cls: 2.7369 (2.8149)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2515 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:02:21 root] (utils.py 302): INFO Epoch: [0]  [1230/4926]  eta: 0:38:31  lr_architecture: 0.000050  loss_cls: 2.7369 (2.8163)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2051 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:02:27 root] (utils.py 302): INFO Epoch: [0]  [1240/4926]  eta: 0:38:24  lr_architecture: 0.000050  loss_cls: 2.9508 (2.8161)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0342 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 18:02:33 root] (utils.py 302): INFO Epoch: [0]  [1250/4926]  eta: 0:38:18  lr_architecture: 0.000050  loss_cls: 2.8318 (2.8153)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8747 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:02:39 root] (utils.py 302): INFO Epoch: [0]  [1260/4926]  eta: 0:38:12  lr_architecture: 0.000050  loss_cls: 2.8485 (2.8159)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7353 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:02:45 root] (utils.py 302): INFO Epoch: [0]  [1270/4926]  eta: 0:38:05  lr_architecture: 0.000050  loss_cls: 2.8667 (2.8151)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0784 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 18:02:51 root] (utils.py 302): INFO Epoch: [0]  [1280/4926]  eta: 0:37:59  lr_architecture: 0.000050  loss_cls: 2.7472 (2.8155)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2665 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 18:02:58 root] (utils.py 302): INFO Epoch: [0]  [1290/4926]  eta: 0:37:52  lr_architecture: 0.000050  loss_cls: 2.8239 (2.8157)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4759 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:03:04 root] (utils.py 302): INFO Epoch: [0]  [1300/4926]  eta: 0:37:46  lr_architecture: 0.000050  loss_cls: 2.8830 (2.8154)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3382 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:03:10 root] (utils.py 302): INFO Epoch: [0]  [1310/4926]  eta: 0:37:39  lr_architecture: 0.000050  loss_cls: 2.9993 (2.8185)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1645 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:03:16 root] (utils.py 302): INFO Epoch: [0]  [1320/4926]  eta: 0:37:33  lr_architecture: 0.000050  loss_cls: 3.0199 (2.8176)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0273 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:03:22 root] (utils.py 302): INFO Epoch: [0]  [1330/4926]  eta: 0:37:26  lr_architecture: 0.000050  loss_cls: 2.7481 (2.8170)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8058 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:03:28 root] (utils.py 302): INFO Epoch: [0]  [1340/4926]  eta: 0:37:20  lr_architecture: 0.000050  loss_cls: 2.9004 (2.8178)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8058 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 18:03:35 root] (utils.py 302): INFO Epoch: [0]  [1350/4926]  eta: 0:37:13  lr_architecture: 0.000050  loss_cls: 2.9094 (2.8183)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8322 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:03:41 root] (utils.py 302): INFO Epoch: [0]  [1360/4926]  eta: 0:37:07  lr_architecture: 0.000050  loss_cls: 2.9094 (2.8186)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8372 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:03:47 root] (utils.py 302): INFO Epoch: [0]  [1370/4926]  eta: 0:37:00  lr_architecture: 0.000050  loss_cls: 3.0531 (2.8202)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2666 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 18:03:53 root] (utils.py 302): INFO Epoch: [0]  [1380/4926]  eta: 0:36:54  lr_architecture: 0.000050  loss_cls: 3.0374 (2.8197)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0463 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:03:59 root] (utils.py 302): INFO Epoch: [0]  [1390/4926]  eta: 0:36:48  lr_architecture: 0.000050  loss_cls: 3.0374 (2.8210)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0112 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:06 root] (utils.py 302): INFO Epoch: [0]  [1400/4926]  eta: 0:36:41  lr_architecture: 0.000050  loss_cls: 2.8706 (2.8208)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0490 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:12 root] (utils.py 302): INFO Epoch: [0]  [1410/4926]  eta: 0:36:35  lr_architecture: 0.000050  loss_cls: 2.7978 (2.8205)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9136 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:04:18 root] (utils.py 302): INFO Epoch: [0]  [1420/4926]  eta: 0:36:28  lr_architecture: 0.000050  loss_cls: 2.9071 (2.8218)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9136 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:04:24 root] (utils.py 302): INFO Epoch: [0]  [1430/4926]  eta: 0:36:22  lr_architecture: 0.000050  loss_cls: 2.9003 (2.8216)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0128 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:30 root] (utils.py 302): INFO Epoch: [0]  [1440/4926]  eta: 0:36:16  lr_architecture: 0.000050  loss_cls: 2.7692 (2.8196)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0694 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:36 root] (utils.py 302): INFO Epoch: [0]  [1450/4926]  eta: 0:36:09  lr_architecture: 0.000050  loss_cls: 2.7772 (2.8206)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0275 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:04:43 root] (utils.py 302): INFO Epoch: [0]  [1460/4926]  eta: 0:36:03  lr_architecture: 0.000050  loss_cls: 2.8743 (2.8200)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7960 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:04:49 root] (utils.py 302): INFO Epoch: [0]  [1470/4926]  eta: 0:35:56  lr_architecture: 0.000050  loss_cls: 2.7456 (2.8197)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8363 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:55 root] (utils.py 302): INFO Epoch: [0]  [1480/4926]  eta: 0:35:50  lr_architecture: 0.000050  loss_cls: 2.7368 (2.8193)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8363 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:05:01 root] (utils.py 302): INFO Epoch: [0]  [1490/4926]  eta: 0:35:44  lr_architecture: 0.000050  loss_cls: 2.8795 (2.8213)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9056 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:05:07 root] (utils.py 302): INFO Epoch: [0]  [1500/4926]  eta: 0:35:37  lr_architecture: 0.000050  loss_cls: 2.8941 (2.8207)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0355 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:05:13 root] (utils.py 302): INFO Epoch: [0]  [1510/4926]  eta: 0:35:31  lr_architecture: 0.000050  loss_cls: 2.8565 (2.8220)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0300 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:05:20 root] (utils.py 302): INFO Epoch: [0]  [1520/4926]  eta: 0:35:24  lr_architecture: 0.000050  loss_cls: 2.8565 (2.8200)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8817 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:05:26 root] (utils.py 302): INFO Epoch: [0]  [1530/4926]  eta: 0:35:18  lr_architecture: 0.000050  loss_cls: 2.4926 (2.8200)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8144 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:05:32 root] (utils.py 302): INFO Epoch: [0]  [1540/4926]  eta: 0:35:12  lr_architecture: 0.000050  loss_cls: 2.8126 (2.8187)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6871 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:05:38 root] (utils.py 302): INFO Epoch: [0]  [1550/4926]  eta: 0:35:05  lr_architecture: 0.000050  loss_cls: 2.9025 (2.8196)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7930 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:05:44 root] (utils.py 302): INFO Epoch: [0]  [1560/4926]  eta: 0:34:59  lr_architecture: 0.000050  loss_cls: 3.0118 (2.8210)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1756 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:05:51 root] (utils.py 302): INFO Epoch: [0]  [1570/4926]  eta: 0:34:52  lr_architecture: 0.000050  loss_cls: 3.0000 (2.8213)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0830 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:05:57 root] (utils.py 302): INFO Epoch: [0]  [1580/4926]  eta: 0:34:46  lr_architecture: 0.000050  loss_cls: 2.9147 (2.8211)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8671 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:06:03 root] (utils.py 302): INFO Epoch: [0]  [1590/4926]  eta: 0:34:40  lr_architecture: 0.000050  loss_cls: 2.9994 (2.8225)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2101 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:06:09 root] (utils.py 302): INFO Epoch: [0]  [1600/4926]  eta: 0:34:33  lr_architecture: 0.000050  loss_cls: 2.9115 (2.8205)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2909 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:06:15 root] (utils.py 302): INFO Epoch: [0]  [1610/4926]  eta: 0:34:27  lr_architecture: 0.000050  loss_cls: 2.8257 (2.8200)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9003 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:06:21 root] (utils.py 302): INFO Epoch: [0]  [1620/4926]  eta: 0:34:21  lr_architecture: 0.000050  loss_cls: 2.8997 (2.8199)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0110 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:06:28 root] (utils.py 302): INFO Epoch: [0]  [1630/4926]  eta: 0:34:14  lr_architecture: 0.000050  loss_cls: 3.0324 (2.8207)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7389 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:06:34 root] (utils.py 302): INFO Epoch: [0]  [1640/4926]  eta: 0:34:08  lr_architecture: 0.000050  loss_cls: 2.8147 (2.8198)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7246 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:06:40 root] (utils.py 302): INFO Epoch: [0]  [1650/4926]  eta: 0:34:02  lr_architecture: 0.000050  loss_cls: 2.7157 (2.8196)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7044 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:06:46 root] (utils.py 302): INFO Epoch: [0]  [1660/4926]  eta: 0:33:55  lr_architecture: 0.000050  loss_cls: 2.8425 (2.8207)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6887 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:06:52 root] (utils.py 302): INFO Epoch: [0]  [1670/4926]  eta: 0:33:49  lr_architecture: 0.000050  loss_cls: 2.8456 (2.8195)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8399 (inf)  time: 0.6166  data: 0.0001  max mem: 14435
[2024-01-21 18:06:58 root] (utils.py 302): INFO Epoch: [0]  [1680/4926]  eta: 0:33:42  lr_architecture: 0.000050  loss_cls: 2.6429 (2.8184)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9823 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:07:05 root] (utils.py 302): INFO Epoch: [0]  [1690/4926]  eta: 0:33:36  lr_architecture: 0.000050  loss_cls: 2.7672 (2.8187)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9823 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:07:11 root] (utils.py 302): INFO Epoch: [0]  [1700/4926]  eta: 0:33:30  lr_architecture: 0.000050  loss_cls: 2.8875 (2.8191)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9441 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:07:17 root] (utils.py 302): INFO Epoch: [0]  [1710/4926]  eta: 0:33:23  lr_architecture: 0.000050  loss_cls: 2.9418 (2.8192)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9527 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:07:23 root] (utils.py 302): INFO Epoch: [0]  [1720/4926]  eta: 0:33:17  lr_architecture: 0.000050  loss_cls: 2.9576 (2.8194)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8780 (inf)  time: 0.6231  data: 0.0001  max mem: 14435
[2024-01-21 18:07:29 root] (utils.py 302): INFO Epoch: [0]  [1730/4926]  eta: 0:33:11  lr_architecture: 0.000050  loss_cls: 2.9867 (2.8204)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9376 (inf)  time: 0.6232  data: 0.0001  max mem: 14435
[2024-01-21 18:07:36 root] (utils.py 302): INFO Epoch: [0]  [1740/4926]  eta: 0:33:05  lr_architecture: 0.000050  loss_cls: 3.0261 (2.8217)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0414 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 18:07:42 root] (utils.py 302): INFO Epoch: [0]  [1750/4926]  eta: 0:32:58  lr_architecture: 0.000050  loss_cls: 3.0151 (2.8228)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9026 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:07:48 root] (utils.py 302): INFO Epoch: [0]  [1760/4926]  eta: 0:32:52  lr_architecture: 0.000050  loss_cls: 2.9678 (2.8232)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8929 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:07:54 root] (utils.py 302): INFO Epoch: [0]  [1770/4926]  eta: 0:32:46  lr_architecture: 0.000050  loss_cls: 2.9562 (2.8242)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9366 (inf)  time: 0.6230  data: 0.0001  max mem: 14435
[2024-01-21 18:08:01 root] (utils.py 302): INFO Epoch: [0]  [1780/4926]  eta: 0:32:40  lr_architecture: 0.000050  loss_cls: 2.9103 (2.8244)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9497 (inf)  time: 0.6294  data: 0.0001  max mem: 14435
[2024-01-21 18:08:07 root] (utils.py 302): INFO Epoch: [0]  [1790/4926]  eta: 0:32:33  lr_architecture: 0.000050  loss_cls: 2.8445 (2.8233)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6417 (inf)  time: 0.6240  data: 0.0001  max mem: 14435
[2024-01-21 18:08:13 root] (utils.py 302): INFO Epoch: [0]  [1800/4926]  eta: 0:32:27  lr_architecture: 0.000050  loss_cls: 2.9406 (2.8240)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6119 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:08:19 root] (utils.py 302): INFO Epoch: [0]  [1810/4926]  eta: 0:32:21  lr_architecture: 0.000050  loss_cls: 2.7292 (2.8229)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.5780 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:08:25 root] (utils.py 302): INFO Epoch: [0]  [1820/4926]  eta: 0:32:14  lr_architecture: 0.000050  loss_cls: 2.7816 (2.8235)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6436 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:08:31 root] (utils.py 302): INFO Epoch: [0]  [1830/4926]  eta: 0:32:08  lr_architecture: 0.000050  loss_cls: 2.8355 (2.8228)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7217 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:08:38 root] (utils.py 302): INFO Epoch: [0]  [1840/4926]  eta: 0:32:02  lr_architecture: 0.000050  loss_cls: 2.8104 (2.8239)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7587 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:08:44 root] (utils.py 302): INFO Epoch: [0]  [1850/4926]  eta: 0:31:55  lr_architecture: 0.000050  loss_cls: 2.8104 (2.8238)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8273 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:08:50 root] (utils.py 302): INFO Epoch: [0]  [1860/4926]  eta: 0:31:49  lr_architecture: 0.000050  loss_cls: 2.8719 (2.8236)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9505 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:08:56 root] (utils.py 302): INFO Epoch: [0]  [1870/4926]  eta: 0:31:43  lr_architecture: 0.000050  loss_cls: 3.0212 (2.8247)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9542 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:09:02 root] (utils.py 302): INFO Epoch: [0]  [1880/4926]  eta: 0:31:37  lr_architecture: 0.000050  loss_cls: 2.8204 (2.8242)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8330 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:09:08 root] (utils.py 302): INFO Epoch: [0]  [1890/4926]  eta: 0:31:30  lr_architecture: 0.000050  loss_cls: 2.6959 (2.8243)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0078 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:09:15 root] (utils.py 302): INFO Epoch: [0]  [1900/4926]  eta: 0:31:24  lr_architecture: 0.000050  loss_cls: 2.6884 (2.8224)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8176 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:09:21 root] (utils.py 302): INFO Epoch: [0]  [1910/4926]  eta: 0:31:18  lr_architecture: 0.000050  loss_cls: 2.7188 (2.8226)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7115 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:09:27 root] (utils.py 302): INFO Epoch: [0]  [1920/4926]  eta: 0:31:11  lr_architecture: 0.000050  loss_cls: 2.8091 (2.8215)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6049 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:09:33 root] (utils.py 302): INFO Epoch: [0]  [1930/4926]  eta: 0:31:05  lr_architecture: 0.000050  loss_cls: 2.6353 (2.8208)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6834 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:09:39 root] (utils.py 302): INFO Epoch: [0]  [1940/4926]  eta: 0:30:59  lr_architecture: 0.000050  loss_cls: 2.7774 (2.8211)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7753 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:09:45 root] (utils.py 302): INFO Epoch: [0]  [1950/4926]  eta: 0:30:52  lr_architecture: 0.000050  loss_cls: 2.9743 (2.8221)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6946 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:09:52 root] (utils.py 302): INFO Epoch: [0]  [1960/4926]  eta: 0:30:46  lr_architecture: 0.000050  loss_cls: 2.9798 (2.8226)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8224 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:09:58 root] (utils.py 302): INFO Epoch: [0]  [1970/4926]  eta: 0:30:40  lr_architecture: 0.000050  loss_cls: 2.8363 (2.8221)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9604 (inf)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 18:10:04 root] (utils.py 302): INFO Epoch: [0]  [1980/4926]  eta: 0:30:34  lr_architecture: 0.000050  loss_cls: 2.8097 (2.8220)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0287 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 18:10:10 root] (utils.py 302): INFO Epoch: [0]  [1990/4926]  eta: 0:30:27  lr_architecture: 0.000050  loss_cls: 2.7323 (2.8214)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9067 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:10:16 root] (utils.py 302): INFO Epoch: [0]  [2000/4926]  eta: 0:30:21  lr_architecture: 0.000050  loss_cls: 2.8319 (2.8209)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6025 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:10:23 root] (utils.py 302): INFO Epoch: [0]  [2010/4926]  eta: 0:30:15  lr_architecture: 0.000050  loss_cls: 2.8319 (2.8200)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8850 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:10:29 root] (utils.py 302): INFO Epoch: [0]  [2020/4926]  eta: 0:30:08  lr_architecture: 0.000050  loss_cls: 2.9412 (2.8214)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7547 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 18:10:35 root] (utils.py 302): INFO Epoch: [0]  [2030/4926]  eta: 0:30:02  lr_architecture: 0.000050  loss_cls: 2.8952 (2.8217)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6635 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 18:10:41 root] (utils.py 302): INFO Epoch: [0]  [2040/4926]  eta: 0:29:56  lr_architecture: 0.000050  loss_cls: 2.8684 (2.8229)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7570 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:10:47 root] (utils.py 302): INFO Epoch: [0]  [2050/4926]  eta: 0:29:49  lr_architecture: 0.000050  loss_cls: 2.9227 (2.8233)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7615 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 18:10:53 root] (utils.py 302): INFO Epoch: [0]  [2060/4926]  eta: 0:29:43  lr_architecture: 0.000050  loss_cls: 2.8544 (2.8224)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1213 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 18:11:00 root] (utils.py 302): INFO Epoch: [0]  [2070/4926]  eta: 0:29:37  lr_architecture: 0.000050  loss_cls: 2.9064 (2.8231)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1233 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:11:06 root] (utils.py 302): INFO Epoch: [0]  [2080/4926]  eta: 0:29:31  lr_architecture: 0.000050  loss_cls: 2.9149 (2.8230)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9250 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:11:12 root] (utils.py 302): INFO Epoch: [0]  [2090/4926]  eta: 0:29:24  lr_architecture: 0.000050  loss_cls: 2.8734 (2.8233)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9216 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:11:18 root] (utils.py 302): INFO Epoch: [0]  [2100/4926]  eta: 0:29:18  lr_architecture: 0.000050  loss_cls: 2.8372 (2.8235)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9333 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 18:11:24 root] (utils.py 302): INFO Epoch: [0]  [2110/4926]  eta: 0:29:12  lr_architecture: 0.000050  loss_cls: 2.8020 (2.8232)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8400 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:11:31 root] (utils.py 302): INFO Epoch: [0]  [2120/4926]  eta: 0:29:05  lr_architecture: 0.000050  loss_cls: 2.9249 (2.8235)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3897 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:11:37 root] (utils.py 302): INFO Epoch: [0]  [2130/4926]  eta: 0:28:59  lr_architecture: 0.000050  loss_cls: 2.9605 (2.8240)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6892 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:11:43 root] (utils.py 302): INFO Epoch: [0]  [2140/4926]  eta: 0:28:53  lr_architecture: 0.000050  loss_cls: 2.8813 (2.8243)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7667 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:11:49 root] (utils.py 302): INFO Epoch: [0]  [2150/4926]  eta: 0:28:47  lr_architecture: 0.000050  loss_cls: 2.9512 (2.8240)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9896 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:11:55 root] (utils.py 302): INFO Epoch: [0]  [2160/4926]  eta: 0:28:40  lr_architecture: 0.000050  loss_cls: 3.0188 (2.8240)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8249 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 18:12:01 root] (utils.py 302): INFO Epoch: [0]  [2170/4926]  eta: 0:28:34  lr_architecture: 0.000050  loss_cls: 3.0623 (2.8249)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8249 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:12:08 root] (utils.py 302): INFO Epoch: [0]  [2180/4926]  eta: 0:28:28  lr_architecture: 0.000050  loss_cls: 3.0810 (2.8255)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9852 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:12:14 root] (utils.py 302): INFO Epoch: [0]  [2190/4926]  eta: 0:28:21  lr_architecture: 0.000050  loss_cls: 3.0918 (2.8272)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9460 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 18:12:20 root] (utils.py 302): INFO Epoch: [0]  [2200/4926]  eta: 0:28:15  lr_architecture: 0.000050  loss_cls: 2.8711 (2.8262)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8984 (inf)  time: 0.6193  data: 0.0001  max mem: 14435
[2024-01-21 18:12:26 root] (utils.py 302): INFO Epoch: [0]  [2210/4926]  eta: 0:28:09  lr_architecture: 0.000050  loss_cls: 2.7695 (2.8260)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8984 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 18:12:32 root] (utils.py 302): INFO Epoch: [0]  [2220/4926]  eta: 0:28:03  lr_architecture: 0.000050  loss_cls: 2.8371 (2.8259)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6894 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:12:38 root] (utils.py 302): INFO Epoch: [0]  [2230/4926]  eta: 0:27:56  lr_architecture: 0.000050  loss_cls: 2.8244 (2.8250)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6500 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:12:45 root] (utils.py 302): INFO Epoch: [0]  [2240/4926]  eta: 0:27:50  lr_architecture: 0.000050  loss_cls: 2.8200 (2.8243)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7908 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:12:51 root] (utils.py 302): INFO Epoch: [0]  [2250/4926]  eta: 0:27:44  lr_architecture: 0.000050  loss_cls: 2.8043 (2.8238)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9979 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:12:57 root] (utils.py 302): INFO Epoch: [0]  [2260/4926]  eta: 0:27:38  lr_architecture: 0.000050  loss_cls: 2.7513 (2.8232)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9130 (inf)  time: 0.6190  data: 0.0001  max mem: 14435
[2024-01-21 18:13:03 root] (utils.py 302): INFO Epoch: [0]  [2270/4926]  eta: 0:27:31  lr_architecture: 0.000050  loss_cls: 2.6611 (2.8224)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7291 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:13:09 root] (utils.py 302): INFO Epoch: [0]  [2280/4926]  eta: 0:27:25  lr_architecture: 0.000050  loss_cls: 2.6611 (2.8217)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7291 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 18:13:16 root] (utils.py 302): INFO Epoch: [0]  [2290/4926]  eta: 0:27:19  lr_architecture: 0.000050  loss_cls: 2.9234 (2.8224)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8465 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 18:13:22 root] (utils.py 302): INFO Epoch: [0]  [2300/4926]  eta: 0:27:13  lr_architecture: 0.000050  loss_cls: 2.9259 (2.8220)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8706 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
