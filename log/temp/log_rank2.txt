[2024-01-21 14:05:51 root] (main_tome.py 216): INFO Namespace(batch_size=256, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:05:55 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:05:57 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:05:59 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:15:13 root] (main_tome.py 216): INFO Namespace(batch_size=256, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:15:17 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:15:19 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:15:21 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:15:21 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:16:41 root] (main_tome.py 216): INFO Namespace(batch_size=256, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:16:45 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:16:48 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:16:50 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:16:50 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:17:53 root] (main_tome.py 216): INFO Namespace(batch_size=100, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:17:58 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:18:01 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:18:01 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:18:01 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:18:32 root] (main_tome.py 216): INFO Namespace(batch_size=100, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:18:37 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:18:40 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:18:40 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:18:40 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:19:04 root] (main_tome.py 216): INFO Namespace(batch_size=103, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:19:09 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:19:12 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:19:13 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:19:13 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:19:35 root] (main_tome.py 216): INFO Namespace(batch_size=259, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:19:39 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:19:42 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:19:48 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:19:48 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:22:09 root] (main_tome.py 216): INFO Namespace(batch_size=259, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:22:14 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:22:18 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:22:22 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:22:22 root] (main_tome.py 416): INFO Start training for 2 epochs
[2024-01-21 14:23:48 root] (main_tome.py 213): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:23:52 root] (main_tome.py 283): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:23:54 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:23:55 root] (main_tome.py 367): INFO number of params: 86567656
[2024-01-21 14:23:55 root] (main_tome.py 413): INFO Start training for 2 epochs
[2024-01-21 14:35:29 root] (main_tome.py 213): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:35:34 root] (main_tome.py 283): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:35:36 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:35:36 root] (main_tome.py 367): INFO number of params: 86567656
[2024-01-21 14:35:36 root] (main_tome.py 413): INFO Start training for 2 epochs
[2024-01-21 14:40:00 root] (main_tome.py 213): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 14:40:04 root] (main_tome.py 283): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:40:06 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:40:07 root] (main_tome.py 367): INFO number of params: 86567656
[2024-01-21 14:40:07 root] (main_tome.py 413): INFO Start training for 2 epochs
[2024-01-21 14:43:13 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 15:48:46 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 15:48:53 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:48:54 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:49:00 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:49:01 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:49:01 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:49:25 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 15:49:40 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:49:41 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:49:44 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:49:45 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:49:45 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:50:55 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 15:50:59 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:51:00 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:51:02 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:51:03 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:51:03 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:51:06 root] (utils.py 293): INFO Epoch: [0]  [   0/3152]  eta: 2:14:04  lr_architecture: 0.010000  loss_cls: 2.6322 (2.6322)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 4.2487 (4.2487)  time: 2.5521  data: 0.0009  max mem: 8707
[2024-01-21 15:51:09 root] (utils.py 293): INFO Epoch: [0]  [  10/3152]  eta: 0:26:55  lr_architecture: 0.010000  loss_cls: 8.3568 (8.2061)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 3.0166 (inf)  time: 0.5140  data: 0.0002  max mem: 9646
[2024-01-21 15:51:12 root] (utils.py 293): INFO Epoch: [0]  [  20/3152]  eta: 0:21:19  lr_architecture: 0.010000  loss_cls: 7.8493 (8.0522)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.9231 (inf)  time: 0.3012  data: 0.0001  max mem: 9646
[2024-01-21 15:51:15 root] (utils.py 293): INFO Epoch: [0]  [  30/3152]  eta: 0:19:19  lr_architecture: 0.010000  loss_cls: 7.5433 (7.8373)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.0680 (inf)  time: 0.2929  data: 0.0001  max mem: 9646
[2024-01-21 15:51:35 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 15:51:39 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:51:40 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:51:43 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:51:44 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:51:44 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:51:46 root] (utils.py 293): INFO Epoch: [0]  [   0/3152]  eta: 2:16:50  lr_architecture: 0.010000  loss_cls: 2.6317 (2.6317)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 4.2495 (4.2495)  time: 2.6049  data: 0.0007  max mem: 8707
[2024-01-21 15:51:49 root] (utils.py 293): INFO Epoch: [0]  [  10/3152]  eta: 0:26:51  lr_architecture: 0.010000  loss_cls: 8.2941 (8.2366)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 2.9235 (inf)  time: 0.5127  data: 0.0002  max mem: 9646
[2024-01-21 15:51:52 root] (utils.py 293): INFO Epoch: [0]  [  20/3152]  eta: 0:21:19  lr_architecture: 0.010000  loss_cls: 7.8616 (8.0635)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.9709 (inf)  time: 0.2986  data: 0.0001  max mem: 9646
[2024-01-21 15:51:55 root] (utils.py 293): INFO Epoch: [0]  [  30/3152]  eta: 0:19:18  lr_architecture: 0.010000  loss_cls: 7.6479 (7.8876)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.0921 (inf)  time: 0.2930  data: 0.0001  max mem: 9646
[2024-01-21 15:51:58 root] (utils.py 293): INFO Epoch: [0]  [  40/3152]  eta: 0:18:14  lr_architecture: 0.010000  loss_cls: 7.2982 (7.7246)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.7031 (inf)  time: 0.2920  data: 0.0001  max mem: 9646
[2024-01-21 15:52:01 root] (utils.py 293): INFO Epoch: [0]  [  50/3152]  eta: 0:17:34  lr_architecture: 0.010000  loss_cls: 7.1551 (7.6094)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.5822 (inf)  time: 0.2919  data: 0.0001  max mem: 9646
[2024-01-21 15:52:04 root] (utils.py 293): INFO Epoch: [0]  [  60/3152]  eta: 0:17:07  lr_architecture: 0.010000  loss_cls: 7.1226 (7.5247)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.5522 (inf)  time: 0.2923  data: 0.0001  max mem: 9646
[2024-01-21 15:52:07 root] (utils.py 293): INFO Epoch: [0]  [  70/3152]  eta: 0:16:46  lr_architecture: 0.010000  loss_cls: 7.0894 (7.4643)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4694 (inf)  time: 0.2929  data: 0.0001  max mem: 9646
[2024-01-21 15:52:11 root] (utils.py 293): INFO Epoch: [0]  [  80/3152]  eta: 0:17:26  lr_architecture: 0.010000  loss_cls: 7.0751 (7.4147)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4471 (inf)  time: 0.3671  data: 0.0742  max mem: 9647
[2024-01-21 15:52:14 root] (utils.py 293): INFO Epoch: [0]  [  90/3152]  eta: 0:17:07  lr_architecture: 0.010000  loss_cls: 7.0751 (7.3754)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4074 (inf)  time: 0.3668  data: 0.0742  max mem: 9647
[2024-01-21 15:52:17 root] (utils.py 293): INFO Epoch: [0]  [ 100/3152]  eta: 0:16:50  lr_architecture: 0.010000  loss_cls: 7.0274 (7.3408)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3984 (inf)  time: 0.2923  data: 0.0001  max mem: 9647
[2024-01-21 15:52:20 root] (utils.py 293): INFO Epoch: [0]  [ 110/3152]  eta: 0:16:36  lr_architecture: 0.010000  loss_cls: 7.0256 (7.3127)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3984 (inf)  time: 0.2923  data: 0.0001  max mem: 9647
[2024-01-21 15:52:25 root] (utils.py 293): INFO Epoch: [0]  [ 120/3152]  eta: 0:17:13  lr_architecture: 0.010000  loss_cls: 7.0234 (7.2903)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4177 (inf)  time: 0.3901  data: 0.0708  max mem: 9647
[2024-01-21 15:52:28 root] (utils.py 293): INFO Epoch: [0]  [ 130/3152]  eta: 0:17:11  lr_architecture: 0.010000  loss_cls: 7.0213 (7.2697)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4432 (inf)  time: 0.4177  data: 0.0983  max mem: 9647
[2024-01-21 15:52:31 root] (utils.py 293): INFO Epoch: [0]  [ 140/3152]  eta: 0:16:58  lr_architecture: 0.010000  loss_cls: 6.9956 (7.2511)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4064 (inf)  time: 0.3202  data: 0.0276  max mem: 9647
[2024-01-21 15:52:34 root] (utils.py 293): INFO Epoch: [0]  [ 150/3152]  eta: 0:16:45  lr_architecture: 0.010000  loss_cls: 7.0028 (7.2350)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3963 (inf)  time: 0.2931  data: 0.0001  max mem: 9647
[2024-01-21 15:52:37 root] (utils.py 293): INFO Epoch: [0]  [ 160/3152]  eta: 0:16:34  lr_architecture: 0.010000  loss_cls: 7.0164 (7.2220)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4188 (inf)  time: 0.2932  data: 0.0001  max mem: 9647
[2024-01-21 15:52:40 root] (utils.py 293): INFO Epoch: [0]  [ 170/3152]  eta: 0:16:24  lr_architecture: 0.010000  loss_cls: 7.0243 (7.2101)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4107 (inf)  time: 0.2930  data: 0.0001  max mem: 9647
[2024-01-21 15:52:44 root] (utils.py 293): INFO Epoch: [0]  [ 180/3152]  eta: 0:16:23  lr_architecture: 0.010000  loss_cls: 7.0041 (7.1988)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3855 (inf)  time: 0.3181  data: 0.0001  max mem: 9647
[2024-01-21 15:52:48 root] (utils.py 293): INFO Epoch: [0]  [ 190/3152]  eta: 0:16:40  lr_architecture: 0.010000  loss_cls: 6.9994 (7.1887)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3616 (inf)  time: 0.4044  data: 0.0188  max mem: 9647
[2024-01-21 15:52:51 root] (utils.py 293): INFO Epoch: [0]  [ 200/3152]  eta: 0:16:31  lr_architecture: 0.010000  loss_cls: 6.9809 (7.1775)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3629 (inf)  time: 0.3812  data: 0.0188  max mem: 9647
[2024-01-21 15:52:54 root] (utils.py 293): INFO Epoch: [0]  [ 210/3152]  eta: 0:16:22  lr_architecture: 0.010000  loss_cls: 6.9676 (7.1683)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3629 (inf)  time: 0.2950  data: 0.0001  max mem: 9647
[2024-01-21 15:52:59 root] (utils.py 293): INFO Epoch: [0]  [ 220/3152]  eta: 0:16:33  lr_architecture: 0.010000  loss_cls: 6.9872 (7.1606)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3753 (inf)  time: 0.3676  data: 0.0243  max mem: 9647
[2024-01-21 15:53:02 root] (utils.py 293): INFO Epoch: [0]  [ 230/3152]  eta: 0:16:24  lr_architecture: 0.010000  loss_cls: 6.9896 (7.1527)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3753 (inf)  time: 0.3679  data: 0.0243  max mem: 9647
[2024-01-21 15:53:05 root] (utils.py 293): INFO Epoch: [0]  [ 240/3152]  eta: 0:16:15  lr_architecture: 0.010000  loss_cls: 6.9704 (7.1448)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3439 (inf)  time: 0.2935  data: 0.0001  max mem: 9647
[2024-01-21 15:53:08 root] (utils.py 293): INFO Epoch: [0]  [ 250/3152]  eta: 0:16:07  lr_architecture: 0.010000  loss_cls: 6.9646 (7.1381)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3414 (inf)  time: 0.2936  data: 0.0001  max mem: 9647
[2024-01-21 15:53:13 root] (utils.py 293): INFO Epoch: [0]  [ 260/3152]  eta: 0:16:28  lr_architecture: 0.010000  loss_cls: 6.9864 (7.1328)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3664 (inf)  time: 0.4257  data: 0.1323  max mem: 9647
[2024-01-21 15:53:16 root] (utils.py 293): INFO Epoch: [0]  [ 270/3152]  eta: 0:16:20  lr_architecture: 0.010000  loss_cls: 6.9864 (7.1270)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3944 (inf)  time: 0.4257  data: 0.1323  max mem: 9647
[2024-01-21 15:53:19 root] (utils.py 293): INFO Epoch: [0]  [ 280/3152]  eta: 0:16:12  lr_architecture: 0.010000  loss_cls: 6.9864 (7.1227)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3886 (inf)  time: 0.2938  data: 0.0001  max mem: 9647
[2024-01-21 15:53:22 root] (utils.py 293): INFO Epoch: [0]  [ 290/3152]  eta: 0:16:04  lr_architecture: 0.010000  loss_cls: 6.9939 (7.1183)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3795 (inf)  time: 0.2932  data: 0.0001  max mem: 9647
[2024-01-21 15:53:25 root] (utils.py 293): INFO Epoch: [0]  [ 300/3152]  eta: 0:16:01  lr_architecture: 0.010000  loss_cls: 6.9867 (7.1140)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3717 (inf)  time: 0.3189  data: 0.0001  max mem: 9647
[2024-01-21 15:53:29 root] (utils.py 293): INFO Epoch: [0]  [ 310/3152]  eta: 0:15:59  lr_architecture: 0.010000  loss_cls: 6.9645 (7.1091)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3426 (inf)  time: 0.3498  data: 0.0302  max mem: 9647
[2024-01-21 15:53:52 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 15:53:56 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:53:57 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:55:48 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:55:48 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:56:06 root] (main_tome.py 217): INFO Namespace(batch_size=64, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 15:56:15 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:56:17 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:56:29 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:56:29 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:56:49 root] (main_tome.py 217): INFO Namespace(batch_size=32, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 15:56:53 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:56:54 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:57:05 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:57:05 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:57:10 root] (utils.py 293): INFO Epoch: [0]  [   0/9852]  eta: 13:27:50  lr_architecture: 0.010000  loss_cls: 2.4931 (2.4931)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 11.2909 (11.2909)  time: 4.9198  data: 0.0006  max mem: 15215
[2024-01-21 15:57:26 root] (main_tome.py 217): INFO Namespace(batch_size=24, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 15:57:29 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:57:30 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:57:41 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:57:41 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:57:46 root] (utils.py 293): INFO Epoch: [0]  [    0/13136]  eta: 17:19:02  lr_architecture: 0.010000  loss_cls: 2.6815 (2.6815)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 15.4189 (15.4189)  time: 4.7459  data: 0.0009  max mem: 14771
[2024-01-21 15:57:56 root] (utils.py 293): INFO Epoch: [0]  [   10/13136]  eta: 4:58:15  lr_architecture: 0.010000  loss_cls: 7.8371 (7.5452)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 2.2490 (3.4921)  time: 1.3633  data: 0.0002  max mem: 20364
[2024-01-21 15:58:06 root] (utils.py 293): INFO Epoch: [0]  [   20/13136]  eta: 4:19:49  lr_architecture: 0.010000  loss_cls: 8.0374 (7.8526)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.7437 (2.6253)  time: 1.0107  data: 0.0001  max mem: 20364
[2024-01-21 15:58:16 root] (utils.py 293): INFO Epoch: [0]  [   30/13136]  eta: 4:06:04  lr_architecture: 0.010000  loss_cls: 7.7973 (7.7953)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.5196 (2.2081)  time: 0.9964  data: 0.0001  max mem: 20364
[2024-01-21 15:58:26 root] (utils.py 293): INFO Epoch: [0]  [   40/13136]  eta: 3:58:56  lr_architecture: 0.010000  loss_cls: 7.5035 (7.7119)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.2482 (1.9753)  time: 0.9962  data: 0.0001  max mem: 20364
[2024-01-21 15:58:36 root] (utils.py 293): INFO Epoch: [0]  [   50/13136]  eta: 3:54:27  lr_architecture: 0.010000  loss_cls: 7.2429 (7.6093)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1611 (1.8107)  time: 0.9950  data: 0.0001  max mem: 20364
[2024-01-21 15:58:46 root] (utils.py 293): INFO Epoch: [0]  [   60/13136]  eta: 3:51:22  lr_architecture: 0.010000  loss_cls: 7.1868 (7.5437)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1580 (1.7199)  time: 0.9940  data: 0.0001  max mem: 20364
[2024-01-21 15:58:56 root] (utils.py 293): INFO Epoch: [0]  [   70/13136]  eta: 3:49:07  lr_architecture: 0.010000  loss_cls: 7.1768 (7.4871)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1705 (1.6395)  time: 0.9939  data: 0.0001  max mem: 20364
[2024-01-21 15:59:06 root] (utils.py 293): INFO Epoch: [0]  [   80/13136]  eta: 3:47:23  lr_architecture: 0.010000  loss_cls: 7.1577 (7.4397)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1332 (1.5799)  time: 0.9942  data: 0.0001  max mem: 20364
[2024-01-21 15:59:16 root] (utils.py 293): INFO Epoch: [0]  [   90/13136]  eta: 3:46:01  lr_architecture: 0.010000  loss_cls: 7.1198 (7.4062)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1230 (1.5309)  time: 0.9949  data: 0.0001  max mem: 20364
[2024-01-21 15:59:26 root] (utils.py 293): INFO Epoch: [0]  [  100/13136]  eta: 3:44:51  lr_architecture: 0.010000  loss_cls: 7.1140 (7.3796)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1081 (1.4941)  time: 0.9944  data: 0.0001  max mem: 20364
[2024-01-21 15:59:36 root] (utils.py 293): INFO Epoch: [0]  [  110/13136]  eta: 3:43:54  lr_architecture: 0.010000  loss_cls: 7.0708 (7.3508)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1869 (1.4695)  time: 0.9941  data: 0.0001  max mem: 20364
[2024-01-21 16:00:00 root] (main_pitome.py 217): INFO Namespace(batch_size=24, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:14:25 root] (main_pitome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:14:26 root] (main_pitome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:14:38 root] (main_pitome.py 389): INFO number of params: 632045800
[2024-01-21 16:14:38 root] (main_pitome.py 435): INFO Start training for 2 epochs
[2024-01-21 16:14:43 root] (utils.py 293): INFO Epoch: [0]  [    0/13136]  eta: 16:56:32  lr_architecture: 0.010000  loss_cls: 2.7471 (2.7471)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 15.5667 (15.5667)  time: 4.6432  data: 0.0009  max mem: 14768
[2024-01-21 16:14:53 root] (utils.py 293): INFO Epoch: [0]  [   10/13136]  eta: 5:01:08  lr_architecture: 0.010000  loss_cls: 8.0154 (7.5101)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 2.0428 (5.1708)  time: 1.3765  data: 0.0002  max mem: 19643
[2024-01-21 16:15:03 root] (utils.py 293): INFO Epoch: [0]  [   20/13136]  eta: 4:24:51  lr_architecture: 0.010000  loss_cls: 7.8389 (7.6583)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.6930 (3.4404)  time: 1.0401  data: 0.0001  max mem: 19643
[2024-01-21 16:15:14 root] (utils.py 293): INFO Epoch: [0]  [   30/13136]  eta: 4:11:48  lr_architecture: 0.010000  loss_cls: 7.7376 (7.6441)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.3881 (2.7646)  time: 1.0298  data: 0.0001  max mem: 19643
[2024-01-21 16:15:24 root] (utils.py 293): INFO Epoch: [0]  [   40/13136]  eta: 4:04:53  lr_architecture: 0.010000  loss_cls: 7.4346 (7.5729)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.2956 (2.3996)  time: 1.0278  data: 0.0001  max mem: 19643
[2024-01-21 16:15:34 root] (utils.py 293): INFO Epoch: [0]  [   50/13136]  eta: 4:00:35  lr_architecture: 0.010000  loss_cls: 7.2253 (7.4915)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.0988 (2.1438)  time: 1.0260  data: 0.0001  max mem: 19643
[2024-01-21 16:16:03 root] (main_pitome.py 218): INFO Namespace(batch_size=20, epochs=2, ratio=0.9625, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:16:07 root] (main_pitome.py 263): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:16:08 root] (main_pitome.py 303): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:16:19 root] (main_pitome.py 390): INFO number of params: 632045800
[2024-01-21 16:16:29 root] (main_pitome.py 444): INFO Start training for 2 epochs
[2024-01-21 16:16:32 root] (utils.py 293): INFO Epoch: [0]  [    0/15763]  eta: 17:02:05  lr_architecture: 0.010000  loss_cls: 2.6664 (2.6664)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 14.2260 (14.2260)  time: 3.8904  data: 0.0006  max mem: 14771
[2024-01-21 16:16:43 root] (utils.py 293): INFO Epoch: [0]  [   10/15763]  eta: 5:42:54  lr_architecture: 0.010000  loss_cls: 7.7501 (7.5404)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 2.0620 (4.1439)  time: 1.3060  data: 0.0001  max mem: 19644
[2024-01-21 16:16:53 root] (utils.py 293): INFO Epoch: [0]  [   20/15763]  eta: 5:07:25  lr_architecture: 0.010000  loss_cls: 7.9887 (7.9191)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.9112 (3.1351)  time: 1.0357  data: 0.0001  max mem: 19644
[2024-01-21 16:17:03 root] (utils.py 293): INFO Epoch: [0]  [   30/15763]  eta: 4:54:40  lr_architecture: 0.010000  loss_cls: 7.9176 (7.8446)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.7437 (2.6213)  time: 1.0235  data: 0.0001  max mem: 19644
[2024-01-21 16:17:14 root] (utils.py 293): INFO Epoch: [0]  [   40/15763]  eta: 4:47:52  lr_architecture: 0.010000  loss_cls: 7.4575 (7.7213)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.3810 (2.2934)  time: 1.0217  data: 0.0001  max mem: 19644
[2024-01-21 16:17:24 root] (utils.py 293): INFO Epoch: [0]  [   50/15763]  eta: 4:43:43  lr_architecture: 0.010000  loss_cls: 7.2359 (7.6156)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1941 (2.0709)  time: 1.0209  data: 0.0001  max mem: 19644
[2024-01-21 16:17:34 root] (utils.py 293): INFO Epoch: [0]  [   60/15763]  eta: 4:40:54  lr_architecture: 0.010000  loss_cls: 7.1505 (7.5437)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1506 (1.9253)  time: 1.0216  data: 0.0001  max mem: 19644
[2024-01-21 16:17:44 root] (utils.py 293): INFO Epoch: [0]  [   70/15763]  eta: 4:38:52  lr_architecture: 0.010000  loss_cls: 7.1809 (7.4881)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1889 (1.8310)  time: 1.0225  data: 0.0001  max mem: 19644
[2024-01-21 16:17:54 root] (utils.py 293): INFO Epoch: [0]  [   80/15763]  eta: 4:37:14  lr_architecture: 0.010000  loss_cls: 7.1290 (7.4420)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2203 (1.7634)  time: 1.0222  data: 0.0001  max mem: 19644
[2024-01-21 16:18:05 root] (utils.py 293): INFO Epoch: [0]  [   90/15763]  eta: 4:35:55  lr_architecture: 0.010000  loss_cls: 7.1081 (7.4096)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2299 (1.7114)  time: 1.0211  data: 0.0001  max mem: 19644
[2024-01-21 16:18:15 root] (utils.py 293): INFO Epoch: [0]  [  100/15763]  eta: 4:34:50  lr_architecture: 0.010000  loss_cls: 7.1130 (7.3784)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2299 (1.6690)  time: 1.0211  data: 0.0001  max mem: 19644
[2024-01-21 16:18:25 root] (utils.py 293): INFO Epoch: [0]  [  110/15763]  eta: 4:33:56  lr_architecture: 0.010000  loss_cls: 7.1301 (7.3586)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2668 (1.6363)  time: 1.0215  data: 0.0001  max mem: 19644
[2024-01-21 16:18:59 root] (main_pitome.py 218): INFO Namespace(batch_size=20, epochs=2, ratio=0.9625, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:19:03 root] (main_pitome.py 263): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:19:04 root] (main_pitome.py 303): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:19:15 root] (main_pitome.py 390): INFO number of params: 632045800
[2024-01-21 16:19:15 root] (main_pitome.py 445): INFO Start training for 2 epochs
[2024-01-21 16:19:25 root] (utils.py 293): INFO Epoch: [0]  [    0/15763]  eta: 1 day, 19:17:14  lr_architecture: 0.010000  loss_cls: 2.6817 (2.6817)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 14.0616 (14.0616)  time: 9.8861  data: 0.0008  max mem: 14768
[2024-01-21 16:19:35 root] (utils.py 293): INFO Epoch: [0]  [   10/15763]  eta: 8:07:30  lr_architecture: 0.010000  loss_cls: 7.6421 (7.5728)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 2.0485 (3.9325)  time: 1.8568  data: 0.0002  max mem: 19639
[2024-01-21 16:19:46 root] (utils.py 293): INFO Epoch: [0]  [   20/15763]  eta: 6:24:15  lr_architecture: 0.010000  loss_cls: 7.8009 (7.6882)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.8645 (2.8990)  time: 1.0434  data: 0.0001  max mem: 19639
[2024-01-21 16:19:56 root] (utils.py 293): INFO Epoch: [0]  [   30/15763]  eta: 5:47:31  lr_architecture: 0.010000  loss_cls: 7.7271 (7.6987)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.5786 (2.4501)  time: 1.0330  data: 0.0001  max mem: 19639
[2024-01-21 16:20:06 root] (utils.py 293): INFO Epoch: [0]  [   40/15763]  eta: 5:28:25  lr_architecture: 0.010000  loss_cls: 7.5412 (7.6452)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.4175 (2.1774)  time: 1.0315  data: 0.0001  max mem: 19639
[2024-01-21 16:20:17 root] (utils.py 293): INFO Epoch: [0]  [   50/15763]  eta: 5:16:41  lr_architecture: 0.010000  loss_cls: 7.3142 (7.5583)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2673 (1.9831)  time: 1.0295  data: 0.0001  max mem: 19639
[2024-01-21 16:20:27 root] (utils.py 293): INFO Epoch: [0]  [   60/15763]  eta: 5:08:44  lr_architecture: 0.010000  loss_cls: 7.1826 (7.4897)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1838 (1.8522)  time: 1.0287  data: 0.0001  max mem: 19639
[2024-01-21 16:20:37 root] (utils.py 293): INFO Epoch: [0]  [   70/15763]  eta: 5:03:00  lr_architecture: 0.010000  loss_cls: 7.1478 (7.4470)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2471 (1.7765)  time: 1.0289  data: 0.0001  max mem: 19639
[2024-01-21 16:20:47 root] (utils.py 293): INFO Epoch: [0]  [   80/15763]  eta: 4:58:33  lr_architecture: 0.010000  loss_cls: 7.1478 (7.4086)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.3396 (1.7197)  time: 1.0280  data: 0.0001  max mem: 19639
[2024-01-21 16:20:55 root] (engine.py 57): INFO Loss is nan, stopping training
[2024-01-21 16:22:23 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.9625, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:22:26 root] (main_pitome.py 263): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:22:28 root] (main_pitome.py 303): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:22:38 root] (main_pitome.py 390): INFO number of params: 632045800
[2024-01-21 16:22:38 root] (main_pitome.py 445): INFO Start training for 2 epochs
[2024-01-21 16:22:48 root] (utils.py 293): INFO Epoch: [0]  [    0/19704]  eta: 2 days, 6:18:01  lr_architecture: 0.001000  loss_cls: 2.3463 (2.3463)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 14.7789 (14.7789)  time: 9.9209  data: 0.0011  max mem: 14758
[2024-01-21 16:22:58 root] (utils.py 293): INFO Epoch: [0]  [   10/19704]  eta: 10:01:15  lr_architecture: 0.001000  loss_cls: 7.2351 (6.9012)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 2.3027 (4.6950)  time: 1.8318  data: 0.0002  max mem: 18411
[2024-01-21 16:23:08 root] (utils.py 293): INFO Epoch: [0]  [   20/19704]  eta: 7:51:45  lr_architecture: 0.001000  loss_cls: 7.1748 (7.0053)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.6159 (3.1720)  time: 1.0139  data: 0.0001  max mem: 18411
[2024-01-21 16:23:19 root] (utils.py 293): INFO Epoch: [0]  [   30/19704]  eta: 7:05:37  lr_architecture: 0.001000  loss_cls: 7.0882 (7.0432)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.3119 (2.5580)  time: 1.0045  data: 0.0001  max mem: 18411
[2024-01-21 16:23:29 root] (utils.py 293): INFO Epoch: [0]  [   40/19704]  eta: 6:41:58  lr_architecture: 0.001000  loss_cls: 7.1634 (7.0730)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2424 (2.2288)  time: 1.0045  data: 0.0001  max mem: 18411
[2024-01-21 16:23:39 root] (utils.py 293): INFO Epoch: [0]  [   50/19704]  eta: 6:27:26  lr_architecture: 0.001000  loss_cls: 7.0840 (7.0704)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1594 (2.0134)  time: 1.0042  data: 0.0001  max mem: 18411
[2024-01-21 16:23:49 root] (utils.py 293): INFO Epoch: [0]  [   60/19704]  eta: 6:17:41  lr_architecture: 0.001000  loss_cls: 7.0652 (7.0712)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0767 (1.8573)  time: 1.0042  data: 0.0001  max mem: 18411
[2024-01-21 16:23:59 root] (utils.py 293): INFO Epoch: [0]  [   70/19704]  eta: 6:10:40  lr_architecture: 0.001000  loss_cls: 7.0752 (7.0732)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0410 (1.7443)  time: 1.0052  data: 0.0001  max mem: 18411
[2024-01-21 16:24:09 root] (utils.py 293): INFO Epoch: [0]  [   80/19704]  eta: 6:05:20  lr_architecture: 0.001000  loss_cls: 7.0171 (7.0600)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0112 (1.6537)  time: 1.0054  data: 0.0001  max mem: 18411
[2024-01-21 16:24:19 root] (utils.py 293): INFO Epoch: [0]  [   90/19704]  eta: 6:01:06  lr_architecture: 0.001000  loss_cls: 7.0183 (7.0633)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0026 (1.5835)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:24:29 root] (utils.py 293): INFO Epoch: [0]  [  100/19704]  eta: 5:57:41  lr_architecture: 0.001000  loss_cls: 7.0543 (7.0595)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9914 (1.5245)  time: 1.0045  data: 0.0001  max mem: 18411
[2024-01-21 16:24:39 root] (utils.py 293): INFO Epoch: [0]  [  110/19704]  eta: 5:54:50  lr_architecture: 0.001000  loss_cls: 6.9767 (7.0544)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9908 (1.4786)  time: 1.0046  data: 0.0001  max mem: 18411
[2024-01-21 16:24:49 root] (utils.py 293): INFO Epoch: [0]  [  120/19704]  eta: 5:52:28  lr_architecture: 0.001000  loss_cls: 6.9904 (7.0507)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9929 (1.4415)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:24:59 root] (utils.py 293): INFO Epoch: [0]  [  130/19704]  eta: 5:50:23  lr_architecture: 0.001000  loss_cls: 7.0571 (7.0535)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0061 (1.4093)  time: 1.0045  data: 0.0001  max mem: 18411
[2024-01-21 16:25:09 root] (utils.py 293): INFO Epoch: [0]  [  140/19704]  eta: 5:48:37  lr_architecture: 0.001000  loss_cls: 7.0393 (7.0516)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9721 (1.3761)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:25:19 root] (utils.py 293): INFO Epoch: [0]  [  150/19704]  eta: 5:47:04  lr_architecture: 0.001000  loss_cls: 7.0053 (7.0495)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9455 (1.3494)  time: 1.0053  data: 0.0001  max mem: 18411
[2024-01-21 16:25:29 root] (utils.py 293): INFO Epoch: [0]  [  160/19704]  eta: 5:45:41  lr_architecture: 0.001000  loss_cls: 6.9671 (7.0438)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9480 (1.3244)  time: 1.0055  data: 0.0001  max mem: 18411
[2024-01-21 16:25:39 root] (utils.py 293): INFO Epoch: [0]  [  170/19704]  eta: 5:44:28  lr_architecture: 0.001000  loss_cls: 6.9559 (7.0405)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9368 (1.3022)  time: 1.0061  data: 0.0001  max mem: 18411
[2024-01-21 16:25:49 root] (utils.py 293): INFO Epoch: [0]  [  180/19704]  eta: 5:43:21  lr_architecture: 0.001000  loss_cls: 6.9965 (7.0381)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9154 (1.2807)  time: 1.0061  data: 0.0001  max mem: 18411
[2024-01-21 16:25:59 root] (utils.py 293): INFO Epoch: [0]  [  190/19704]  eta: 5:42:21  lr_architecture: 0.001000  loss_cls: 6.9651 (7.0352)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9069 (1.2611)  time: 1.0062  data: 0.0001  max mem: 18411
[2024-01-21 16:26:09 root] (utils.py 293): INFO Epoch: [0]  [  200/19704]  eta: 5:41:26  lr_architecture: 0.001000  loss_cls: 6.9813 (7.0319)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9136 (1.2445)  time: 1.0066  data: 0.0001  max mem: 18411
[2024-01-21 16:26:19 root] (utils.py 293): INFO Epoch: [0]  [  210/19704]  eta: 5:40:34  lr_architecture: 0.001000  loss_cls: 6.9711 (7.0290)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9213 (1.2297)  time: 1.0063  data: 0.0001  max mem: 18411
[2024-01-21 16:26:30 root] (utils.py 293): INFO Epoch: [0]  [  220/19704]  eta: 5:39:47  lr_architecture: 0.001000  loss_cls: 6.9807 (7.0288)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9386 (1.2189)  time: 1.0061  data: 0.0001  max mem: 18411
[2024-01-21 16:26:40 root] (utils.py 293): INFO Epoch: [0]  [  230/19704]  eta: 5:39:02  lr_architecture: 0.001000  loss_cls: 7.0121 (7.0268)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9483 (1.2068)  time: 1.0059  data: 0.0001  max mem: 18411
[2024-01-21 16:26:50 root] (utils.py 293): INFO Epoch: [0]  [  240/19704]  eta: 5:38:20  lr_architecture: 0.001000  loss_cls: 6.9769 (7.0248)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9445 (1.1969)  time: 1.0055  data: 0.0001  max mem: 18411
[2024-01-21 16:27:00 root] (utils.py 293): INFO Epoch: [0]  [  250/19704]  eta: 5:37:41  lr_architecture: 0.001000  loss_cls: 6.9901 (7.0244)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9109 (1.1859)  time: 1.0057  data: 0.0001  max mem: 18411
[2024-01-21 16:28:14 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:28:46 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:29:49 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:30:45 root] (main_pitome.py 219): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:30:48 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:30:49 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:30:55 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:30:55 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:37:11 root] (main_pitome.py 219): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:37:15 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:37:16 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:37:21 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:37:21 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:39:33 root] (main_pitome.py 219): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:39:36 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:39:37 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:39:43 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:39:43 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:39:53 root] (utils.py 302): INFO Epoch: [0]  [    0/19704]  eta: 2 days, 6:55:40  lr_architecture: 0.001000  loss_cls: 2.4295 (2.4295)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 14.5901 (14.5901)  time: 10.0355  data: 0.0009  max mem: 6996
[2024-01-21 16:39:58 root] (utils.py 302): INFO Epoch: [0]  [   10/19704]  eta: 7:29:16  lr_architecture: 0.001000  loss_cls: 7.2034 (6.8373)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 2.4817 (5.9157)  time: 1.3688  data: 0.0001  max mem: 8651
[2024-01-21 16:40:03 root] (utils.py 302): INFO Epoch: [0]  [   20/19704]  eta: 5:11:40  lr_architecture: 0.001000  loss_cls: 7.1815 (6.9813)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.6502 (3.8500)  time: 0.4957  data: 0.0001  max mem: 8651
[2024-01-21 16:40:08 root] (utils.py 302): INFO Epoch: [0]  [   30/19704]  eta: 4:22:53  lr_architecture: 0.001000  loss_cls: 7.0559 (7.0283)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.4636 (3.0519)  time: 0.4899  data: 0.0001  max mem: 8651
[2024-01-21 16:40:13 root] (utils.py 302): INFO Epoch: [0]  [   40/19704]  eta: 3:57:55  lr_architecture: 0.001000  loss_cls: 7.1528 (7.0601)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.3467 (2.6464)  time: 0.4907  data: 0.0001  max mem: 8652
[2024-01-21 16:40:18 root] (utils.py 302): INFO Epoch: [0]  [   50/19704]  eta: 3:42:39  lr_architecture: 0.001000  loss_cls: 7.1381 (7.0683)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.1890 (2.3518)  time: 0.4906  data: 0.0001  max mem: 8652
[2024-01-21 16:40:23 root] (utils.py 302): INFO Epoch: [0]  [   60/19704]  eta: 3:32:24  lr_architecture: 0.001000  loss_cls: 7.0821 (7.0634)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0877 (2.1393)  time: 0.4905  data: 0.0001  max mem: 8652
[2024-01-21 16:40:28 root] (utils.py 302): INFO Epoch: [0]  [   70/19704]  eta: 3:24:58  lr_architecture: 0.001000  loss_cls: 7.0933 (7.0712)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0673 (1.9903)  time: 0.4904  data: 0.0001  max mem: 8652
[2024-01-21 16:40:33 root] (utils.py 302): INFO Epoch: [0]  [   80/19704]  eta: 3:19:24  lr_architecture: 0.001000  loss_cls: 7.0634 (7.0585)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0308 (1.8719)  time: 0.4906  data: 0.0001  max mem: 8652
[2024-01-21 16:40:37 root] (utils.py 302): INFO Epoch: [0]  [   90/19704]  eta: 3:15:01  lr_architecture: 0.001000  loss_cls: 6.9894 (7.0584)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0060 (1.7741)  time: 0.4907  data: 0.0001  max mem: 8652
[2024-01-21 16:40:42 root] (utils.py 302): INFO Epoch: [0]  [  100/19704]  eta: 3:11:29  lr_architecture: 0.001000  loss_cls: 7.0250 (7.0548)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 0.9568 (1.6908)  time: 0.4904  data: 0.0001  max mem: 8652
[2024-01-21 16:40:47 root] (utils.py 302): INFO Epoch: [0]  [  110/19704]  eta: 3:08:35  lr_architecture: 0.001000  loss_cls: 6.9897 (7.0485)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 0.9513 (1.6245)  time: 0.4907  data: 0.0001  max mem: 8652
[2024-01-21 16:40:52 root] (utils.py 302): INFO Epoch: [0]  [  120/19704]  eta: 3:06:08  lr_architecture: 0.001000  loss_cls: 6.9634 (7.0429)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 0.9818 (1.5726)  time: 0.4906  data: 0.0001  max mem: 8652
[2024-01-21 16:41:33 root] (main_pitome.py 219): INFO Namespace(batch_size=32, epochs=2, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:41:36 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:41:37 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:41:43 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:41:43 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:41:53 root] (utils.py 302): INFO Epoch: [0]  [   0/9852]  eta: 1 day, 2:42:35  lr_architecture: 0.010000  loss_cls: 2.5477 (2.5477)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 12.0292 (12.0292)  time: 9.7600  data: 0.0009  max mem: 7078
[2024-01-21 16:41:58 root] (utils.py 302): INFO Epoch: [0]  [  10/9852]  eta: 3:46:18  lr_architecture: 0.010000  loss_cls: 7.8892 (7.4136)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.8356 (3.0640)  time: 1.3796  data: 0.0002  max mem: 10516
[2024-01-21 16:42:03 root] (utils.py 302): INFO Epoch: [0]  [  20/9852]  eta: 2:39:35  lr_architecture: 0.010000  loss_cls: 7.8892 (7.6435)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.5667 (2.2947)  time: 0.5346  data: 0.0001  max mem: 10516
[2024-01-21 16:42:09 root] (utils.py 302): INFO Epoch: [0]  [  30/9852]  eta: 2:15:49  lr_architecture: 0.010000  loss_cls: 7.6202 (7.5758)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.1799 (1.8932)  time: 0.5273  data: 0.0001  max mem: 10516
[2024-01-21 16:42:14 root] (utils.py 302): INFO Epoch: [0]  [  40/9852]  eta: 2:03:36  lr_architecture: 0.010000  loss_cls: 7.2132 (7.4787)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.9142 (1.6448)  time: 0.5270  data: 0.0001  max mem: 10516
[2024-01-21 16:42:19 root] (utils.py 302): INFO Epoch: [0]  [  50/9852]  eta: 1:56:07  lr_architecture: 0.010000  loss_cls: 7.1531 (7.4097)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8550 (1.4894)  time: 0.5265  data: 0.0001  max mem: 10516
[2024-01-21 16:42:24 root] (utils.py 302): INFO Epoch: [0]  [  60/9852]  eta: 1:51:04  lr_architecture: 0.010000  loss_cls: 7.0910 (7.3560)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8443 (1.3881)  time: 0.5264  data: 0.0001  max mem: 10516
[2024-01-21 16:42:30 root] (utils.py 302): INFO Epoch: [0]  [  70/9852]  eta: 1:47:24  lr_architecture: 0.010000  loss_cls: 7.0882 (7.3190)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8859 (1.3188)  time: 0.5263  data: 0.0001  max mem: 10516
[2024-01-21 16:42:35 root] (utils.py 302): INFO Epoch: [0]  [  80/9852]  eta: 1:44:37  lr_architecture: 0.010000  loss_cls: 7.0882 (7.2902)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8728 (1.2655)  time: 0.5258  data: 0.0001  max mem: 10516
[2024-01-21 16:42:40 root] (utils.py 302): INFO Epoch: [0]  [  90/9852]  eta: 1:42:25  lr_architecture: 0.010000  loss_cls: 7.0566 (7.2613)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8262 (1.2171)  time: 0.5256  data: 0.0001  max mem: 10516
[2024-01-21 16:42:45 root] (utils.py 302): INFO Epoch: [0]  [ 100/9852]  eta: 1:40:39  lr_architecture: 0.010000  loss_cls: 7.0464 (7.2398)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8447 (1.1850)  time: 0.5256  data: 0.0001  max mem: 10516
[2024-01-21 16:43:23 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=3, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:43:27 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:43:29 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:43:35 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:43:35 root] (main_pitome.py 446): INFO Start training for 3 epochs
[2024-01-21 16:43:44 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:57:43  lr_architecture: 0.010000  loss_cls: 2.8164 (2.8164)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 9.5476 (9.5476)  time: 9.4729  data: 0.0006  max mem: 11145
[2024-01-21 16:43:50 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:57:32  lr_architecture: 0.010000  loss_cls: 7.7878 (7.3296)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.2086 (2.0786)  time: 1.4345  data: 0.0001  max mem: 14587
[2024-01-21 16:43:57 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:15  lr_architecture: 0.010000  loss_cls: 7.5172 (7.3724)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.0176 (1.4736)  time: 0.6211  data: 0.0001  max mem: 14587
[2024-01-21 16:44:03 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:13:45  lr_architecture: 0.010000  loss_cls: 7.1652 (7.2779)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6878 (1.1953)  time: 0.6120  data: 0.0001  max mem: 14587
[2024-01-21 16:44:09 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:07:45  lr_architecture: 0.010000  loss_cls: 7.0622 (7.2337)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5932 (1.0519)  time: 0.6112  data: 0.0001  max mem: 14587
[2024-01-21 16:44:15 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:05  lr_architecture: 0.010000  loss_cls: 7.0404 (7.1957)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6034 (0.9680)  time: 0.6099  data: 0.0001  max mem: 14587
[2024-01-21 16:44:21 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:01:34  lr_architecture: 0.010000  loss_cls: 7.0187 (7.1637)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5947 (0.9048)  time: 0.6098  data: 0.0001  max mem: 14587
[2024-01-21 16:44:27 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 0:59:44  lr_architecture: 0.010000  loss_cls: 6.9985 (7.1425)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5919 (0.8626)  time: 0.6096  data: 0.0001  max mem: 14587
[2024-01-21 16:44:33 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:21  lr_architecture: 0.010000  loss_cls: 7.0072 (7.1256)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5755 (0.8258)  time: 0.6103  data: 0.0001  max mem: 14587
[2024-01-21 16:44:39 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:13  lr_architecture: 0.010000  loss_cls: 7.0126 (7.1142)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5978 (0.8059)  time: 0.6100  data: 0.0001  max mem: 14587
[2024-01-21 16:44:45 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:19  lr_architecture: 0.010000  loss_cls: 7.0067 (7.1023)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6228 (0.7881)  time: 0.6103  data: 0.0001  max mem: 14587
[2024-01-21 16:44:51 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:55:33  lr_architecture: 0.010000  loss_cls: 6.9971 (7.0959)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6215 (0.7723)  time: 0.6105  data: 0.0001  max mem: 14587
[2024-01-21 16:44:58 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:54:53  lr_architecture: 0.010000  loss_cls: 7.0186 (7.0887)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5993 (0.7575)  time: 0.6100  data: 0.0001  max mem: 14587
[2024-01-21 16:45:04 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:19  lr_architecture: 0.010000  loss_cls: 6.9824 (7.0804)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5901 (0.7444)  time: 0.6101  data: 0.0001  max mem: 14587
[2024-01-21 16:45:10 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:53:48  lr_architecture: 0.010000  loss_cls: 6.9824 (7.0762)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6096 (0.7388)  time: 0.6093  data: 0.0001  max mem: 14587
[2024-01-21 16:45:16 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:21  lr_architecture: 0.010000  loss_cls: 7.0096 (7.0712)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6405 (0.7313)  time: 0.6098  data: 0.0001  max mem: 14587
[2024-01-21 16:45:22 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:52:57  lr_architecture: 0.010000  loss_cls: 7.0014 (7.0678)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6468 (0.7292)  time: 0.6110  data: 0.0001  max mem: 14587
[2024-01-21 16:45:28 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:52:35  lr_architecture: 0.010000  loss_cls: 6.9814 (7.0628)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6568 (0.7253)  time: 0.6110  data: 0.0001  max mem: 14587
[2024-01-21 16:45:34 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:14  lr_architecture: 0.010000  loss_cls: 6.9814 (7.0596)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6547 (0.7218)  time: 0.6102  data: 0.0001  max mem: 14587
[2024-01-21 16:45:40 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:51:55  lr_architecture: 0.010000  loss_cls: 7.0038 (7.0565)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6482 (0.7182)  time: 0.6100  data: 0.0001  max mem: 14587
[2024-01-21 16:45:46 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:51:37  lr_architecture: 0.010000  loss_cls: 7.0025 (7.0542)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6583 (0.7165)  time: 0.6103  data: 0.0001  max mem: 14587
[2024-01-21 16:45:53 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:21  lr_architecture: 0.010000  loss_cls: 6.9924 (7.0506)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6292 (0.7114)  time: 0.6105  data: 0.0001  max mem: 14587
[2024-01-21 16:45:59 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:05  lr_architecture: 0.010000  loss_cls: 6.9665 (7.0476)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6292 (0.7089)  time: 0.6104  data: 0.0001  max mem: 14587
[2024-01-21 16:46:05 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:50:50  lr_architecture: 0.010000  loss_cls: 6.9675 (7.0453)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6738 (0.7082)  time: 0.6105  data: 0.0001  max mem: 14587
[2024-01-21 16:46:11 root] (utils.py 302): INFO Epoch: [0]  [ 240/4926]  eta: 0:50:36  lr_architecture: 0.010000  loss_cls: 6.9803 (7.0428)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6858 (0.7072)  time: 0.6112  data: 0.0001  max mem: 14587
[2024-01-21 16:46:50 root] (main_pitome.py 219): INFO Namespace(batch_size=128, epochs=5, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:46:53 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:46:54 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:47:03 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:47:03 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:47:13 root] (utils.py 302): INFO Epoch: [0]  [   0/2463]  eta: 6:42:54  lr_architecture: 0.010000  loss_cls: 2.7129 (2.7129)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 5.5643 (5.5643)  time: 9.8149  data: 0.0011  max mem: 19309
[2024-01-21 16:47:41 root] (main_pitome.py 219): INFO Namespace(batch_size=80, epochs=5, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:47:45 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:47:46 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:47:53 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:47:53 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:48:02 root] (utils.py 302): INFO Epoch: [0]  [   0/3940]  eta: 10:18:28  lr_architecture: 0.010000  loss_cls: 2.7788 (2.7788)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 7.8372 (7.8372)  time: 9.4183  data: 0.0008  max mem: 13172
[2024-01-21 16:48:09 root] (utils.py 302): INFO Epoch: [0]  [  10/3940]  eta: 1:37:09  lr_architecture: 0.010000  loss_cls: 7.4851 (7.1980)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.1815 (1.9207)  time: 1.4833  data: 0.0002  max mem: 16611
[2024-01-21 16:48:16 root] (utils.py 302): INFO Epoch: [0]  [  20/3940]  eta: 1:11:25  lr_architecture: 0.010000  loss_cls: 7.4778 (7.3194)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.9072 (1.3803)  time: 0.6769  data: 0.0001  max mem: 16611
[2024-01-21 16:48:22 root] (utils.py 302): INFO Epoch: [0]  [  30/3940]  eta: 1:02:13  lr_architecture: 0.010000  loss_cls: 7.1908 (7.2571)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6089 (1.1205)  time: 0.6643  data: 0.0001  max mem: 16611
[2024-01-21 16:48:29 root] (utils.py 302): INFO Epoch: [0]  [  40/3940]  eta: 0:57:29  lr_architecture: 0.010000  loss_cls: 7.0584 (7.2072)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5684 (0.9822)  time: 0.6655  data: 0.0001  max mem: 16611
[2024-01-21 16:48:35 root] (utils.py 302): INFO Epoch: [0]  [  50/3940]  eta: 0:54:33  lr_architecture: 0.010000  loss_cls: 7.0286 (7.1682)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5125 (0.8878)  time: 0.6656  data: 0.0001  max mem: 16611
[2024-01-21 16:48:42 root] (utils.py 302): INFO Epoch: [0]  [  60/3940]  eta: 0:52:32  lr_architecture: 0.010000  loss_cls: 7.0059 (7.1415)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5004 (0.8275)  time: 0.6646  data: 0.0001  max mem: 16611
[2024-01-21 16:48:49 root] (utils.py 302): INFO Epoch: [0]  [  70/3940]  eta: 0:51:04  lr_architecture: 0.010000  loss_cls: 7.0059 (7.1206)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5141 (0.7836)  time: 0.6650  data: 0.0001  max mem: 16611
[2024-01-21 16:48:55 root] (utils.py 302): INFO Epoch: [0]  [  80/3940]  eta: 0:49:55  lr_architecture: 0.010000  loss_cls: 6.9712 (7.1049)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5245 (0.7501)  time: 0.6652  data: 0.0001  max mem: 16611
[2024-01-21 16:49:02 root] (utils.py 302): INFO Epoch: [0]  [  90/3940]  eta: 0:49:01  lr_architecture: 0.010000  loss_cls: 6.9883 (7.0924)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5218 (0.7254)  time: 0.6657  data: 0.0001  max mem: 16611
[2024-01-21 16:49:09 root] (utils.py 302): INFO Epoch: [0]  [ 100/3940]  eta: 0:48:16  lr_architecture: 0.010000  loss_cls: 6.9883 (7.0825)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5230 (0.7064)  time: 0.6660  data: 0.0001  max mem: 16611
[2024-01-21 16:49:15 root] (utils.py 302): INFO Epoch: [0]  [ 110/3940]  eta: 0:47:38  lr_architecture: 0.010000  loss_cls: 6.9813 (7.0736)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5271 (0.6889)  time: 0.6659  data: 0.0001  max mem: 16611
[2024-01-21 16:49:22 root] (utils.py 302): INFO Epoch: [0]  [ 120/3940]  eta: 0:47:05  lr_architecture: 0.010000  loss_cls: 6.9923 (7.0681)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5311 (0.6777)  time: 0.6657  data: 0.0001  max mem: 16611
[2024-01-21 16:49:29 root] (utils.py 302): INFO Epoch: [0]  [ 130/3940]  eta: 0:46:36  lr_architecture: 0.010000  loss_cls: 6.9927 (7.0614)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5357 (0.6670)  time: 0.6650  data: 0.0001  max mem: 16611
[2024-01-21 16:49:35 root] (utils.py 302): INFO Epoch: [0]  [ 140/3940]  eta: 0:46:10  lr_architecture: 0.010000  loss_cls: 6.9800 (7.0567)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5181 (0.6600)  time: 0.6657  data: 0.0001  max mem: 16611
[2024-01-21 16:49:42 root] (utils.py 302): INFO Epoch: [0]  [ 150/3940]  eta: 0:45:47  lr_architecture: 0.010000  loss_cls: 6.9905 (7.0522)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5398 (0.6537)  time: 0.6664  data: 0.0001  max mem: 16611
[2024-01-21 16:49:49 root] (utils.py 302): INFO Epoch: [0]  [ 160/3940]  eta: 0:45:27  lr_architecture: 0.010000  loss_cls: 6.9982 (7.0479)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5398 (0.6481)  time: 0.6671  data: 0.0001  max mem: 16611
[2024-01-21 16:49:55 root] (utils.py 302): INFO Epoch: [0]  [ 170/3940]  eta: 0:45:07  lr_architecture: 0.010000  loss_cls: 6.9816 (7.0442)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5600 (0.6432)  time: 0.6674  data: 0.0001  max mem: 16611
[2024-01-21 16:50:02 root] (utils.py 302): INFO Epoch: [0]  [ 180/3940]  eta: 0:44:50  lr_architecture: 0.010000  loss_cls: 6.9816 (7.0410)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5694 (0.6411)  time: 0.6673  data: 0.0001  max mem: 16611
[2024-01-21 16:50:09 root] (utils.py 302): INFO Epoch: [0]  [ 190/3940]  eta: 0:44:33  lr_architecture: 0.010000  loss_cls: 6.9847 (7.0388)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5969 (0.6390)  time: 0.6666  data: 0.0001  max mem: 16611
[2024-01-21 16:55:52 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:55:56 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:55:58 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:56:04 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:56:04 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:56:13 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:53:31  lr_architecture: 0.010000  loss_cls: 2.8427 (2.8427)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 8.7332 (8.7332)  time: 9.4218  data: 0.0007  max mem: 11145
[2024-01-21 16:56:20 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:58:27  lr_architecture: 0.010000  loss_cls: 7.6544 (7.2634)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.2290 (2.5742)  time: 1.4459  data: 0.0001  max mem: 14587
[2024-01-21 16:56:26 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:26:00  lr_architecture: 0.010000  loss_cls: 7.5129 (7.3712)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.0066 (1.7664)  time: 0.6333  data: 0.0001  max mem: 14587
[2024-01-21 16:56:32 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:21  lr_architecture: 0.010000  loss_cls: 7.2674 (7.3040)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.7483 (1.4092)  time: 0.6173  data: 0.0001  max mem: 14587
[2024-01-21 16:56:38 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:19  lr_architecture: 0.010000  loss_cls: 7.1299 (7.2530)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6479 (1.2223)  time: 0.6157  data: 0.0001  max mem: 14587
[2024-01-21 16:56:44 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:36  lr_architecture: 0.010000  loss_cls: 7.0322 (7.2083)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6165 (1.1017)  time: 0.6147  data: 0.0001  max mem: 14587
[2024-01-21 16:56:51 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:05  lr_architecture: 0.010000  loss_cls: 7.0143 (7.1752)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5763 (1.0137)  time: 0.6149  data: 0.0001  max mem: 14587
[2024-01-21 16:56:57 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:15  lr_architecture: 0.010000  loss_cls: 6.9981 (7.1510)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5724 (0.9545)  time: 0.6160  data: 0.0001  max mem: 14587
[2024-01-21 16:57:03 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:51  lr_architecture: 0.010000  loss_cls: 6.9985 (7.1337)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5724 (0.9083)  time: 0.6165  data: 0.0001  max mem: 14587
[2024-01-21 16:57:09 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:45  lr_architecture: 0.010000  loss_cls: 7.0059 (7.1204)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5978 (0.8753)  time: 0.6171  data: 0.0001  max mem: 14587
[2024-01-21 16:57:15 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:51  lr_architecture: 0.010000  loss_cls: 6.9944 (7.1081)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6027 (0.8503)  time: 0.6182  data: 0.0001  max mem: 14587
[2024-01-21 16:57:22 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:56:06  lr_architecture: 0.010000  loss_cls: 7.0086 (7.0999)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5893 (0.8274)  time: 0.6187  data: 0.0001  max mem: 14587
[2024-01-21 16:57:28 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:27  lr_architecture: 0.010000  loss_cls: 7.0097 (7.0924)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6008 (0.8094)  time: 0.6190  data: 0.0001  max mem: 14587
[2024-01-21 16:57:34 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:52  lr_architecture: 0.010000  loss_cls: 6.9939 (7.0829)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6098 (0.7932)  time: 0.6179  data: 0.0001  max mem: 14587
[2024-01-21 16:57:40 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:22  lr_architecture: 0.010000  loss_cls: 6.9856 (7.0781)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6150 (0.7830)  time: 0.6176  data: 0.0001  max mem: 14587
[2024-01-21 16:57:46 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:55  lr_architecture: 0.010000  loss_cls: 6.9931 (7.0732)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6277 (0.7720)  time: 0.6184  data: 0.0001  max mem: 14587
[2024-01-21 16:57:52 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:31  lr_architecture: 0.010000  loss_cls: 6.9965 (7.0691)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6486 (0.7663)  time: 0.6187  data: 0.0001  max mem: 14587
[2024-01-21 16:57:59 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:53:09  lr_architecture: 0.010000  loss_cls: 6.9865 (7.0639)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6558 (0.7601)  time: 0.6184  data: 0.0001  max mem: 14587
[2024-01-21 16:58:05 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:48  lr_architecture: 0.010000  loss_cls: 6.9865 (7.0602)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6606 (0.7542)  time: 0.6177  data: 0.0001  max mem: 14587
[2024-01-21 16:58:11 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:30  lr_architecture: 0.010000  loss_cls: 7.0033 (7.0571)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6469 (0.7486)  time: 0.6183  data: 0.0001  max mem: 14587
[2024-01-21 16:58:17 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:52:12  lr_architecture: 0.010000  loss_cls: 7.0026 (7.0542)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6659 (0.7464)  time: 0.6182  data: 0.0001  max mem: 14587
[2024-01-21 16:58:23 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:55  lr_architecture: 0.010000  loss_cls: 6.9851 (7.0510)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6335 (0.7410)  time: 0.6172  data: 0.0001  max mem: 14587
[2024-01-21 16:58:30 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:39  lr_architecture: 0.010000  loss_cls: 6.9698 (7.0476)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6337 (0.7367)  time: 0.6180  data: 0.0001  max mem: 14587
[2024-01-21 16:58:36 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:51:25  lr_architecture: 0.010000  loss_cls: 6.9678 (7.0449)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6513 (0.7340)  time: 0.6189  data: 0.0001  max mem: 14587
[2024-01-21 16:59:18 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 16:59:21 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:59:23 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:59:29 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:59:29 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:59:39 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:59:32  lr_architecture: 0.010000  loss_cls: 2.8870 (2.8870)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.4229 (9.4229)  time: 9.4950  data: 0.0010  max mem: 10987
[2024-01-21 16:59:45 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:59:05  lr_architecture: 0.010000  loss_cls: 7.7262 (7.3014)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 1.1803 (2.0192)  time: 1.4536  data: 0.0002  max mem: 14435
[2024-01-21 16:59:51 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:26:10  lr_architecture: 0.010000  loss_cls: 7.4597 (7.3389)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.9764 (1.4387)  time: 0.6319  data: 0.0001  max mem: 14435
[2024-01-21 16:59:58 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:24  lr_architecture: 0.010000  loss_cls: 7.1618 (7.2581)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6895 (1.1816)  time: 0.6138  data: 0.0001  max mem: 14435
[2024-01-21 17:00:04 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:20  lr_architecture: 0.010000  loss_cls: 7.0591 (7.2108)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6252 (1.0374)  time: 0.6137  data: 0.0001  max mem: 14435
[2024-01-21 17:00:10 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:36  lr_architecture: 0.010000  loss_cls: 7.0418 (7.1764)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5731 (0.9519)  time: 0.6140  data: 0.0001  max mem: 14435
[2024-01-21 17:00:16 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:03  lr_architecture: 0.010000  loss_cls: 7.0321 (7.1491)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5753 (0.8909)  time: 0.6135  data: 0.0001  max mem: 14435
[2024-01-21 17:00:22 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:13  lr_architecture: 0.010000  loss_cls: 6.9854 (7.1272)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5796 (0.8483)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:00:28 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:48  lr_architecture: 0.010000  loss_cls: 7.0051 (7.1141)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5887 (0.8158)  time: 0.6151  data: 0.0001  max mem: 14435
[2024-01-21 17:00:34 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:40  lr_architecture: 0.010000  loss_cls: 7.0159 (7.1029)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5943 (0.7935)  time: 0.6141  data: 0.0001  max mem: 14435
[2024-01-21 17:00:41 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:45  lr_architecture: 0.010000  loss_cls: 7.0063 (7.0931)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6133 (0.7774)  time: 0.6146  data: 0.0001  max mem: 14435
[2024-01-21 17:00:47 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:55:58  lr_architecture: 0.010000  loss_cls: 7.0065 (7.0863)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6184 (0.7631)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:00:53 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:17  lr_architecture: 0.010000  loss_cls: 7.0161 (7.0810)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6203 (0.7528)  time: 0.6131  data: 0.0001  max mem: 14435
[2024-01-21 17:00:59 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:42  lr_architecture: 0.010000  loss_cls: 6.9977 (7.0724)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5944 (0.7393)  time: 0.6124  data: 0.0001  max mem: 14435
[2024-01-21 17:01:05 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:11  lr_architecture: 0.010000  loss_cls: 6.9785 (7.0684)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5944 (0.7343)  time: 0.6124  data: 0.0001  max mem: 14435
[2024-01-21 17:01:11 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:43  lr_architecture: 0.010000  loss_cls: 7.0164 (7.0647)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6336 (0.7273)  time: 0.6132  data: 0.0001  max mem: 14435
[2024-01-21 17:01:17 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:19  lr_architecture: 0.010000  loss_cls: 7.0164 (7.0609)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6168 (0.7225)  time: 0.6143  data: 0.0001  max mem: 14435
[2024-01-21 17:01:24 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:52:56  lr_architecture: 0.010000  loss_cls: 7.0102 (7.0563)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6369 (0.7186)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:30 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:35  lr_architecture: 0.010000  loss_cls: 6.9972 (7.0527)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6385 (0.7139)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:36 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:16  lr_architecture: 0.010000  loss_cls: 6.9972 (7.0503)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6392 (0.7104)  time: 0.6149  data: 0.0001  max mem: 14435
[2024-01-21 17:01:42 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:51:58  lr_architecture: 0.010000  loss_cls: 6.9820 (7.0472)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6539 (0.7093)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:48 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:42  lr_architecture: 0.010000  loss_cls: 6.9796 (7.0445)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6626 (0.7069)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:54 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:26  lr_architecture: 0.010000  loss_cls: 6.9708 (7.0410)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6700 (0.7048)  time: 0.6146  data: 0.0001  max mem: 14435
[2024-01-21 17:01:57 root] (engine.py 60): INFO Loss is nan, stopping training
[2024-01-21 17:07:01 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 17:07:05 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:07:07 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:07:13 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 17:07:13 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 17:08:29 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 17:08:32 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:08:34 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:08:40 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 17:08:40 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 17:08:50 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:49:27  lr_architecture: 0.010000  loss_cls: 2.8947 (2.8947)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 8.7347 (8.7347)  time: 9.3723  data: 0.0009  max mem: 10987
[2024-01-21 17:08:56 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:56:59  lr_architecture: 0.010000  loss_cls: 7.8298 (7.3970)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 1.2888 (2.1405)  time: 1.4280  data: 0.0002  max mem: 14435
[2024-01-21 17:09:02 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:01  lr_architecture: 0.010000  loss_cls: 7.7093 (7.4949)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 1.0581 (1.5520)  time: 0.6232  data: 0.0001  max mem: 14435
[2024-01-21 17:09:08 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:13:37  lr_architecture: 0.010000  loss_cls: 7.2985 (7.3856)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.7337 (1.2633)  time: 0.6130  data: 0.0001  max mem: 14435
[2024-01-21 17:09:15 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:07:44  lr_architecture: 0.010000  loss_cls: 7.0634 (7.3117)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6310 (1.1065)  time: 0.6136  data: 0.0001  max mem: 14435
[2024-01-21 17:09:21 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:05  lr_architecture: 0.010000  loss_cls: 7.0320 (7.2532)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5784 (0.9989)  time: 0.6128  data: 0.0001  max mem: 14435
[2024-01-21 17:09:27 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:01:38  lr_architecture: 0.010000  loss_cls: 7.0023 (7.2104)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5475 (0.9240)  time: 0.6128  data: 0.0001  max mem: 14435
[2024-01-21 17:09:33 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 0:59:49  lr_architecture: 0.010000  loss_cls: 6.9866 (7.1803)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5477 (0.8724)  time: 0.6132  data: 0.0001  max mem: 14435
[2024-01-21 17:09:39 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:26  lr_architecture: 0.010000  loss_cls: 6.9920 (7.1577)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5538 (0.8330)  time: 0.6126  data: 0.0001  max mem: 14435
[2024-01-21 17:09:45 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:21  lr_architecture: 0.010000  loss_cls: 7.0125 (7.1423)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5538 (0.8083)  time: 0.6138  data: 0.0001  max mem: 14435
[2024-01-21 17:09:51 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:26  lr_architecture: 0.010000  loss_cls: 7.0099 (7.1278)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5798 (0.7882)  time: 0.6133  data: 0.0001  max mem: 14435
[2024-01-21 17:09:54 root] (engine.py 60): INFO Loss is nan, stopping training
[2024-01-21 17:40:55 root] (main_pitome.py 215): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 17:40:59 root] (main_pitome.py 260): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:41:00 root] (main_pitome.py 300): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:41:07 root] (main_pitome.py 386): INFO number of params: 304326632
[2024-01-21 17:41:07 root] (main_pitome.py 441): INFO Start training for 5 epochs
[2024-01-21 17:41:16 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:59:59  lr_architecture: 0.000250  loss_cls: 2.9281 (2.9281)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.1179 (9.1179)  time: 9.5005  data: 0.0007  max mem: 10987
[2024-01-21 17:41:23 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:58:38  lr_architecture: 0.000250  loss_cls: 3.5184 (3.5336)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.0908 (6.7913)  time: 1.4480  data: 0.0002  max mem: 14435
[2024-01-21 17:41:29 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:26:05  lr_architecture: 0.000250  loss_cls: 3.5184 (3.4595)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2937 (6.0860)  time: 0.6305  data: 0.0001  max mem: 14435
[2024-01-21 17:41:35 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:28  lr_architecture: 0.000250  loss_cls: 3.4079 (3.4057)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9420 (5.7617)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:41:41 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:26  lr_architecture: 0.000250  loss_cls: 3.3877 (3.3740)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2213 (5.6275)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:41:48 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:43  lr_architecture: 0.000250  loss_cls: 3.3877 (3.3804)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8586 (5.4696)  time: 0.6166  data: 0.0001  max mem: 14435
[2024-01-21 17:41:54 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:11  lr_architecture: 0.000250  loss_cls: 3.4794 (3.3859)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8267 (5.3690)  time: 0.6162  data: 0.0001  max mem: 14435
[2024-01-21 17:42:00 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:21  lr_architecture: 0.000250  loss_cls: 3.6034 (3.4102)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8266 (5.2887)  time: 0.6163  data: 0.0001  max mem: 14435
[2024-01-21 17:42:06 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:56  lr_architecture: 0.000250  loss_cls: 3.3609 (3.3910)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8266 (5.2455)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:42:12 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:49  lr_architecture: 0.000250  loss_cls: 3.2980 (3.3713)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6682 (5.1779)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:42:18 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:54  lr_architecture: 0.000250  loss_cls: 3.4480 (3.3958)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6899 (5.1399)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:42:25 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:56:09  lr_architecture: 0.000250  loss_cls: 3.3978 (3.3579)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7572 (5.0956)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:42:31 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:29  lr_architecture: 0.000250  loss_cls: 3.0882 (3.3584)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7733 (5.0728)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:42:37 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:55  lr_architecture: 0.000250  loss_cls: 3.4864 (3.3670)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7034 (5.0340)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:42:43 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:24  lr_architecture: 0.000250  loss_cls: 3.4864 (3.3651)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.4624 (4.9954)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:42:49 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:57  lr_architecture: 0.000250  loss_cls: 3.5066 (3.3804)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.4797 (4.9662)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:42:55 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:33  lr_architecture: 0.000250  loss_cls: 3.5066 (3.3754)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.4463 (4.9276)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:43:02 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:53:11  lr_architecture: 0.000250  loss_cls: 3.4001 (3.3779)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2543 (4.8925)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:43:08 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:50  lr_architecture: 0.000250  loss_cls: 3.5791 (3.3792)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2543 (4.8659)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:43:14 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:31  lr_architecture: 0.000250  loss_cls: 3.5868 (3.3918)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3014 (4.8381)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:43:20 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:52:13  lr_architecture: 0.000250  loss_cls: 3.5141 (3.3746)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3791 (4.8190)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:43:26 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:57  lr_architecture: 0.000250  loss_cls: 3.1553 (3.3646)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3336 (4.7891)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:43:33 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:41  lr_architecture: 0.000250  loss_cls: 3.3543 (3.3681)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1175 (4.7570)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:43:39 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:51:26  lr_architecture: 0.000250  loss_cls: 3.5516 (3.3672)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0804 (4.7331)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:43:45 root] (utils.py 302): INFO Epoch: [0]  [ 240/4926]  eta: 0:51:12  lr_architecture: 0.000250  loss_cls: 3.4152 (3.3631)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2741 (4.7161)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:43:51 root] (utils.py 302): INFO Epoch: [0]  [ 250/4926]  eta: 0:50:59  lr_architecture: 0.000250  loss_cls: 3.5388 (3.3665)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3100 (4.7060)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:43:57 root] (utils.py 302): INFO Epoch: [0]  [ 260/4926]  eta: 0:50:46  lr_architecture: 0.000250  loss_cls: 3.4813 (3.3596)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2500 (4.6867)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:44:03 root] (utils.py 302): INFO Epoch: [0]  [ 270/4926]  eta: 0:50:33  lr_architecture: 0.000250  loss_cls: 3.4740 (3.3642)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1848 (4.6718)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:44:10 root] (utils.py 302): INFO Epoch: [0]  [ 280/4926]  eta: 0:50:21  lr_architecture: 0.000250  loss_cls: 3.4740 (3.3657)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1996 (4.6519)  time: 0.6191  data: 0.0001  max mem: 14435
[2024-01-21 17:44:16 root] (utils.py 302): INFO Epoch: [0]  [ 290/4926]  eta: 0:50:10  lr_architecture: 0.000250  loss_cls: 3.3361 (3.3601)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1996 (4.6436)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:44:22 root] (utils.py 302): INFO Epoch: [0]  [ 300/4926]  eta: 0:49:59  lr_architecture: 0.000250  loss_cls: 3.0011 (3.3542)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1383 (4.6274)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:44:28 root] (utils.py 302): INFO Epoch: [0]  [ 310/4926]  eta: 0:49:48  lr_architecture: 0.000250  loss_cls: 3.3449 (3.3583)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1383 (4.6173)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:44:34 root] (utils.py 302): INFO Epoch: [0]  [ 320/4926]  eta: 0:49:37  lr_architecture: 0.000250  loss_cls: 3.3622 (3.3629)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3412 (4.6076)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:44:41 root] (utils.py 302): INFO Epoch: [0]  [ 330/4926]  eta: 0:49:27  lr_architecture: 0.000250  loss_cls: 3.2982 (3.3584)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3926 (4.5995)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:44:47 root] (utils.py 302): INFO Epoch: [0]  [ 340/4926]  eta: 0:49:17  lr_architecture: 0.000250  loss_cls: 3.3948 (3.3661)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0816 (4.5827)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:44:53 root] (utils.py 302): INFO Epoch: [0]  [ 350/4926]  eta: 0:49:07  lr_architecture: 0.000250  loss_cls: 3.5792 (3.3671)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0780 (4.5714)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:44:59 root] (utils.py 302): INFO Epoch: [0]  [ 360/4926]  eta: 0:48:57  lr_architecture: 0.000250  loss_cls: 3.5430 (3.3696)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2127 (4.5629)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:45:05 root] (utils.py 302): INFO Epoch: [0]  [ 370/4926]  eta: 0:48:47  lr_architecture: 0.000250  loss_cls: 3.4168 (3.3658)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1301 (4.5478)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:45:12 root] (utils.py 302): INFO Epoch: [0]  [ 380/4926]  eta: 0:48:38  lr_architecture: 0.000250  loss_cls: 3.3842 (3.3638)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0326 (4.5363)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:45:18 root] (utils.py 302): INFO Epoch: [0]  [ 390/4926]  eta: 0:48:29  lr_architecture: 0.000250  loss_cls: 3.2996 (3.3614)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0326 (4.5250)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:45:24 root] (utils.py 302): INFO Epoch: [0]  [ 400/4926]  eta: 0:48:20  lr_architecture: 0.000250  loss_cls: 3.5241 (3.3643)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0574 (4.5138)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:45:30 root] (utils.py 302): INFO Epoch: [0]  [ 410/4926]  eta: 0:48:11  lr_architecture: 0.000250  loss_cls: 3.5739 (3.3588)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1395 (4.5044)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:45:36 root] (utils.py 302): INFO Epoch: [0]  [ 420/4926]  eta: 0:48:02  lr_architecture: 0.000250  loss_cls: 3.6394 (3.3653)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0826 (4.4948)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:45:42 root] (utils.py 302): INFO Epoch: [0]  [ 430/4926]  eta: 0:47:54  lr_architecture: 0.000250  loss_cls: 3.6855 (3.3664)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0192 (4.4848)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:45:49 root] (utils.py 302): INFO Epoch: [0]  [ 440/4926]  eta: 0:47:45  lr_architecture: 0.000250  loss_cls: 3.5024 (3.3666)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9928 (4.4721)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:45:55 root] (utils.py 302): INFO Epoch: [0]  [ 450/4926]  eta: 0:47:37  lr_architecture: 0.000250  loss_cls: 3.4771 (3.3681)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7533 (4.4552)  time: 0.6194  data: 0.0001  max mem: 14435
[2024-01-21 17:46:01 root] (utils.py 302): INFO Epoch: [0]  [ 460/4926]  eta: 0:47:29  lr_architecture: 0.000250  loss_cls: 3.4280 (3.3676)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7370 (4.4458)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:46:07 root] (utils.py 302): INFO Epoch: [0]  [ 470/4926]  eta: 0:47:20  lr_architecture: 0.000250  loss_cls: 3.3401 (3.3687)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9635 (4.4350)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:46:13 root] (utils.py 302): INFO Epoch: [0]  [ 480/4926]  eta: 0:47:12  lr_architecture: 0.000250  loss_cls: 3.3054 (3.3708)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9889 (4.4305)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:46:20 root] (utils.py 302): INFO Epoch: [0]  [ 490/4926]  eta: 0:47:04  lr_architecture: 0.000250  loss_cls: 3.3649 (3.3721)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1521 (4.4282)  time: 0.6195  data: 0.0001  max mem: 14435
[2024-01-21 17:46:26 root] (utils.py 302): INFO Epoch: [0]  [ 500/4926]  eta: 0:46:56  lr_architecture: 0.000250  loss_cls: 3.4603 (3.3740)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0871 (4.4180)  time: 0.6190  data: 0.0001  max mem: 14435
[2024-01-21 17:46:32 root] (utils.py 302): INFO Epoch: [0]  [ 510/4926]  eta: 0:46:48  lr_architecture: 0.000250  loss_cls: 3.3946 (3.3691)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7761 (4.4069)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:46:38 root] (utils.py 302): INFO Epoch: [0]  [ 520/4926]  eta: 0:46:41  lr_architecture: 0.000250  loss_cls: 3.3946 (3.3731)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8723 (4.3979)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:46:44 root] (utils.py 302): INFO Epoch: [0]  [ 530/4926]  eta: 0:46:34  lr_architecture: 0.000250  loss_cls: 3.2967 (3.3678)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8621 (4.3888)  time: 0.6249  data: 0.0001  max mem: 14435
[2024-01-21 17:46:51 root] (utils.py 302): INFO Epoch: [0]  [ 540/4926]  eta: 0:46:26  lr_architecture: 0.000250  loss_cls: 3.2762 (3.3673)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9374 (4.3807)  time: 0.6248  data: 0.0001  max mem: 14435
[2024-01-21 17:46:57 root] (utils.py 302): INFO Epoch: [0]  [ 550/4926]  eta: 0:46:18  lr_architecture: 0.000250  loss_cls: 3.4103 (3.3673)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9464 (4.3732)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:47:03 root] (utils.py 302): INFO Epoch: [0]  [ 560/4926]  eta: 0:46:11  lr_architecture: 0.000250  loss_cls: 3.6199 (3.3744)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9717 (4.3677)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:47:09 root] (utils.py 302): INFO Epoch: [0]  [ 570/4926]  eta: 0:46:03  lr_architecture: 0.000250  loss_cls: 3.7831 (3.3820)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7966 (4.3585)  time: 0.6194  data: 0.0001  max mem: 14435
[2024-01-21 17:47:15 root] (utils.py 302): INFO Epoch: [0]  [ 580/4926]  eta: 0:45:56  lr_architecture: 0.000250  loss_cls: 3.7808 (3.3831)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7333 (4.3502)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:47:22 root] (utils.py 302): INFO Epoch: [0]  [ 590/4926]  eta: 0:45:48  lr_architecture: 0.000250  loss_cls: 3.7074 (3.3883)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8094 (4.3409)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:47:28 root] (utils.py 302): INFO Epoch: [0]  [ 600/4926]  eta: 0:45:42  lr_architecture: 0.000250  loss_cls: 3.7074 (3.3919)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7948 (4.3303)  time: 0.6261  data: 0.0001  max mem: 14435
[2024-01-21 17:47:34 root] (utils.py 302): INFO Epoch: [0]  [ 610/4926]  eta: 0:45:34  lr_architecture: 0.000250  loss_cls: 3.6725 (3.3902)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8225 (4.3231)  time: 0.6255  data: 0.0001  max mem: 14435
[2024-01-21 17:47:40 root] (utils.py 302): INFO Epoch: [0]  [ 620/4926]  eta: 0:45:27  lr_architecture: 0.000250  loss_cls: 3.4118 (3.3941)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8725 (4.3156)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:47:46 root] (utils.py 302): INFO Epoch: [0]  [ 630/4926]  eta: 0:45:20  lr_architecture: 0.000250  loss_cls: 3.8435 (3.3960)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8833 (4.3100)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:47:53 root] (utils.py 302): INFO Epoch: [0]  [ 640/4926]  eta: 0:45:12  lr_architecture: 0.000250  loss_cls: 3.6520 (3.3953)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9091 (4.3019)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:47:59 root] (utils.py 302): INFO Epoch: [0]  [ 650/4926]  eta: 0:45:05  lr_architecture: 0.000250  loss_cls: 3.1936 (3.3906)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7774 (4.2962)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:48:05 root] (utils.py 302): INFO Epoch: [0]  [ 660/4926]  eta: 0:44:58  lr_architecture: 0.000250  loss_cls: 3.1936 (3.3894)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7880 (4.2907)  time: 0.6193  data: 0.0001  max mem: 14435
[2024-01-21 17:48:11 root] (utils.py 302): INFO Epoch: [0]  [ 670/4926]  eta: 0:44:51  lr_architecture: 0.000250  loss_cls: 3.4623 (3.3906)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9316 (4.2878)  time: 0.6190  data: 0.0001  max mem: 14435
[2024-01-21 17:48:17 root] (utils.py 302): INFO Epoch: [0]  [ 680/4926]  eta: 0:44:44  lr_architecture: 0.000250  loss_cls: 3.5275 (3.3939)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8267 (4.2795)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:48:24 root] (utils.py 302): INFO Epoch: [0]  [ 690/4926]  eta: 0:44:36  lr_architecture: 0.000250  loss_cls: 3.7003 (3.3986)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7496 (4.2732)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:48:30 root] (utils.py 302): INFO Epoch: [0]  [ 700/4926]  eta: 0:44:29  lr_architecture: 0.000250  loss_cls: 3.7336 (3.4024)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.6901 (4.2645)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:48:36 root] (utils.py 302): INFO Epoch: [0]  [ 710/4926]  eta: 0:44:22  lr_architecture: 0.000250  loss_cls: 3.7358 (3.4041)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7017 (4.2592)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:48:42 root] (utils.py 302): INFO Epoch: [0]  [ 720/4926]  eta: 0:44:15  lr_architecture: 0.000250  loss_cls: 3.4664 (3.4029)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8216 (4.2531)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:48:48 root] (utils.py 302): INFO Epoch: [0]  [ 730/4926]  eta: 0:44:08  lr_architecture: 0.000250  loss_cls: 3.4567 (3.4046)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7615 (4.2469)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:49:19 root] (main_pitome.py 215): INFO Namespace(batch_size=64, epochs=10, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=2, gpu=2, distributed=True, dist_backend='nccl')
[2024-01-21 17:49:22 root] (main_pitome.py 260): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:49:24 root] (main_pitome.py 300): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:49:31 root] (main_pitome.py 386): INFO number of params: 304326632
[2024-01-21 17:49:31 root] (main_pitome.py 441): INFO Start training for 10 epochs
[2024-01-21 17:49:40 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 13:02:13  lr_architecture: 0.000050  loss_cls: 2.8459 (2.8459)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.9756 (9.9756)  time: 9.5277  data: 0.0008  max mem: 10987
[2024-01-21 17:49:47 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:58:35  lr_architecture: 0.000050  loss_cls: 2.7420 (2.8744)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.9756 (10.1166)  time: 1.4474  data: 0.0002  max mem: 14435
[2024-01-21 17:49:53 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:58  lr_architecture: 0.000050  loss_cls: 2.7420 (2.8425)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 7.8741 (8.4506)  time: 0.6276  data: 0.0001  max mem: 14435
[2024-01-21 17:49:59 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:21  lr_architecture: 0.000050  loss_cls: 2.8151 (2.8077)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.3897 (7.8338)  time: 0.6163  data: 0.0001  max mem: 14435
[2024-01-21 17:50:05 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:14  lr_architecture: 0.000050  loss_cls: 2.8678 (2.8126)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.6886 (inf)  time: 0.6141  data: 0.0001  max mem: 14435
[2024-01-21 17:50:11 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:35  lr_architecture: 0.000050  loss_cls: 2.8678 (2.8152)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.6692 (inf)  time: 0.6142  data: 0.0001  max mem: 14435
[2024-01-21 17:50:17 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:04  lr_architecture: 0.000050  loss_cls: 2.8523 (2.8303)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.5364 (inf)  time: 0.6162  data: 0.0001  max mem: 14435
[2024-01-21 17:50:23 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:14  lr_architecture: 0.000050  loss_cls: 3.0058 (2.8510)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.3946 (inf)  time: 0.6157  data: 0.0001  max mem: 14435
[2024-01-21 17:50:30 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:50  lr_architecture: 0.000050  loss_cls: 2.8802 (2.8353)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.1701 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 17:50:36 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:43  lr_architecture: 0.000050  loss_cls: 2.7720 (2.8201)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.2078 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:50:42 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:49  lr_architecture: 0.000050  loss_cls: 2.8882 (2.8405)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.4734 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:50:48 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:56:03  lr_architecture: 0.000050  loss_cls: 2.8040 (2.8127)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.4934 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:50:54 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:24  lr_architecture: 0.000050  loss_cls: 2.6829 (2.8110)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.4413 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:51:00 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:49  lr_architecture: 0.000050  loss_cls: 2.9433 (2.8170)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.2094 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:51:07 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:19  lr_architecture: 0.000050  loss_cls: 2.8884 (2.8111)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7837 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 17:51:13 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:52  lr_architecture: 0.000050  loss_cls: 2.9654 (2.8240)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8021 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 17:51:19 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:28  lr_architecture: 0.000050  loss_cls: 2.9538 (2.8200)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8019 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:51:25 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:53:05  lr_architecture: 0.000050  loss_cls: 2.9021 (2.8222)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6494 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 17:51:31 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:45  lr_architecture: 0.000050  loss_cls: 2.9021 (2.8224)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6344 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:51:38 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:26  lr_architecture: 0.000050  loss_cls: 2.8090 (2.8294)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6800 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:51:44 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:52:08  lr_architecture: 0.000050  loss_cls: 2.8056 (2.8134)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6800 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:51:50 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:52  lr_architecture: 0.000050  loss_cls: 2.5671 (2.8069)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6706 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:51:56 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:36  lr_architecture: 0.000050  loss_cls: 2.7884 (2.8095)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7570 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:02 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:51:21  lr_architecture: 0.000050  loss_cls: 2.8597 (2.8069)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7570 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:52:08 root] (utils.py 302): INFO Epoch: [0]  [ 240/4926]  eta: 0:51:07  lr_architecture: 0.000050  loss_cls: 2.7655 (2.7992)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8120 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:15 root] (utils.py 302): INFO Epoch: [0]  [ 250/4926]  eta: 0:50:54  lr_architecture: 0.000050  loss_cls: 2.8212 (2.8004)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8120 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:52:21 root] (utils.py 302): INFO Epoch: [0]  [ 260/4926]  eta: 0:50:41  lr_architecture: 0.000050  loss_cls: 2.8212 (2.7928)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1731 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:52:27 root] (utils.py 302): INFO Epoch: [0]  [ 270/4926]  eta: 0:50:29  lr_architecture: 0.000050  loss_cls: 2.8816 (2.7968)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3726 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:52:33 root] (utils.py 302): INFO Epoch: [0]  [ 280/4926]  eta: 0:50:17  lr_architecture: 0.000050  loss_cls: 2.9046 (2.7988)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6691 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:39 root] (utils.py 302): INFO Epoch: [0]  [ 290/4926]  eta: 0:50:05  lr_architecture: 0.000050  loss_cls: 2.7847 (2.7931)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6690 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:52:45 root] (utils.py 302): INFO Epoch: [0]  [ 300/4926]  eta: 0:49:54  lr_architecture: 0.000050  loss_cls: 2.5723 (2.7870)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6340 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:52:52 root] (utils.py 302): INFO Epoch: [0]  [ 310/4926]  eta: 0:49:43  lr_architecture: 0.000050  loss_cls: 2.7842 (2.7883)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4217 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:58 root] (utils.py 302): INFO Epoch: [0]  [ 320/4926]  eta: 0:49:33  lr_architecture: 0.000050  loss_cls: 2.8516 (2.7913)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3677 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:53:04 root] (utils.py 302): INFO Epoch: [0]  [ 330/4926]  eta: 0:49:22  lr_architecture: 0.000050  loss_cls: 2.7489 (2.7866)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4007 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:53:10 root] (utils.py 302): INFO Epoch: [0]  [ 340/4926]  eta: 0:49:12  lr_architecture: 0.000050  loss_cls: 2.8646 (2.7902)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7561 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:53:16 root] (utils.py 302): INFO Epoch: [0]  [ 350/4926]  eta: 0:49:03  lr_architecture: 0.000050  loss_cls: 2.9270 (2.7894)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.9009 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:53:23 root] (utils.py 302): INFO Epoch: [0]  [ 360/4926]  eta: 0:48:53  lr_architecture: 0.000050  loss_cls: 2.8376 (2.7884)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.9009 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:53:29 root] (utils.py 302): INFO Epoch: [0]  [ 370/4926]  eta: 0:48:43  lr_architecture: 0.000050  loss_cls: 2.8052 (2.7839)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4450 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:53:35 root] (utils.py 302): INFO Epoch: [0]  [ 380/4926]  eta: 0:48:34  lr_architecture: 0.000050  loss_cls: 2.6470 (2.7806)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2555 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:53:41 root] (utils.py 302): INFO Epoch: [0]  [ 390/4926]  eta: 0:48:25  lr_architecture: 0.000050  loss_cls: 2.6645 (2.7765)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2525 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:53:47 root] (utils.py 302): INFO Epoch: [0]  [ 400/4926]  eta: 0:48:16  lr_architecture: 0.000050  loss_cls: 2.8649 (2.7776)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2586 (inf)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:53:53 root] (utils.py 302): INFO Epoch: [0]  [ 410/4926]  eta: 0:48:07  lr_architecture: 0.000050  loss_cls: 2.8783 (2.7717)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.9010 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:54:00 root] (utils.py 302): INFO Epoch: [0]  [ 420/4926]  eta: 0:47:59  lr_architecture: 0.000050  loss_cls: 2.8833 (2.7758)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7760 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:54:06 root] (utils.py 302): INFO Epoch: [0]  [ 430/4926]  eta: 0:47:50  lr_architecture: 0.000050  loss_cls: 3.0339 (2.7756)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4484 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:54:12 root] (utils.py 302): INFO Epoch: [0]  [ 440/4926]  eta: 0:47:42  lr_architecture: 0.000050  loss_cls: 2.8626 (2.7736)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4484 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:54:18 root] (utils.py 302): INFO Epoch: [0]  [ 450/4926]  eta: 0:47:33  lr_architecture: 0.000050  loss_cls: 2.8626 (2.7744)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4086 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:54:24 root] (utils.py 302): INFO Epoch: [0]  [ 460/4926]  eta: 0:47:25  lr_architecture: 0.000050  loss_cls: 2.8148 (2.7722)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4560 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:54:31 root] (utils.py 302): INFO Epoch: [0]  [ 470/4926]  eta: 0:47:17  lr_architecture: 0.000050  loss_cls: 2.6157 (2.7709)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4456 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:54:37 root] (utils.py 302): INFO Epoch: [0]  [ 480/4926]  eta: 0:47:09  lr_architecture: 0.000050  loss_cls: 2.6166 (2.7714)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2645 (inf)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:54:43 root] (utils.py 302): INFO Epoch: [0]  [ 490/4926]  eta: 0:47:01  lr_architecture: 0.000050  loss_cls: 2.7211 (2.7728)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2740 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:54:49 root] (utils.py 302): INFO Epoch: [0]  [ 500/4926]  eta: 0:46:53  lr_architecture: 0.000050  loss_cls: 2.9882 (2.7729)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2740 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:54:55 root] (utils.py 302): INFO Epoch: [0]  [ 510/4926]  eta: 0:46:45  lr_architecture: 0.000050  loss_cls: 2.6826 (2.7691)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3210 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 17:55:01 root] (utils.py 302): INFO Epoch: [0]  [ 520/4926]  eta: 0:46:37  lr_architecture: 0.000050  loss_cls: 2.7394 (2.7713)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2423 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:55:08 root] (utils.py 302): INFO Epoch: [0]  [ 530/4926]  eta: 0:46:31  lr_architecture: 0.000050  loss_cls: 2.6577 (2.7668)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0388 (inf)  time: 0.6261  data: 0.0001  max mem: 14435
[2024-01-21 17:55:14 root] (utils.py 302): INFO Epoch: [0]  [ 540/4926]  eta: 0:46:23  lr_architecture: 0.000050  loss_cls: 2.5981 (2.7645)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3543 (inf)  time: 0.6266  data: 0.0001  max mem: 14435
[2024-01-21 17:55:20 root] (utils.py 302): INFO Epoch: [0]  [ 550/4926]  eta: 0:46:15  lr_architecture: 0.000050  loss_cls: 2.7135 (2.7644)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4547 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:55:26 root] (utils.py 302): INFO Epoch: [0]  [ 560/4926]  eta: 0:46:08  lr_architecture: 0.000050  loss_cls: 2.9303 (2.7699)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4547 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 17:55:33 root] (utils.py 302): INFO Epoch: [0]  [ 570/4926]  eta: 0:46:00  lr_architecture: 0.000050  loss_cls: 3.0509 (2.7739)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2466 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:55:39 root] (utils.py 302): INFO Epoch: [0]  [ 580/4926]  eta: 0:45:53  lr_architecture: 0.000050  loss_cls: 3.0710 (2.7751)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2466 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:55:45 root] (utils.py 302): INFO Epoch: [0]  [ 590/4926]  eta: 0:45:45  lr_architecture: 0.000050  loss_cls: 2.9475 (2.7779)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2825 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:55:51 root] (utils.py 302): INFO Epoch: [0]  [ 600/4926]  eta: 0:45:40  lr_architecture: 0.000050  loss_cls: 2.8868 (2.7801)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3029 (inf)  time: 0.6311  data: 0.0001  max mem: 14435
[2024-01-21 17:55:58 root] (utils.py 302): INFO Epoch: [0]  [ 610/4926]  eta: 0:45:32  lr_architecture: 0.000050  loss_cls: 2.8269 (2.7773)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1365 (inf)  time: 0.6316  data: 0.0001  max mem: 14435
[2024-01-21 17:56:04 root] (utils.py 302): INFO Epoch: [0]  [ 620/4926]  eta: 0:45:25  lr_architecture: 0.000050  loss_cls: 2.8140 (2.7798)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1741 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:56:10 root] (utils.py 302): INFO Epoch: [0]  [ 630/4926]  eta: 0:45:17  lr_architecture: 0.000050  loss_cls: 3.0731 (2.7806)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3385 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:56:16 root] (utils.py 302): INFO Epoch: [0]  [ 640/4926]  eta: 0:45:10  lr_architecture: 0.000050  loss_cls: 2.9060 (2.7790)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1989 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:56:22 root] (utils.py 302): INFO Epoch: [0]  [ 650/4926]  eta: 0:45:03  lr_architecture: 0.000050  loss_cls: 2.6701 (2.7744)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3483 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:56:28 root] (utils.py 302): INFO Epoch: [0]  [ 660/4926]  eta: 0:44:56  lr_architecture: 0.000050  loss_cls: 2.4725 (2.7730)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3698 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:56:35 root] (utils.py 302): INFO Epoch: [0]  [ 670/4926]  eta: 0:44:48  lr_architecture: 0.000050  loss_cls: 2.8096 (2.7727)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6347 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 17:56:41 root] (utils.py 302): INFO Epoch: [0]  [ 680/4926]  eta: 0:44:41  lr_architecture: 0.000050  loss_cls: 2.8946 (2.7747)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6008 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:56:47 root] (utils.py 302): INFO Epoch: [0]  [ 690/4926]  eta: 0:44:34  lr_architecture: 0.000050  loss_cls: 2.9309 (2.7775)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4817 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 17:56:53 root] (utils.py 302): INFO Epoch: [0]  [ 700/4926]  eta: 0:44:27  lr_architecture: 0.000050  loss_cls: 2.9418 (2.7795)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4146 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:56:59 root] (utils.py 302): INFO Epoch: [0]  [ 710/4926]  eta: 0:44:20  lr_architecture: 0.000050  loss_cls: 2.9562 (2.7812)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2502 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:57:05 root] (utils.py 302): INFO Epoch: [0]  [ 720/4926]  eta: 0:44:13  lr_architecture: 0.000050  loss_cls: 2.9082 (2.7797)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.5115 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:57:12 root] (utils.py 302): INFO Epoch: [0]  [ 730/4926]  eta: 0:44:06  lr_architecture: 0.000050  loss_cls: 2.8440 (2.7804)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.5500 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:57:18 root] (utils.py 302): INFO Epoch: [0]  [ 740/4926]  eta: 0:43:59  lr_architecture: 0.000050  loss_cls: 2.9178 (2.7826)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1203 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:57:24 root] (utils.py 302): INFO Epoch: [0]  [ 750/4926]  eta: 0:43:52  lr_architecture: 0.000050  loss_cls: 2.7186 (2.7802)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0405 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:57:30 root] (utils.py 302): INFO Epoch: [0]  [ 760/4926]  eta: 0:43:45  lr_architecture: 0.000050  loss_cls: 2.5908 (2.7789)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3628 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:57:36 root] (utils.py 302): INFO Epoch: [0]  [ 770/4926]  eta: 0:43:38  lr_architecture: 0.000050  loss_cls: 2.9546 (2.7808)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6187 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:57:43 root] (utils.py 302): INFO Epoch: [0]  [ 780/4926]  eta: 0:43:31  lr_architecture: 0.000050  loss_cls: 2.9925 (2.7816)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4230 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:57:49 root] (utils.py 302): INFO Epoch: [0]  [ 790/4926]  eta: 0:43:24  lr_architecture: 0.000050  loss_cls: 2.6129 (2.7785)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1810 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:57:55 root] (utils.py 302): INFO Epoch: [0]  [ 800/4926]  eta: 0:43:17  lr_architecture: 0.000050  loss_cls: 2.7568 (2.7798)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1586 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:58:01 root] (utils.py 302): INFO Epoch: [0]  [ 810/4926]  eta: 0:43:10  lr_architecture: 0.000050  loss_cls: 2.9759 (2.7808)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1471 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:58:07 root] (utils.py 302): INFO Epoch: [0]  [ 820/4926]  eta: 0:43:03  lr_architecture: 0.000050  loss_cls: 2.9759 (2.7815)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9418 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:58:13 root] (utils.py 302): INFO Epoch: [0]  [ 830/4926]  eta: 0:42:56  lr_architecture: 0.000050  loss_cls: 2.7576 (2.7814)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0879 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:58:20 root] (utils.py 302): INFO Epoch: [0]  [ 840/4926]  eta: 0:42:49  lr_architecture: 0.000050  loss_cls: 2.6468 (2.7788)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2562 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:58:26 root] (utils.py 302): INFO Epoch: [0]  [ 850/4926]  eta: 0:42:43  lr_architecture: 0.000050  loss_cls: 2.6468 (2.7800)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4314 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:58:32 root] (utils.py 302): INFO Epoch: [0]  [ 860/4926]  eta: 0:42:36  lr_architecture: 0.000050  loss_cls: 2.8479 (2.7809)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0060 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 17:58:38 root] (utils.py 302): INFO Epoch: [0]  [ 870/4926]  eta: 0:42:29  lr_architecture: 0.000050  loss_cls: 2.5972 (2.7777)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9770 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 17:58:44 root] (utils.py 302): INFO Epoch: [0]  [ 880/4926]  eta: 0:42:22  lr_architecture: 0.000050  loss_cls: 2.8039 (2.7788)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2608 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:58:50 root] (utils.py 302): INFO Epoch: [0]  [ 890/4926]  eta: 0:42:15  lr_architecture: 0.000050  loss_cls: 2.7814 (2.7788)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1742 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:58:57 root] (utils.py 302): INFO Epoch: [0]  [ 900/4926]  eta: 0:42:09  lr_architecture: 0.000050  loss_cls: 2.8483 (2.7801)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1742 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:59:03 root] (utils.py 302): INFO Epoch: [0]  [ 910/4926]  eta: 0:42:02  lr_architecture: 0.000050  loss_cls: 2.6149 (2.7765)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2509 (inf)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:59:09 root] (utils.py 302): INFO Epoch: [0]  [ 920/4926]  eta: 0:41:55  lr_architecture: 0.000050  loss_cls: 2.6149 (2.7781)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4124 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:59:15 root] (utils.py 302): INFO Epoch: [0]  [ 930/4926]  eta: 0:41:49  lr_architecture: 0.000050  loss_cls: 2.9460 (2.7792)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4424 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:59:21 root] (utils.py 302): INFO Epoch: [0]  [ 940/4926]  eta: 0:41:42  lr_architecture: 0.000050  loss_cls: 2.9234 (2.7797)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3109 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:59:28 root] (utils.py 302): INFO Epoch: [0]  [ 950/4926]  eta: 0:41:35  lr_architecture: 0.000050  loss_cls: 2.7736 (2.7788)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2997 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:59:34 root] (utils.py 302): INFO Epoch: [0]  [ 960/4926]  eta: 0:41:28  lr_architecture: 0.000050  loss_cls: 2.8271 (2.7802)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1092 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:59:40 root] (utils.py 302): INFO Epoch: [0]  [ 970/4926]  eta: 0:41:22  lr_architecture: 0.000050  loss_cls: 2.9261 (2.7807)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1582 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:59:46 root] (utils.py 302): INFO Epoch: [0]  [ 980/4926]  eta: 0:41:15  lr_architecture: 0.000050  loss_cls: 2.7949 (2.7800)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2626 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:59:52 root] (utils.py 302): INFO Epoch: [0]  [ 990/4926]  eta: 0:41:08  lr_architecture: 0.000050  loss_cls: 2.8533 (2.7816)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4092 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 17:59:58 root] (utils.py 302): INFO Epoch: [0]  [1000/4926]  eta: 0:41:02  lr_architecture: 0.000050  loss_cls: 2.9128 (2.7821)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1340 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:00:05 root] (utils.py 302): INFO Epoch: [0]  [1010/4926]  eta: 0:40:55  lr_architecture: 0.000050  loss_cls: 2.9128 (2.7816)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0145 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:00:11 root] (utils.py 302): INFO Epoch: [0]  [1020/4926]  eta: 0:40:48  lr_architecture: 0.000050  loss_cls: 2.7964 (2.7795)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1103 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:00:17 root] (utils.py 302): INFO Epoch: [0]  [1030/4926]  eta: 0:40:42  lr_architecture: 0.000050  loss_cls: 2.8535 (2.7808)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2242 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:00:23 root] (utils.py 302): INFO Epoch: [0]  [1040/4926]  eta: 0:40:35  lr_architecture: 0.000050  loss_cls: 3.0631 (2.7813)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1670 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:00:29 root] (utils.py 302): INFO Epoch: [0]  [1050/4926]  eta: 0:40:29  lr_architecture: 0.000050  loss_cls: 3.0136 (2.7827)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1596 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:00:36 root] (utils.py 302): INFO Epoch: [0]  [1060/4926]  eta: 0:40:22  lr_architecture: 0.000050  loss_cls: 3.0800 (2.7848)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0127 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 18:00:42 root] (utils.py 302): INFO Epoch: [0]  [1070/4926]  eta: 0:40:15  lr_architecture: 0.000050  loss_cls: 2.7918 (2.7845)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9255 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 18:00:48 root] (utils.py 302): INFO Epoch: [0]  [1080/4926]  eta: 0:40:09  lr_architecture: 0.000050  loss_cls: 2.6925 (2.7837)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0092 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:00:54 root] (utils.py 302): INFO Epoch: [0]  [1090/4926]  eta: 0:40:02  lr_architecture: 0.000050  loss_cls: 2.9486 (2.7859)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0889 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 18:01:00 root] (utils.py 302): INFO Epoch: [0]  [1100/4926]  eta: 0:39:56  lr_architecture: 0.000050  loss_cls: 3.1612 (2.7888)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1153 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:01:06 root] (utils.py 302): INFO Epoch: [0]  [1110/4926]  eta: 0:39:49  lr_architecture: 0.000050  loss_cls: 3.0744 (2.7888)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2399 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 18:01:13 root] (utils.py 302): INFO Epoch: [0]  [1120/4926]  eta: 0:39:43  lr_architecture: 0.000050  loss_cls: 3.0026 (2.7893)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2399 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:01:19 root] (utils.py 302): INFO Epoch: [0]  [1130/4926]  eta: 0:39:36  lr_architecture: 0.000050  loss_cls: 2.9983 (2.7923)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0182 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:01:25 root] (utils.py 302): INFO Epoch: [0]  [1140/4926]  eta: 0:39:30  lr_architecture: 0.000050  loss_cls: 2.8836 (2.7912)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8994 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:01:31 root] (utils.py 302): INFO Epoch: [0]  [1150/4926]  eta: 0:39:23  lr_architecture: 0.000050  loss_cls: 2.7613 (2.7905)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8269 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 18:01:37 root] (utils.py 302): INFO Epoch: [0]  [1160/4926]  eta: 0:39:17  lr_architecture: 0.000050  loss_cls: 2.6899 (2.7885)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8349 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:01:44 root] (utils.py 302): INFO Epoch: [0]  [1170/4926]  eta: 0:39:10  lr_architecture: 0.000050  loss_cls: 2.6362 (2.7874)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7490 (inf)  time: 0.6157  data: 0.0001  max mem: 14435
[2024-01-21 18:01:50 root] (utils.py 302): INFO Epoch: [0]  [1180/4926]  eta: 0:39:04  lr_architecture: 0.000050  loss_cls: 2.6828 (2.7889)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8116 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 18:01:56 root] (utils.py 302): INFO Epoch: [0]  [1190/4926]  eta: 0:38:57  lr_architecture: 0.000050  loss_cls: 2.9586 (2.7903)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8462 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:02:02 root] (utils.py 302): INFO Epoch: [0]  [1200/4926]  eta: 0:38:51  lr_architecture: 0.000050  loss_cls: 2.9586 (2.7912)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0580 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:02:08 root] (utils.py 302): INFO Epoch: [0]  [1210/4926]  eta: 0:38:44  lr_architecture: 0.000050  loss_cls: 2.8363 (2.7905)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2814 (inf)  time: 0.6166  data: 0.0001  max mem: 14435
[2024-01-21 18:02:14 root] (utils.py 302): INFO Epoch: [0]  [1220/4926]  eta: 0:38:37  lr_architecture: 0.000050  loss_cls: 2.8987 (2.7924)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2515 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:02:21 root] (utils.py 302): INFO Epoch: [0]  [1230/4926]  eta: 0:38:31  lr_architecture: 0.000050  loss_cls: 3.1194 (2.7947)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2051 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:02:27 root] (utils.py 302): INFO Epoch: [0]  [1240/4926]  eta: 0:38:24  lr_architecture: 0.000050  loss_cls: 2.8749 (2.7931)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0342 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 18:02:33 root] (utils.py 302): INFO Epoch: [0]  [1250/4926]  eta: 0:38:18  lr_architecture: 0.000050  loss_cls: 2.7442 (2.7922)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8747 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:02:39 root] (utils.py 302): INFO Epoch: [0]  [1260/4926]  eta: 0:38:12  lr_architecture: 0.000050  loss_cls: 2.8777 (2.7935)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7353 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:02:45 root] (utils.py 302): INFO Epoch: [0]  [1270/4926]  eta: 0:38:05  lr_architecture: 0.000050  loss_cls: 2.9737 (2.7932)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0784 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 18:02:51 root] (utils.py 302): INFO Epoch: [0]  [1280/4926]  eta: 0:37:59  lr_architecture: 0.000050  loss_cls: 2.9772 (2.7945)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2665 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 18:02:58 root] (utils.py 302): INFO Epoch: [0]  [1290/4926]  eta: 0:37:52  lr_architecture: 0.000050  loss_cls: 2.8507 (2.7941)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4759 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:03:04 root] (utils.py 302): INFO Epoch: [0]  [1300/4926]  eta: 0:37:46  lr_architecture: 0.000050  loss_cls: 2.8161 (2.7932)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3382 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:03:10 root] (utils.py 302): INFO Epoch: [0]  [1310/4926]  eta: 0:37:39  lr_architecture: 0.000050  loss_cls: 2.9351 (2.7939)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1645 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:03:16 root] (utils.py 302): INFO Epoch: [0]  [1320/4926]  eta: 0:37:33  lr_architecture: 0.000050  loss_cls: 2.8180 (2.7917)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0273 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 18:03:22 root] (utils.py 302): INFO Epoch: [0]  [1330/4926]  eta: 0:37:26  lr_architecture: 0.000050  loss_cls: 2.7263 (2.7927)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8058 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:03:28 root] (utils.py 302): INFO Epoch: [0]  [1340/4926]  eta: 0:37:20  lr_architecture: 0.000050  loss_cls: 2.8805 (2.7925)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8058 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 18:03:35 root] (utils.py 302): INFO Epoch: [0]  [1350/4926]  eta: 0:37:13  lr_architecture: 0.000050  loss_cls: 2.8643 (2.7911)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8322 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:03:41 root] (utils.py 302): INFO Epoch: [0]  [1360/4926]  eta: 0:37:07  lr_architecture: 0.000050  loss_cls: 2.9031 (2.7918)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8372 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:03:47 root] (utils.py 302): INFO Epoch: [0]  [1370/4926]  eta: 0:37:01  lr_architecture: 0.000050  loss_cls: 2.9616 (2.7926)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2666 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 18:03:53 root] (utils.py 302): INFO Epoch: [0]  [1380/4926]  eta: 0:36:54  lr_architecture: 0.000050  loss_cls: 2.9214 (2.7935)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0463 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:03:59 root] (utils.py 302): INFO Epoch: [0]  [1390/4926]  eta: 0:36:48  lr_architecture: 0.000050  loss_cls: 2.9214 (2.7947)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0112 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:06 root] (utils.py 302): INFO Epoch: [0]  [1400/4926]  eta: 0:36:41  lr_architecture: 0.000050  loss_cls: 2.9811 (2.7956)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0490 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:12 root] (utils.py 302): INFO Epoch: [0]  [1410/4926]  eta: 0:36:35  lr_architecture: 0.000050  loss_cls: 2.8494 (2.7953)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9136 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:04:18 root] (utils.py 302): INFO Epoch: [0]  [1420/4926]  eta: 0:36:28  lr_architecture: 0.000050  loss_cls: 2.8394 (2.7953)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9136 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:04:24 root] (utils.py 302): INFO Epoch: [0]  [1430/4926]  eta: 0:36:22  lr_architecture: 0.000050  loss_cls: 2.8844 (2.7951)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0128 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:30 root] (utils.py 302): INFO Epoch: [0]  [1440/4926]  eta: 0:36:16  lr_architecture: 0.000050  loss_cls: 2.8972 (2.7959)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0694 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:36 root] (utils.py 302): INFO Epoch: [0]  [1450/4926]  eta: 0:36:09  lr_architecture: 0.000050  loss_cls: 2.7066 (2.7937)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0275 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:04:43 root] (utils.py 302): INFO Epoch: [0]  [1460/4926]  eta: 0:36:03  lr_architecture: 0.000050  loss_cls: 2.6427 (2.7938)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7960 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:04:49 root] (utils.py 302): INFO Epoch: [0]  [1470/4926]  eta: 0:35:56  lr_architecture: 0.000050  loss_cls: 2.6427 (2.7930)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8363 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:55 root] (utils.py 302): INFO Epoch: [0]  [1480/4926]  eta: 0:35:50  lr_architecture: 0.000050  loss_cls: 2.7394 (2.7937)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8363 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:05:01 root] (utils.py 302): INFO Epoch: [0]  [1490/4926]  eta: 0:35:44  lr_architecture: 0.000050  loss_cls: 2.7394 (2.7928)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9056 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:05:07 root] (utils.py 302): INFO Epoch: [0]  [1500/4926]  eta: 0:35:37  lr_architecture: 0.000050  loss_cls: 2.8901 (2.7938)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0355 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:05:13 root] (utils.py 302): INFO Epoch: [0]  [1510/4926]  eta: 0:35:31  lr_architecture: 0.000050  loss_cls: 2.8697 (2.7939)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0300 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:05:20 root] (utils.py 302): INFO Epoch: [0]  [1520/4926]  eta: 0:35:24  lr_architecture: 0.000050  loss_cls: 2.8679 (2.7935)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8817 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:05:26 root] (utils.py 302): INFO Epoch: [0]  [1530/4926]  eta: 0:35:18  lr_architecture: 0.000050  loss_cls: 2.8541 (2.7934)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8144 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:05:32 root] (utils.py 302): INFO Epoch: [0]  [1540/4926]  eta: 0:35:12  lr_architecture: 0.000050  loss_cls: 2.7596 (2.7926)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6871 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:05:38 root] (utils.py 302): INFO Epoch: [0]  [1550/4926]  eta: 0:35:05  lr_architecture: 0.000050  loss_cls: 2.9324 (2.7950)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7930 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:05:44 root] (utils.py 302): INFO Epoch: [0]  [1560/4926]  eta: 0:34:59  lr_architecture: 0.000050  loss_cls: 2.9324 (2.7931)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1756 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:05:51 root] (utils.py 302): INFO Epoch: [0]  [1570/4926]  eta: 0:34:52  lr_architecture: 0.000050  loss_cls: 2.6397 (2.7931)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0830 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:05:57 root] (utils.py 302): INFO Epoch: [0]  [1580/4926]  eta: 0:34:46  lr_architecture: 0.000050  loss_cls: 2.8971 (2.7945)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8671 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:06:03 root] (utils.py 302): INFO Epoch: [0]  [1590/4926]  eta: 0:34:40  lr_architecture: 0.000050  loss_cls: 2.8971 (2.7946)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2101 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:06:09 root] (utils.py 302): INFO Epoch: [0]  [1600/4926]  eta: 0:34:33  lr_architecture: 0.000050  loss_cls: 3.0276 (2.7964)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2909 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:06:15 root] (utils.py 302): INFO Epoch: [0]  [1610/4926]  eta: 0:34:27  lr_architecture: 0.000050  loss_cls: 2.9678 (2.7958)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9003 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:06:21 root] (utils.py 302): INFO Epoch: [0]  [1620/4926]  eta: 0:34:21  lr_architecture: 0.000050  loss_cls: 2.9678 (2.7975)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0110 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:06:28 root] (utils.py 302): INFO Epoch: [0]  [1630/4926]  eta: 0:34:14  lr_architecture: 0.000050  loss_cls: 3.0649 (2.7980)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7389 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:06:34 root] (utils.py 302): INFO Epoch: [0]  [1640/4926]  eta: 0:34:08  lr_architecture: 0.000050  loss_cls: 3.0436 (2.7986)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7246 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:06:40 root] (utils.py 302): INFO Epoch: [0]  [1650/4926]  eta: 0:34:02  lr_architecture: 0.000050  loss_cls: 3.0816 (2.7996)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7044 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:06:46 root] (utils.py 302): INFO Epoch: [0]  [1660/4926]  eta: 0:33:55  lr_architecture: 0.000050  loss_cls: 3.0566 (2.7984)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6887 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:06:52 root] (utils.py 302): INFO Epoch: [0]  [1670/4926]  eta: 0:33:49  lr_architecture: 0.000050  loss_cls: 2.7218 (2.7986)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8399 (inf)  time: 0.6166  data: 0.0001  max mem: 14435
[2024-01-21 18:06:58 root] (utils.py 302): INFO Epoch: [0]  [1680/4926]  eta: 0:33:42  lr_architecture: 0.000050  loss_cls: 2.9400 (2.7985)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9823 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:07:05 root] (utils.py 302): INFO Epoch: [0]  [1690/4926]  eta: 0:33:36  lr_architecture: 0.000050  loss_cls: 2.9400 (2.7988)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9823 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:07:11 root] (utils.py 302): INFO Epoch: [0]  [1700/4926]  eta: 0:33:30  lr_architecture: 0.000050  loss_cls: 2.8726 (2.7981)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9441 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:07:17 root] (utils.py 302): INFO Epoch: [0]  [1710/4926]  eta: 0:33:23  lr_architecture: 0.000050  loss_cls: 2.8328 (2.7975)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9527 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:07:23 root] (utils.py 302): INFO Epoch: [0]  [1720/4926]  eta: 0:33:17  lr_architecture: 0.000050  loss_cls: 2.7545 (2.7969)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8780 (inf)  time: 0.6231  data: 0.0001  max mem: 14435
[2024-01-21 18:07:29 root] (utils.py 302): INFO Epoch: [0]  [1730/4926]  eta: 0:33:11  lr_architecture: 0.000050  loss_cls: 2.7545 (2.7979)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9376 (inf)  time: 0.6232  data: 0.0001  max mem: 14435
[2024-01-21 18:07:36 root] (utils.py 302): INFO Epoch: [0]  [1740/4926]  eta: 0:33:05  lr_architecture: 0.000050  loss_cls: 2.9398 (2.7976)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0414 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:07:42 root] (utils.py 302): INFO Epoch: [0]  [1750/4926]  eta: 0:32:58  lr_architecture: 0.000050  loss_cls: 2.8501 (2.7966)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9026 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:07:48 root] (utils.py 302): INFO Epoch: [0]  [1760/4926]  eta: 0:32:52  lr_architecture: 0.000050  loss_cls: 2.9915 (2.7987)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8929 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:07:54 root] (utils.py 302): INFO Epoch: [0]  [1770/4926]  eta: 0:32:46  lr_architecture: 0.000050  loss_cls: 3.1119 (2.7993)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9366 (inf)  time: 0.6231  data: 0.0001  max mem: 14435
[2024-01-21 18:08:01 root] (utils.py 302): INFO Epoch: [0]  [1780/4926]  eta: 0:32:40  lr_architecture: 0.000050  loss_cls: 2.8931 (2.7995)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9497 (inf)  time: 0.6294  data: 0.0001  max mem: 14435
[2024-01-21 18:08:07 root] (utils.py 302): INFO Epoch: [0]  [1790/4926]  eta: 0:32:33  lr_architecture: 0.000050  loss_cls: 2.6967 (2.7983)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6417 (inf)  time: 0.6240  data: 0.0001  max mem: 14435
[2024-01-21 18:08:13 root] (utils.py 302): INFO Epoch: [0]  [1800/4926]  eta: 0:32:27  lr_architecture: 0.000050  loss_cls: 2.6122 (2.7979)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6119 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:08:19 root] (utils.py 302): INFO Epoch: [0]  [1810/4926]  eta: 0:32:21  lr_architecture: 0.000050  loss_cls: 2.6142 (2.7975)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.5780 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:08:25 root] (utils.py 302): INFO Epoch: [0]  [1820/4926]  eta: 0:32:14  lr_architecture: 0.000050  loss_cls: 2.7166 (2.7974)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6436 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:08:31 root] (utils.py 302): INFO Epoch: [0]  [1830/4926]  eta: 0:32:08  lr_architecture: 0.000050  loss_cls: 2.7892 (2.7977)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7217 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:08:38 root] (utils.py 302): INFO Epoch: [0]  [1840/4926]  eta: 0:32:02  lr_architecture: 0.000050  loss_cls: 2.8376 (2.7979)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7587 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:08:44 root] (utils.py 302): INFO Epoch: [0]  [1850/4926]  eta: 0:31:55  lr_architecture: 0.000050  loss_cls: 2.9562 (2.7973)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8273 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:08:50 root] (utils.py 302): INFO Epoch: [0]  [1860/4926]  eta: 0:31:49  lr_architecture: 0.000050  loss_cls: 3.0498 (2.7989)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9505 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:08:56 root] (utils.py 302): INFO Epoch: [0]  [1870/4926]  eta: 0:31:43  lr_architecture: 0.000050  loss_cls: 2.9670 (2.7983)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9542 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:09:02 root] (utils.py 302): INFO Epoch: [0]  [1880/4926]  eta: 0:31:37  lr_architecture: 0.000050  loss_cls: 2.7727 (2.7986)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8330 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:09:08 root] (utils.py 302): INFO Epoch: [0]  [1890/4926]  eta: 0:31:30  lr_architecture: 0.000050  loss_cls: 3.0997 (2.7995)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0078 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:09:15 root] (utils.py 302): INFO Epoch: [0]  [1900/4926]  eta: 0:31:24  lr_architecture: 0.000050  loss_cls: 2.9547 (2.8001)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8176 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:09:21 root] (utils.py 302): INFO Epoch: [0]  [1910/4926]  eta: 0:31:18  lr_architecture: 0.000050  loss_cls: 3.0021 (2.8008)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7115 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:09:27 root] (utils.py 302): INFO Epoch: [0]  [1920/4926]  eta: 0:31:11  lr_architecture: 0.000050  loss_cls: 2.6811 (2.7996)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6049 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:09:33 root] (utils.py 302): INFO Epoch: [0]  [1930/4926]  eta: 0:31:05  lr_architecture: 0.000050  loss_cls: 2.6867 (2.7996)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6834 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:09:39 root] (utils.py 302): INFO Epoch: [0]  [1940/4926]  eta: 0:30:59  lr_architecture: 0.000050  loss_cls: 2.7474 (2.7975)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7753 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:09:45 root] (utils.py 302): INFO Epoch: [0]  [1950/4926]  eta: 0:30:52  lr_architecture: 0.000050  loss_cls: 2.7465 (2.7974)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6946 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:09:52 root] (utils.py 302): INFO Epoch: [0]  [1960/4926]  eta: 0:30:46  lr_architecture: 0.000050  loss_cls: 2.9660 (2.7985)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8224 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:09:58 root] (utils.py 302): INFO Epoch: [0]  [1970/4926]  eta: 0:30:40  lr_architecture: 0.000050  loss_cls: 2.8645 (2.7973)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9604 (inf)  time: 0.6191  data: 0.0001  max mem: 14435
[2024-01-21 18:10:04 root] (utils.py 302): INFO Epoch: [0]  [1980/4926]  eta: 0:30:34  lr_architecture: 0.000050  loss_cls: 2.8019 (2.7978)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0287 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 18:10:10 root] (utils.py 302): INFO Epoch: [0]  [1990/4926]  eta: 0:30:27  lr_architecture: 0.000050  loss_cls: 2.9010 (2.7980)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9067 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:10:16 root] (utils.py 302): INFO Epoch: [0]  [2000/4926]  eta: 0:30:21  lr_architecture: 0.000050  loss_cls: 3.0409 (2.7980)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6025 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:10:23 root] (utils.py 302): INFO Epoch: [0]  [2010/4926]  eta: 0:30:15  lr_architecture: 0.000050  loss_cls: 2.9224 (2.7969)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8850 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:10:29 root] (utils.py 302): INFO Epoch: [0]  [2020/4926]  eta: 0:30:08  lr_architecture: 0.000050  loss_cls: 2.7940 (2.7970)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7547 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 18:10:35 root] (utils.py 302): INFO Epoch: [0]  [2030/4926]  eta: 0:30:02  lr_architecture: 0.000050  loss_cls: 2.9116 (2.7979)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6635 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 18:10:41 root] (utils.py 302): INFO Epoch: [0]  [2040/4926]  eta: 0:29:56  lr_architecture: 0.000050  loss_cls: 3.0256 (2.7988)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7570 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:10:47 root] (utils.py 302): INFO Epoch: [0]  [2050/4926]  eta: 0:29:49  lr_architecture: 0.000050  loss_cls: 3.0495 (2.7993)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7615 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:10:53 root] (utils.py 302): INFO Epoch: [0]  [2060/4926]  eta: 0:29:43  lr_architecture: 0.000050  loss_cls: 2.9991 (2.7994)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1213 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 18:11:00 root] (utils.py 302): INFO Epoch: [0]  [2070/4926]  eta: 0:29:37  lr_architecture: 0.000050  loss_cls: 2.9031 (2.7991)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1233 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:11:06 root] (utils.py 302): INFO Epoch: [0]  [2080/4926]  eta: 0:29:31  lr_architecture: 0.000050  loss_cls: 2.8720 (2.7996)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9250 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:11:12 root] (utils.py 302): INFO Epoch: [0]  [2090/4926]  eta: 0:29:24  lr_architecture: 0.000050  loss_cls: 2.9123 (2.7994)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9216 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:11:18 root] (utils.py 302): INFO Epoch: [0]  [2100/4926]  eta: 0:29:18  lr_architecture: 0.000050  loss_cls: 2.8985 (2.7994)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9333 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 18:11:24 root] (utils.py 302): INFO Epoch: [0]  [2110/4926]  eta: 0:29:12  lr_architecture: 0.000050  loss_cls: 2.8945 (2.7997)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8400 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:11:31 root] (utils.py 302): INFO Epoch: [0]  [2120/4926]  eta: 0:29:05  lr_architecture: 0.000050  loss_cls: 2.8945 (2.7991)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3897 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:11:37 root] (utils.py 302): INFO Epoch: [0]  [2130/4926]  eta: 0:28:59  lr_architecture: 0.000050  loss_cls: 2.8404 (2.8000)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6892 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:11:43 root] (utils.py 302): INFO Epoch: [0]  [2140/4926]  eta: 0:28:53  lr_architecture: 0.000050  loss_cls: 2.9145 (2.8005)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7667 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:11:49 root] (utils.py 302): INFO Epoch: [0]  [2150/4926]  eta: 0:28:47  lr_architecture: 0.000050  loss_cls: 2.7963 (2.7996)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9896 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:11:55 root] (utils.py 302): INFO Epoch: [0]  [2160/4926]  eta: 0:28:40  lr_architecture: 0.000050  loss_cls: 2.6795 (2.7993)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8249 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 18:12:01 root] (utils.py 302): INFO Epoch: [0]  [2170/4926]  eta: 0:28:34  lr_architecture: 0.000050  loss_cls: 2.9607 (2.8002)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8249 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:12:08 root] (utils.py 302): INFO Epoch: [0]  [2180/4926]  eta: 0:28:28  lr_architecture: 0.000050  loss_cls: 3.0286 (2.7998)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9852 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:12:14 root] (utils.py 302): INFO Epoch: [0]  [2190/4926]  eta: 0:28:21  lr_architecture: 0.000050  loss_cls: 2.8702 (2.8001)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9460 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 18:12:20 root] (utils.py 302): INFO Epoch: [0]  [2200/4926]  eta: 0:28:15  lr_architecture: 0.000050  loss_cls: 2.8463 (2.8001)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8984 (inf)  time: 0.6193  data: 0.0001  max mem: 14435
[2024-01-21 18:12:26 root] (utils.py 302): INFO Epoch: [0]  [2210/4926]  eta: 0:28:09  lr_architecture: 0.000050  loss_cls: 3.0144 (2.8015)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8984 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:12:32 root] (utils.py 302): INFO Epoch: [0]  [2220/4926]  eta: 0:28:03  lr_architecture: 0.000050  loss_cls: 3.0162 (2.8014)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6894 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:12:38 root] (utils.py 302): INFO Epoch: [0]  [2230/4926]  eta: 0:27:56  lr_architecture: 0.000050  loss_cls: 2.7683 (2.8020)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6500 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:12:45 root] (utils.py 302): INFO Epoch: [0]  [2240/4926]  eta: 0:27:50  lr_architecture: 0.000050  loss_cls: 2.9289 (2.8021)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7908 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:12:51 root] (utils.py 302): INFO Epoch: [0]  [2250/4926]  eta: 0:27:44  lr_architecture: 0.000050  loss_cls: 2.9712 (2.8026)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9979 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:12:57 root] (utils.py 302): INFO Epoch: [0]  [2260/4926]  eta: 0:27:38  lr_architecture: 0.000050  loss_cls: 2.8549 (2.8032)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9130 (inf)  time: 0.6190  data: 0.0001  max mem: 14435
[2024-01-21 18:13:03 root] (utils.py 302): INFO Epoch: [0]  [2270/4926]  eta: 0:27:31  lr_architecture: 0.000050  loss_cls: 2.8170 (2.8032)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7291 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 18:13:09 root] (utils.py 302): INFO Epoch: [0]  [2280/4926]  eta: 0:27:25  lr_architecture: 0.000050  loss_cls: 2.8400 (2.8038)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7291 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:13:16 root] (utils.py 302): INFO Epoch: [0]  [2290/4926]  eta: 0:27:19  lr_architecture: 0.000050  loss_cls: 2.9066 (2.8033)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8465 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 18:13:22 root] (utils.py 302): INFO Epoch: [0]  [2300/4926]  eta: 0:27:13  lr_architecture: 0.000050  loss_cls: 2.6112 (2.8022)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8706 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
