{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfEddie/HypCLIP/blob/perceiver/lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jarvis/anaconda3/envs/HypCLIP/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-01-10 02:30:44,889] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        }
      ],
      "source": [
        "from model.modules.dct import dct\n",
        "from lavis.datasets.builders import load_dataset\n",
        "from utils.data_utils import  get_loaders\n",
        "from lavis.models import load_model_and_preprocess\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as  F\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: /mnt/data/itr_dataset/dataset/flickr30k/annotations/train.json\n",
            "Using downloaded and verified file: /mnt/data/itr_dataset/dataset/flickr30k/annotations/val.json\n",
            "Using downloaded and verified file: /mnt/data/itr_dataset/dataset/flickr30k/annotations/test.json\n"
          ]
        }
      ],
      "source": [
        "COCO_PATH = \"/mnt/data/itr_dataset/dataset/coco/images\"\n",
        "FLICKR_PATH = \"/mnt/data/itr_dataset/dataset/flickr30k/flickr30k_images\"\n",
        "dataset = load_dataset(\"flickr30k\", vis_path=FLICKR_PATH, cfg_path=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PmEy9aKM44JU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_hidden_states(hidden_states, filtered=False):\n",
        "  for hidden_state in hidden_states:\n",
        "    hidden_state=hidden_state.permute(1,0,2)\n",
        "    x_dct = dct(hidden_state.transpose(0,2), norm='ortho').transpose(0,2)\n",
        "    if not filtered:\n",
        "      numpy_array = ((x_dct.permute(1,0,2).mean(0).mean(1))).cpu().detach().numpy()\n",
        "    else:\n",
        "      numpy_array = ((x_dct.permute(1,0,2).mean(0).mean(1)))[:350].cpu().detach().numpy()\n",
        "    plt.figure(figsize=(10, 2))\n",
        "\n",
        "    # plt.imshow(numpy_array, cmap='viridis')  # You can choose a different colormap\n",
        "    plt.plot(numpy_array)# You can choose a different colormap\n",
        "    # plt.colorbar()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import parser, config_dict, config_args, add_flags_from_config, argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "for _, config_dict in config_args.items():\n",
        "    parser = add_flags_from_config(parser, config_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EomhLZ96FiFZ"
      },
      "outputs": [],
      "source": [
        "from lavis.models import load_model_and_preprocess\n",
        "from model.modules.compressed_models import CompressedLAVISBLIP, CompressedLAVISBLIP2\n",
        "\n",
        "\n",
        "model, vis_processors, txt_processors = load_model_and_preprocess(\"blip_retrieval\", \"flickr\", is_eval=False)\n",
        "train_loader, val_loader, test_loader = get_loaders(\n",
        "    10, \n",
        "    dataset,\n",
        "    vis_processor=vis_processors['eval'],\n",
        "    txt_processor=txt_processors['eval'],\n",
        "    tokenizer=model.tokenizer,\n",
        ")\n",
        "device = torch.device('cpu')\n",
        "model = CompressedLAVISBLIP(model, compress_method='dct')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompressedLAVISBLIP(\n",
              "  (vision_model): VisionTransformerEncoder(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (blocks): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (text_model): XBertEncoder(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (crossattention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (vision_proj): Linear(in_features=768, out_features=256, bias=True)\n",
              "  (text_proj): Linear(in_features=768, out_features=256, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoProcessor\n",
        "from model.modules.compressed_models import CompressedHFCLIP\n",
        "BLIP_BASE_COCO = \"Salesforce/blip-itm-base-coco\"\n",
        "BLIP_LARGE_COCO = \"Salesforce/blip-itm-large-coco\"\n",
        "BLIP_2 = \"Salesforce/blip2-opt-2.7b\"\n",
        "CLIP_BASE_PATCH_32 = \"openai/clip-vit-base-patch32\"\n",
        "CLIP_BASE_PATCH_16 = \"openai/clip-vit-base-patch16\"\n",
        "CLIP_LARGE_PATCH_14 = \"openai/clip-vit-large-patch14\"\n",
        "BLIP_BASE = \"Salesforce/blip-image-captioning-base\"\n",
        "BLIP_BASE_FLICKR = \"Salesforce/blip-itm-base-flickr\"\n",
        "BLIP_LARGE_FLICKR = \"Salesforce/blip-itm-large-flickr\"\n",
        "BLIP_LARGE_FLICKR = \"Salesforce/blip-itm-large-coco\"\n",
        "LAVIS_BLIP_BASE_FLICKR = \"lavis-blip-itm-base-flickr\"\n",
        "LAVIS_BLIP_BASE_COCO= \"lavis-blip-itm-base-coco\"\n",
        "model_ckt = CLIP_BASE_PATCH_32 \n",
        "ori_model = AutoModel.from_pretrained(model_ckt)\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cpu')\n",
        "# model = CompressedHFCLIP(ori_model, 'dct').to(device)\n",
        "processor = AutoProcessor.from_pretrained(model_ckt)\n",
        "train_loader, val_loader, test_loader = get_loaders(\n",
        "    40, \n",
        "    dataset,\n",
        "    vis_processor=processor,\n",
        "    txt_processor=None,\n",
        "    tokenizer=processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for batch in train_loader:\n",
        "    print(batch['pixel_values'].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clip_output = ori_model.vision_model(\n",
        "    pixel_values=batch['pixel_values'], \n",
        "    output_hidden_states=True,\n",
        "    output_attentions=True,\n",
        "    return_dict=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'clip_output' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m total_coef \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mclip_output\u001b[49m\u001b[38;5;241m.\u001b[39mattentions)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(clip_output\u001b[38;5;241m.\u001b[39mattentions)):\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clip_output' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "total_coef = [0]*len(clip_output.attentions)\n",
        "for i in range(len(clip_output.attentions)):\n",
        "    df = pd.DataFrame()\n",
        "    attention = clip_output.attentions[i][:,:,:,1:].mean(0).mean(0).mean(0)\n",
        "    attention -= attention.min() \n",
        "    attention /= attention.max() \n",
        "    feature_std = clip_output.hidden_states[i][:,1:,:].mean(0).std(1)\n",
        "    feature_std -= feature_std.min() \n",
        "    feature_std /= feature_std.max() \n",
        "    df['attn'] = attention.detach().numpy()\n",
        "    df['feat_std'] = feature_std.detach().numpy()\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot( df['feat_std'] )\n",
        "    plt.plot( df['attn'] )\n",
        "    \n",
        "    correlation_matrix = df[['attn', 'feat_std']].corr()\n",
        "    correlation_coefficient = correlation_matrix.loc['attn', 'feat_std']\n",
        "    total_coef[i] += correlation_coefficient\n",
        "    \n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(np.array(total_coef))\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'total_coef' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Show the plot\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 39\u001b[0m plot_coeff_attn_feat_std(\u001b[43mtotal_coef\u001b[49m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'total_coef' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_coeff_attn_feat_std(coeff):\n",
        "# Generate sample data\n",
        "\n",
        "\n",
        "    # Set the threshold value\n",
        "    threshold = 0.5 \n",
        "    x = np.linspace(0, len(coeff), len(coeff))\n",
        "    y = np.array(coeff)\n",
        "\n",
        "    # Create two masks for values above and below the threshold\n",
        "    mask_above = x >= len(coeff)/2 \n",
        "    mask_below = y < len(coeff)/2 \n",
        "\n",
        "    # Plot values above the threshold in one color\n",
        "    plt.plot(x, y, color='blue', linestyle='-', label='Connected Line')\n",
        "    plt.plot(x[mask_above], y[mask_above], color='orange', label='Decision making')\n",
        "\n",
        "    # Plot values below the threshold in another color\n",
        "    # plt.plot(x[mask_below], y[mask_below], color='red', label='Below Threshold')\n",
        "    \n",
        "    # Add a horizontal line to indicate the threshold\n",
        "    plt.axhline(y=threshold, color='black', linestyle='--', label='Feature extract')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Layers')\n",
        "    plt.ylabel('Correlation coefficient')\n",
        "    plt.title('Correlation coefficient between attention scores and std of features in each vector')\n",
        "\n",
        "    # Add legend\n",
        "    plt.legend()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "plot_coeff_attn_feat_std(total_coef)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNU3kY2hKMcYkBu4YiC5IrJ",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
