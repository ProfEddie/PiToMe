[2024-01-21 14:05:51 root] (main_tome.py 216): INFO Namespace(batch_size=256, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:05:55 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:05:57 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:05:59 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:15:13 root] (main_tome.py 216): INFO Namespace(batch_size=256, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:15:17 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:15:19 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:15:21 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:15:21 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:16:41 root] (main_tome.py 216): INFO Namespace(batch_size=256, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:16:46 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:16:48 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:16:50 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:16:50 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:17:53 root] (main_tome.py 216): INFO Namespace(batch_size=100, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:17:58 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:18:01 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:18:01 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:18:01 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:18:32 root] (main_tome.py 216): INFO Namespace(batch_size=100, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:18:37 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:18:39 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:18:40 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:18:40 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:19:04 root] (main_tome.py 216): INFO Namespace(batch_size=103, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:19:09 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:19:12 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:19:13 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:19:13 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:19:35 root] (main_tome.py 216): INFO Namespace(batch_size=259, epochs=3, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:19:39 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:19:42 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:19:47 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:19:47 root] (main_tome.py 416): INFO Start training for 3 epochs
[2024-01-21 14:22:09 root] (main_tome.py 216): INFO Namespace(batch_size=259, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:22:14 root] (main_tome.py 286): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:22:17 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:22:22 root] (main_tome.py 370): INFO number of params: 86567656
[2024-01-21 14:22:22 root] (main_tome.py 416): INFO Start training for 2 epochs
[2024-01-21 14:23:48 root] (main_tome.py 213): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:23:53 root] (main_tome.py 283): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:23:54 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:23:55 root] (main_tome.py 367): INFO number of params: 86567656
[2024-01-21 14:23:55 root] (main_tome.py 413): INFO Start training for 2 epochs
[2024-01-21 14:35:29 root] (main_tome.py 213): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:35:34 root] (main_tome.py 283): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:35:35 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:35:36 root] (main_tome.py 367): INFO number of params: 86567656
[2024-01-21 14:35:36 root] (main_tome.py 413): INFO Start training for 2 epochs
[2024-01-21 14:40:00 root] (main_tome.py 213): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 14:40:05 root] (main_tome.py 283): INFO Creating model: deit_base_patch16_224
[2024-01-21 14:40:07 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 14:40:07 root] (main_tome.py 367): INFO number of params: 86567656
[2024-01-21 14:40:07 root] (main_tome.py 413): INFO Start training for 2 epochs
[2024-01-21 14:43:13 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 15:48:46 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 15:48:53 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:48:54 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:49:00 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:49:01 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:49:01 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:49:25 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 15:49:40 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:49:41 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:49:44 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:49:45 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:49:45 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:50:55 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 15:50:59 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:51:00 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:51:03 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:51:03 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:51:03 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:51:06 root] (utils.py 293): INFO Epoch: [0]  [   0/3152]  eta: 2:14:10  lr_architecture: 0.010000  loss_cls: 1.6773 (1.6773)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 4.2487 (4.2487)  time: 2.5540  data: 0.0009  max mem: 8707
[2024-01-21 15:51:09 root] (utils.py 293): INFO Epoch: [0]  [  10/3152]  eta: 0:26:54  lr_architecture: 0.010000  loss_cls: 8.3472 (8.1347)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 3.0166 (inf)  time: 0.5140  data: 0.0002  max mem: 9646
[2024-01-21 15:51:12 root] (utils.py 293): INFO Epoch: [0]  [  20/3152]  eta: 0:21:21  lr_architecture: 0.010000  loss_cls: 7.9029 (8.0089)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.9231 (inf)  time: 0.3019  data: 0.0001  max mem: 9646
[2024-01-21 15:51:15 root] (utils.py 293): INFO Epoch: [0]  [  30/3152]  eta: 0:19:20  lr_architecture: 0.010000  loss_cls: 7.5309 (7.8136)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.0680 (inf)  time: 0.2937  data: 0.0001  max mem: 9646
[2024-01-21 15:51:35 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='deit_base_patch16_224', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 15:51:39 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:51:40 root] (main_tome.py 302): INFO Creating model: deit_base_patch16_224
[2024-01-21 15:51:43 timm.models.helpers] (helpers.py 183): INFO Loading pretrained weights from url (https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth)
[2024-01-21 15:51:44 root] (main_tome.py 386): INFO number of params: 86567656
[2024-01-21 15:51:44 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:51:46 root] (utils.py 293): INFO Epoch: [0]  [   0/3152]  eta: 2:16:48  lr_architecture: 0.010000  loss_cls: 1.6767 (1.6767)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 4.2495 (4.2495)  time: 2.6041  data: 0.0007  max mem: 8707
[2024-01-21 15:51:49 root] (utils.py 293): INFO Epoch: [0]  [  10/3152]  eta: 0:26:50  lr_architecture: 0.010000  loss_cls: 8.3535 (8.1571)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 2.9235 (inf)  time: 0.5127  data: 0.0002  max mem: 9646
[2024-01-21 15:51:52 root] (utils.py 293): INFO Epoch: [0]  [  20/3152]  eta: 0:21:19  lr_architecture: 0.010000  loss_cls: 7.9180 (8.0159)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.9709 (inf)  time: 0.2986  data: 0.0001  max mem: 9646
[2024-01-21 15:51:55 root] (utils.py 293): INFO Epoch: [0]  [  30/3152]  eta: 0:19:18  lr_architecture: 0.010000  loss_cls: 7.6001 (7.8552)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 1.0921 (inf)  time: 0.2930  data: 0.0001  max mem: 9646
[2024-01-21 15:51:58 root] (utils.py 293): INFO Epoch: [0]  [  40/3152]  eta: 0:18:14  lr_architecture: 0.010000  loss_cls: 7.3588 (7.6951)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.7031 (inf)  time: 0.2920  data: 0.0001  max mem: 9646
[2024-01-21 15:52:01 root] (utils.py 293): INFO Epoch: [0]  [  50/3152]  eta: 0:17:34  lr_architecture: 0.010000  loss_cls: 7.1661 (7.5870)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.5822 (inf)  time: 0.2919  data: 0.0001  max mem: 9646
[2024-01-21 15:52:04 root] (utils.py 293): INFO Epoch: [0]  [  60/3152]  eta: 0:17:07  lr_architecture: 0.010000  loss_cls: 7.1362 (7.5090)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.5522 (inf)  time: 0.2923  data: 0.0001  max mem: 9646
[2024-01-21 15:52:07 root] (utils.py 293): INFO Epoch: [0]  [  70/3152]  eta: 0:16:46  lr_architecture: 0.010000  loss_cls: 7.0848 (7.4506)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4694 (inf)  time: 0.2929  data: 0.0001  max mem: 9646
[2024-01-21 15:52:11 root] (utils.py 293): INFO Epoch: [0]  [  80/3152]  eta: 0:17:26  lr_architecture: 0.010000  loss_cls: 7.0664 (7.4002)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4471 (inf)  time: 0.3671  data: 0.0325  max mem: 9647
[2024-01-21 15:52:14 root] (utils.py 293): INFO Epoch: [0]  [  90/3152]  eta: 0:17:07  lr_architecture: 0.010000  loss_cls: 7.0489 (7.3614)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4074 (inf)  time: 0.3668  data: 0.0325  max mem: 9647
[2024-01-21 15:52:17 root] (utils.py 293): INFO Epoch: [0]  [ 100/3152]  eta: 0:16:50  lr_architecture: 0.010000  loss_cls: 7.0381 (7.3305)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3984 (inf)  time: 0.2924  data: 0.0001  max mem: 9647
[2024-01-21 15:52:20 root] (utils.py 293): INFO Epoch: [0]  [ 110/3152]  eta: 0:16:36  lr_architecture: 0.010000  loss_cls: 7.0381 (7.3043)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3984 (inf)  time: 0.2922  data: 0.0001  max mem: 9647
[2024-01-21 15:52:25 root] (utils.py 293): INFO Epoch: [0]  [ 120/3152]  eta: 0:17:13  lr_architecture: 0.010000  loss_cls: 7.0297 (7.2827)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4177 (inf)  time: 0.3901  data: 0.0727  max mem: 9647
[2024-01-21 15:52:28 root] (utils.py 293): INFO Epoch: [0]  [ 130/3152]  eta: 0:17:11  lr_architecture: 0.010000  loss_cls: 7.0317 (7.2631)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4432 (inf)  time: 0.4177  data: 0.0727  max mem: 9647
[2024-01-21 15:52:31 root] (utils.py 293): INFO Epoch: [0]  [ 140/3152]  eta: 0:17:00  lr_architecture: 0.010000  loss_cls: 7.0078 (7.2438)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4064 (inf)  time: 0.3248  data: 0.0001  max mem: 9647
[2024-01-21 15:52:34 root] (utils.py 293): INFO Epoch: [0]  [ 150/3152]  eta: 0:16:47  lr_architecture: 0.010000  loss_cls: 7.0102 (7.2281)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3963 (inf)  time: 0.2976  data: 0.0001  max mem: 9647
[2024-01-21 15:52:37 root] (utils.py 293): INFO Epoch: [0]  [ 160/3152]  eta: 0:16:39  lr_architecture: 0.010000  loss_cls: 7.0226 (7.2152)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4188 (inf)  time: 0.3011  data: 0.0001  max mem: 9647
[2024-01-21 15:52:40 root] (utils.py 293): INFO Epoch: [0]  [ 170/3152]  eta: 0:16:28  lr_architecture: 0.010000  loss_cls: 7.0226 (7.2040)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.4107 (inf)  time: 0.3010  data: 0.0001  max mem: 9647
[2024-01-21 15:52:44 root] (utils.py 293): INFO Epoch: [0]  [ 180/3152]  eta: 0:16:27  lr_architecture: 0.010000  loss_cls: 7.0045 (7.1926)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3855 (inf)  time: 0.3181  data: 0.0253  max mem: 9647
[2024-01-21 15:52:48 root] (utils.py 293): INFO Epoch: [0]  [ 190/3152]  eta: 0:16:44  lr_architecture: 0.010000  loss_cls: 7.0048 (7.1833)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3616 (inf)  time: 0.4044  data: 0.1117  max mem: 9647
[2024-01-21 15:52:51 root] (utils.py 293): INFO Epoch: [0]  [ 200/3152]  eta: 0:16:34  lr_architecture: 0.010000  loss_cls: 7.0048 (7.1733)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3629 (inf)  time: 0.3792  data: 0.0865  max mem: 9647
[2024-01-21 15:52:54 root] (utils.py 293): INFO Epoch: [0]  [ 210/3152]  eta: 0:16:25  lr_architecture: 0.010000  loss_cls: 6.9893 (7.1649)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3629 (inf)  time: 0.2930  data: 0.0001  max mem: 9647
[2024-01-21 15:52:59 root] (utils.py 293): INFO Epoch: [0]  [ 220/3152]  eta: 0:16:35  lr_architecture: 0.010000  loss_cls: 6.9890 (7.1570)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3753 (inf)  time: 0.3677  data: 0.0039  max mem: 9647
[2024-01-21 15:53:02 root] (utils.py 293): INFO Epoch: [0]  [ 230/3152]  eta: 0:16:26  lr_architecture: 0.010000  loss_cls: 6.9916 (7.1496)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3753 (inf)  time: 0.3679  data: 0.0038  max mem: 9647
[2024-01-21 15:53:05 root] (utils.py 293): INFO Epoch: [0]  [ 240/3152]  eta: 0:16:18  lr_architecture: 0.010000  loss_cls: 6.9709 (7.1423)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3439 (inf)  time: 0.2935  data: 0.0001  max mem: 9647
[2024-01-21 15:53:08 root] (utils.py 293): INFO Epoch: [0]  [ 250/3152]  eta: 0:16:09  lr_architecture: 0.010000  loss_cls: 6.9728 (7.1363)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3414 (inf)  time: 0.2936  data: 0.0001  max mem: 9647
[2024-01-21 15:53:13 root] (utils.py 293): INFO Epoch: [0]  [ 260/3152]  eta: 0:16:31  lr_architecture: 0.010000  loss_cls: 6.9931 (7.1315)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3664 (inf)  time: 0.4256  data: 0.0001  max mem: 9647
[2024-01-21 15:53:16 root] (utils.py 293): INFO Epoch: [0]  [ 270/3152]  eta: 0:16:22  lr_architecture: 0.010000  loss_cls: 6.9654 (7.1255)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3944 (inf)  time: 0.4257  data: 0.0001  max mem: 9647
[2024-01-21 15:53:19 root] (utils.py 293): INFO Epoch: [0]  [ 280/3152]  eta: 0:16:14  lr_architecture: 0.010000  loss_cls: 6.9828 (7.1213)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3886 (inf)  time: 0.2938  data: 0.0001  max mem: 9647
[2024-01-21 15:53:22 root] (utils.py 293): INFO Epoch: [0]  [ 290/3152]  eta: 0:16:06  lr_architecture: 0.010000  loss_cls: 6.9858 (7.1164)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3795 (inf)  time: 0.2932  data: 0.0001  max mem: 9647
[2024-01-21 15:53:25 root] (utils.py 293): INFO Epoch: [0]  [ 300/3152]  eta: 0:16:03  lr_architecture: 0.010000  loss_cls: 6.9795 (7.1123)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3717 (inf)  time: 0.3189  data: 0.0259  max mem: 9647
[2024-01-21 15:53:29 root] (utils.py 293): INFO Epoch: [0]  [ 310/3152]  eta: 0:16:01  lr_architecture: 0.010000  loss_cls: 6.9719 (7.1076)  loss_flops: 98.8134 (98.8134)  flops: 12.9405 (12.9405)  grad_norm: 0.3426 (inf)  time: 0.3498  data: 0.0559  max mem: 9647
[2024-01-21 15:53:52 root] (main_tome.py 217): INFO Namespace(batch_size=100, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 15:53:56 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:53:57 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:55:48 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:55:48 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:56:06 root] (main_tome.py 217): INFO Namespace(batch_size=64, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 15:56:15 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:56:17 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:56:29 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:56:29 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:56:49 root] (main_tome.py 217): INFO Namespace(batch_size=32, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 15:56:52 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:56:54 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:57:05 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:57:05 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:57:10 root] (utils.py 293): INFO Epoch: [0]  [   0/9852]  eta: 13:27:49  lr_architecture: 0.010000  loss_cls: 1.3979 (1.3979)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 11.2909 (11.2909)  time: 4.9198  data: 0.0006  max mem: 15215
[2024-01-21 15:57:26 root] (main_tome.py 217): INFO Namespace(batch_size=24, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 15:57:29 root] (main_tome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 15:57:30 root] (main_tome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 15:57:41 root] (main_tome.py 386): INFO number of params: 632045800
[2024-01-21 15:57:41 root] (main_tome.py 432): INFO Start training for 2 epochs
[2024-01-21 15:57:46 root] (utils.py 293): INFO Epoch: [0]  [    0/13136]  eta: 17:18:46  lr_architecture: 0.010000  loss_cls: 1.4116 (1.4116)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 15.4189 (15.4189)  time: 4.7447  data: 0.0007  max mem: 14771
[2024-01-21 15:57:56 root] (utils.py 293): INFO Epoch: [0]  [   10/13136]  eta: 4:58:13  lr_architecture: 0.010000  loss_cls: 7.8887 (7.4181)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 2.2490 (3.4921)  time: 1.3632  data: 0.0001  max mem: 20364
[2024-01-21 15:58:06 root] (utils.py 293): INFO Epoch: [0]  [   20/13136]  eta: 4:19:48  lr_architecture: 0.010000  loss_cls: 7.9897 (7.7216)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.7437 (2.6253)  time: 1.0107  data: 0.0001  max mem: 20364
[2024-01-21 15:58:16 root] (utils.py 293): INFO Epoch: [0]  [   30/13136]  eta: 4:06:04  lr_architecture: 0.010000  loss_cls: 7.7843 (7.6906)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.5196 (2.2081)  time: 0.9963  data: 0.0001  max mem: 20364
[2024-01-21 15:58:26 root] (utils.py 293): INFO Epoch: [0]  [   40/13136]  eta: 3:58:55  lr_architecture: 0.010000  loss_cls: 7.5058 (7.6262)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.2482 (1.9753)  time: 0.9962  data: 0.0001  max mem: 20364
[2024-01-21 15:58:36 root] (utils.py 293): INFO Epoch: [0]  [   50/13136]  eta: 3:54:26  lr_architecture: 0.010000  loss_cls: 7.2103 (7.5359)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1611 (1.8107)  time: 0.9950  data: 0.0001  max mem: 20364
[2024-01-21 15:58:46 root] (utils.py 293): INFO Epoch: [0]  [   60/13136]  eta: 3:51:22  lr_architecture: 0.010000  loss_cls: 7.1842 (7.4784)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1580 (1.7199)  time: 0.9940  data: 0.0001  max mem: 20364
[2024-01-21 15:58:56 root] (utils.py 293): INFO Epoch: [0]  [   70/13136]  eta: 3:49:06  lr_architecture: 0.010000  loss_cls: 7.1405 (7.4211)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1705 (1.6395)  time: 0.9939  data: 0.0001  max mem: 20364
[2024-01-21 15:59:06 root] (utils.py 293): INFO Epoch: [0]  [   80/13136]  eta: 3:47:23  lr_architecture: 0.010000  loss_cls: 7.1102 (7.3829)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1332 (1.5799)  time: 0.9941  data: 0.0001  max mem: 20364
[2024-01-21 15:59:16 root] (utils.py 293): INFO Epoch: [0]  [   90/13136]  eta: 3:46:01  lr_architecture: 0.010000  loss_cls: 7.0745 (7.3478)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1230 (1.5309)  time: 0.9948  data: 0.0001  max mem: 20364
[2024-01-21 15:59:26 root] (utils.py 293): INFO Epoch: [0]  [  100/13136]  eta: 3:44:51  lr_architecture: 0.010000  loss_cls: 7.0440 (7.3230)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1081 (1.4941)  time: 0.9944  data: 0.0001  max mem: 20364
[2024-01-21 15:59:36 root] (utils.py 293): INFO Epoch: [0]  [  110/13136]  eta: 3:43:54  lr_architecture: 0.010000  loss_cls: 7.0404 (7.2980)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.1869 (1.4695)  time: 0.9942  data: 0.0001  max mem: 20364
[2024-01-21 16:00:00 root] (main_pitome.py 217): INFO Namespace(batch_size=24, epochs=2, ratio=0.95, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:14:25 root] (main_pitome.py 262): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:14:26 root] (main_pitome.py 302): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:14:38 root] (main_pitome.py 389): INFO number of params: 632045800
[2024-01-21 16:14:38 root] (main_pitome.py 435): INFO Start training for 2 epochs
[2024-01-21 16:14:43 root] (utils.py 293): INFO Epoch: [0]  [    0/13136]  eta: 16:56:15  lr_architecture: 0.010000  loss_cls: 1.4385 (1.4385)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 15.5667 (15.5667)  time: 4.6418  data: 0.0009  max mem: 14768
[2024-01-21 16:14:53 root] (utils.py 293): INFO Epoch: [0]  [   10/13136]  eta: 5:01:08  lr_architecture: 0.010000  loss_cls: 8.0445 (7.3722)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 2.0428 (5.1708)  time: 1.3765  data: 0.0002  max mem: 19643
[2024-01-21 16:15:03 root] (utils.py 293): INFO Epoch: [0]  [   20/13136]  eta: 4:24:53  lr_architecture: 0.010000  loss_cls: 7.7538 (7.5343)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.6930 (3.4404)  time: 1.0403  data: 0.0001  max mem: 19643
[2024-01-21 16:15:14 root] (utils.py 293): INFO Epoch: [0]  [   30/13136]  eta: 4:11:50  lr_architecture: 0.010000  loss_cls: 7.6567 (7.5337)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.3881 (2.7646)  time: 1.0299  data: 0.0001  max mem: 19643
[2024-01-21 16:15:24 root] (utils.py 293): INFO Epoch: [0]  [   40/13136]  eta: 4:04:54  lr_architecture: 0.010000  loss_cls: 7.3665 (7.4820)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.2956 (2.3996)  time: 1.0278  data: 0.0001  max mem: 19643
[2024-01-21 16:15:34 root] (utils.py 293): INFO Epoch: [0]  [   50/13136]  eta: 4:00:34  lr_architecture: 0.010000  loss_cls: 7.2106 (7.4270)  loss_flops: 6026.1841 (6026.1841)  flops: 80.6285 (80.6285)  grad_norm: 1.0988 (2.1438)  time: 1.0258  data: 0.0001  max mem: 19643
[2024-01-21 16:16:03 root] (main_pitome.py 218): INFO Namespace(batch_size=20, epochs=2, ratio=0.9625, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:16:07 root] (main_pitome.py 263): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:16:08 root] (main_pitome.py 303): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:16:19 root] (main_pitome.py 390): INFO number of params: 632045800
[2024-01-21 16:16:28 root] (main_pitome.py 444): INFO Start training for 2 epochs
[2024-01-21 16:16:32 root] (utils.py 293): INFO Epoch: [0]  [    0/15763]  eta: 17:23:03  lr_architecture: 0.010000  loss_cls: 1.3850 (1.3850)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 14.2260 (14.2260)  time: 3.9703  data: 0.0007  max mem: 14771
[2024-01-21 16:16:43 root] (utils.py 293): INFO Epoch: [0]  [   10/15763]  eta: 5:44:48  lr_architecture: 0.010000  loss_cls: 7.7112 (7.3552)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 2.0620 (4.1439)  time: 1.3133  data: 0.0001  max mem: 19644
[2024-01-21 16:16:53 root] (utils.py 293): INFO Epoch: [0]  [   20/15763]  eta: 5:08:25  lr_architecture: 0.010000  loss_cls: 8.0028 (7.7713)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.9112 (3.1351)  time: 1.0357  data: 0.0001  max mem: 19644
[2024-01-21 16:17:03 root] (utils.py 293): INFO Epoch: [0]  [   30/15763]  eta: 4:55:20  lr_architecture: 0.010000  loss_cls: 7.8720 (7.7143)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.7437 (2.6213)  time: 1.0235  data: 0.0001  max mem: 19644
[2024-01-21 16:17:14 root] (utils.py 293): INFO Epoch: [0]  [   40/15763]  eta: 4:48:22  lr_architecture: 0.010000  loss_cls: 7.3697 (7.6154)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.3810 (2.2934)  time: 1.0217  data: 0.0001  max mem: 19644
[2024-01-21 16:17:24 root] (utils.py 293): INFO Epoch: [0]  [   50/15763]  eta: 4:44:08  lr_architecture: 0.010000  loss_cls: 7.2273 (7.5265)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1941 (2.0709)  time: 1.0209  data: 0.0001  max mem: 19644
[2024-01-21 16:17:34 root] (utils.py 293): INFO Epoch: [0]  [   60/15763]  eta: 4:41:14  lr_architecture: 0.010000  loss_cls: 7.1501 (7.4722)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1506 (1.9253)  time: 1.0216  data: 0.0001  max mem: 19644
[2024-01-21 16:17:44 root] (utils.py 293): INFO Epoch: [0]  [   70/15763]  eta: 4:39:10  lr_architecture: 0.010000  loss_cls: 7.1818 (7.4291)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1889 (1.8310)  time: 1.0225  data: 0.0001  max mem: 19644
[2024-01-21 16:17:54 root] (utils.py 293): INFO Epoch: [0]  [   80/15763]  eta: 4:37:30  lr_architecture: 0.010000  loss_cls: 7.1361 (7.3808)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2203 (1.7634)  time: 1.0223  data: 0.0001  max mem: 19644
[2024-01-21 16:18:05 root] (utils.py 293): INFO Epoch: [0]  [   90/15763]  eta: 4:36:09  lr_architecture: 0.010000  loss_cls: 7.1010 (7.3550)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2299 (1.7114)  time: 1.0211  data: 0.0001  max mem: 19644
[2024-01-21 16:18:15 root] (utils.py 293): INFO Epoch: [0]  [  100/15763]  eta: 4:35:04  lr_architecture: 0.010000  loss_cls: 7.1313 (7.3328)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2299 (1.6690)  time: 1.0213  data: 0.0001  max mem: 19644
[2024-01-21 16:18:25 root] (utils.py 293): INFO Epoch: [0]  [  110/15763]  eta: 4:34:07  lr_architecture: 0.010000  loss_cls: 7.1313 (7.3124)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2668 (1.6363)  time: 1.0215  data: 0.0001  max mem: 19644
[2024-01-21 16:18:59 root] (main_pitome.py 218): INFO Namespace(batch_size=20, epochs=2, ratio=0.9625, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:19:03 root] (main_pitome.py 263): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:19:04 root] (main_pitome.py 303): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:19:15 root] (main_pitome.py 390): INFO number of params: 632045800
[2024-01-21 16:19:15 root] (main_pitome.py 445): INFO Start training for 2 epochs
[2024-01-21 16:19:25 root] (utils.py 293): INFO Epoch: [0]  [    0/15763]  eta: 1 day, 19:16:12  lr_architecture: 0.010000  loss_cls: 1.4076 (1.4076)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 14.0616 (14.0616)  time: 9.8821  data: 0.0007  max mem: 14768
[2024-01-21 16:19:35 root] (utils.py 293): INFO Epoch: [0]  [   10/15763]  eta: 8:07:26  lr_architecture: 0.010000  loss_cls: 7.8206 (7.3806)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 2.0485 (3.9325)  time: 1.8565  data: 0.0001  max mem: 19639
[2024-01-21 16:19:46 root] (utils.py 293): INFO Epoch: [0]  [   20/15763]  eta: 6:24:12  lr_architecture: 0.010000  loss_cls: 7.7880 (7.5912)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.8645 (2.8990)  time: 1.0434  data: 0.0001  max mem: 19639
[2024-01-21 16:19:56 root] (utils.py 293): INFO Epoch: [0]  [   30/15763]  eta: 5:47:29  lr_architecture: 0.010000  loss_cls: 7.6533 (7.5969)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.5786 (2.4501)  time: 1.0330  data: 0.0001  max mem: 19639
[2024-01-21 16:20:06 root] (utils.py 293): INFO Epoch: [0]  [   40/15763]  eta: 5:28:23  lr_architecture: 0.010000  loss_cls: 7.5303 (7.5539)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.4175 (2.1774)  time: 1.0315  data: 0.0001  max mem: 19639
[2024-01-21 16:20:17 root] (utils.py 293): INFO Epoch: [0]  [   50/15763]  eta: 5:16:40  lr_architecture: 0.010000  loss_cls: 7.2266 (7.4835)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2673 (1.9831)  time: 1.0295  data: 0.0001  max mem: 19639
[2024-01-21 16:20:27 root] (utils.py 293): INFO Epoch: [0]  [   60/15763]  eta: 5:08:43  lr_architecture: 0.010000  loss_cls: 7.2018 (7.4315)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1838 (1.8522)  time: 1.0287  data: 0.0001  max mem: 19639
[2024-01-21 16:20:37 root] (utils.py 293): INFO Epoch: [0]  [   70/15763]  eta: 5:02:59  lr_architecture: 0.010000  loss_cls: 7.1766 (7.3975)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2471 (1.7765)  time: 1.0289  data: 0.0001  max mem: 19639
[2024-01-21 16:20:47 root] (utils.py 293): INFO Epoch: [0]  [   80/15763]  eta: 4:58:32  lr_architecture: 0.010000  loss_cls: 7.1516 (7.3550)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.3396 (1.7197)  time: 1.0280  data: 0.0001  max mem: 19639
[2024-01-21 16:20:55 root] (engine.py 57): INFO Loss is nan, stopping training
[2024-01-21 16:22:23 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.9625, model='vit_huge_patch14_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:22:27 root] (main_pitome.py 263): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:22:28 root] (main_pitome.py 303): INFO Creating model: vit_huge_patch14_mae
[2024-01-21 16:22:38 root] (main_pitome.py 390): INFO number of params: 632045800
[2024-01-21 16:22:38 root] (main_pitome.py 445): INFO Start training for 2 epochs
[2024-01-21 16:22:48 root] (utils.py 293): INFO Epoch: [0]  [    0/19704]  eta: 2 days, 6:19:23  lr_architecture: 0.001000  loss_cls: 1.1206 (1.1206)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 14.7789 (14.7789)  time: 9.9250  data: 0.0009  max mem: 14758
[2024-01-21 16:22:58 root] (utils.py 293): INFO Epoch: [0]  [   10/19704]  eta: 10:01:16  lr_architecture: 0.001000  loss_cls: 7.2736 (6.7745)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 2.3027 (4.6950)  time: 1.8319  data: 0.0002  max mem: 18411
[2024-01-21 16:23:08 root] (utils.py 293): INFO Epoch: [0]  [   20/19704]  eta: 7:51:46  lr_architecture: 0.001000  loss_cls: 7.1354 (6.9376)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.6159 (3.1720)  time: 1.0137  data: 0.0001  max mem: 18411
[2024-01-21 16:23:19 root] (utils.py 293): INFO Epoch: [0]  [   30/19704]  eta: 7:05:38  lr_architecture: 0.001000  loss_cls: 7.0945 (6.9814)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.3119 (2.5580)  time: 1.0045  data: 0.0001  max mem: 18411
[2024-01-21 16:23:29 root] (utils.py 293): INFO Epoch: [0]  [   40/19704]  eta: 6:41:58  lr_architecture: 0.001000  loss_cls: 7.1217 (7.0221)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.2424 (2.2288)  time: 1.0045  data: 0.0001  max mem: 18411
[2024-01-21 16:23:39 root] (utils.py 293): INFO Epoch: [0]  [   50/19704]  eta: 6:27:27  lr_architecture: 0.001000  loss_cls: 7.1456 (7.0395)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.1594 (2.0134)  time: 1.0041  data: 0.0001  max mem: 18411
[2024-01-21 16:23:49 root] (utils.py 293): INFO Epoch: [0]  [   60/19704]  eta: 6:17:41  lr_architecture: 0.001000  loss_cls: 7.0868 (7.0426)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0767 (1.8573)  time: 1.0042  data: 0.0001  max mem: 18411
[2024-01-21 16:23:59 root] (utils.py 293): INFO Epoch: [0]  [   70/19704]  eta: 6:10:41  lr_architecture: 0.001000  loss_cls: 7.0612 (7.0470)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0410 (1.7443)  time: 1.0052  data: 0.0001  max mem: 18411
[2024-01-21 16:24:09 root] (utils.py 293): INFO Epoch: [0]  [   80/19704]  eta: 6:05:19  lr_architecture: 0.001000  loss_cls: 7.0467 (7.0451)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0112 (1.6537)  time: 1.0052  data: 0.0001  max mem: 18411
[2024-01-21 16:24:19 root] (utils.py 293): INFO Epoch: [0]  [   90/19704]  eta: 6:01:06  lr_architecture: 0.001000  loss_cls: 7.0467 (7.0482)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0026 (1.5835)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:24:29 root] (utils.py 293): INFO Epoch: [0]  [  100/19704]  eta: 5:57:41  lr_architecture: 0.001000  loss_cls: 7.0080 (7.0427)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9914 (1.5245)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:24:39 root] (utils.py 293): INFO Epoch: [0]  [  110/19704]  eta: 5:54:51  lr_architecture: 0.001000  loss_cls: 6.9796 (7.0377)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9908 (1.4786)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:24:49 root] (utils.py 293): INFO Epoch: [0]  [  120/19704]  eta: 5:52:28  lr_architecture: 0.001000  loss_cls: 7.0212 (7.0376)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9929 (1.4415)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:24:59 root] (utils.py 293): INFO Epoch: [0]  [  130/19704]  eta: 5:50:24  lr_architecture: 0.001000  loss_cls: 7.0764 (7.0398)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 1.0061 (1.4093)  time: 1.0045  data: 0.0001  max mem: 18411
[2024-01-21 16:25:09 root] (utils.py 293): INFO Epoch: [0]  [  140/19704]  eta: 5:48:38  lr_architecture: 0.001000  loss_cls: 7.0144 (7.0365)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9721 (1.3761)  time: 1.0047  data: 0.0001  max mem: 18411
[2024-01-21 16:25:19 root] (utils.py 293): INFO Epoch: [0]  [  150/19704]  eta: 5:47:04  lr_architecture: 0.001000  loss_cls: 6.9973 (7.0339)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9455 (1.3494)  time: 1.0053  data: 0.0001  max mem: 18411
[2024-01-21 16:25:29 root] (utils.py 293): INFO Epoch: [0]  [  160/19704]  eta: 5:45:42  lr_architecture: 0.001000  loss_cls: 6.9768 (7.0305)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9480 (1.3244)  time: 1.0056  data: 0.0001  max mem: 18411
[2024-01-21 16:25:39 root] (utils.py 293): INFO Epoch: [0]  [  170/19704]  eta: 5:44:28  lr_architecture: 0.001000  loss_cls: 6.9929 (7.0307)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9368 (1.3022)  time: 1.0061  data: 0.0001  max mem: 18411
[2024-01-21 16:25:49 root] (utils.py 293): INFO Epoch: [0]  [  180/19704]  eta: 5:43:21  lr_architecture: 0.001000  loss_cls: 7.0099 (7.0291)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9154 (1.2807)  time: 1.0061  data: 0.0001  max mem: 18411
[2024-01-21 16:25:59 root] (utils.py 293): INFO Epoch: [0]  [  190/19704]  eta: 5:42:21  lr_architecture: 0.001000  loss_cls: 6.9904 (7.0276)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9069 (1.2611)  time: 1.0062  data: 0.0001  max mem: 18411
[2024-01-21 16:26:09 root] (utils.py 293): INFO Epoch: [0]  [  200/19704]  eta: 5:41:26  lr_architecture: 0.001000  loss_cls: 6.9717 (7.0251)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9136 (1.2445)  time: 1.0064  data: 0.0001  max mem: 18411
[2024-01-21 16:26:19 root] (utils.py 293): INFO Epoch: [0]  [  210/19704]  eta: 5:40:34  lr_architecture: 0.001000  loss_cls: 6.9717 (7.0221)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9213 (1.2297)  time: 1.0063  data: 0.0001  max mem: 18411
[2024-01-21 16:26:30 root] (utils.py 293): INFO Epoch: [0]  [  220/19704]  eta: 5:39:47  lr_architecture: 0.001000  loss_cls: 7.0049 (7.0220)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9386 (1.2189)  time: 1.0062  data: 0.0001  max mem: 18411
[2024-01-21 16:26:40 root] (utils.py 293): INFO Epoch: [0]  [  230/19704]  eta: 5:39:02  lr_architecture: 0.001000  loss_cls: 7.0128 (7.0196)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9483 (1.2068)  time: 1.0059  data: 0.0001  max mem: 18411
[2024-01-21 16:26:50 root] (utils.py 293): INFO Epoch: [0]  [  240/19704]  eta: 5:38:20  lr_architecture: 0.001000  loss_cls: 6.9538 (7.0177)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9445 (1.1969)  time: 1.0055  data: 0.0001  max mem: 18411
[2024-01-21 16:27:00 root] (utils.py 293): INFO Epoch: [0]  [  250/19704]  eta: 5:37:41  lr_architecture: 0.001000  loss_cls: 6.9587 (7.0162)  loss_flops: 8464.0889 (8464.0889)  flops: 95.0005 (95.0005)  grad_norm: 0.9109 (1.1859)  time: 1.0059  data: 0.0001  max mem: 18411
[2024-01-21 16:28:14 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:28:46 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:29:49 root] (main_pitome.py 218): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:30:45 root] (main_pitome.py 219): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:30:48 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:30:49 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:30:55 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:30:55 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:37:11 root] (main_pitome.py 219): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:37:14 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:37:15 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:37:21 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:37:21 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:39:33 root] (main_pitome.py 219): INFO Namespace(batch_size=16, epochs=2, ratio=0.95, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.0001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:39:36 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:39:38 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:39:43 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:39:43 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:39:53 root] (utils.py 302): INFO Epoch: [0]  [    0/19704]  eta: 2 days, 6:55:57  lr_architecture: 0.001000  loss_cls: 1.1332 (1.1332)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 14.5901 (14.5901)  time: 10.0364  data: 0.0008  max mem: 6996
[2024-01-21 16:39:58 root] (utils.py 302): INFO Epoch: [0]  [   10/19704]  eta: 7:29:16  lr_architecture: 0.001000  loss_cls: 7.1873 (6.6967)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 2.4817 (5.9157)  time: 1.3688  data: 0.0001  max mem: 8651
[2024-01-21 16:40:03 root] (utils.py 302): INFO Epoch: [0]  [   20/19704]  eta: 5:11:40  lr_architecture: 0.001000  loss_cls: 7.1776 (6.9110)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.6502 (3.8500)  time: 0.4957  data: 0.0001  max mem: 8651
[2024-01-21 16:40:08 root] (utils.py 302): INFO Epoch: [0]  [   30/19704]  eta: 4:22:53  lr_architecture: 0.001000  loss_cls: 7.1054 (6.9647)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.4636 (3.0519)  time: 0.4899  data: 0.0001  max mem: 8651
[2024-01-21 16:40:13 root] (utils.py 302): INFO Epoch: [0]  [   40/19704]  eta: 3:57:55  lr_architecture: 0.001000  loss_cls: 7.0945 (6.9991)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.3467 (2.6464)  time: 0.4907  data: 0.0001  max mem: 8652
[2024-01-21 16:40:18 root] (utils.py 302): INFO Epoch: [0]  [   50/19704]  eta: 3:42:39  lr_architecture: 0.001000  loss_cls: 7.1033 (7.0224)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.1890 (2.3518)  time: 0.4906  data: 0.0001  max mem: 8652
[2024-01-21 16:40:23 root] (utils.py 302): INFO Epoch: [0]  [   60/19704]  eta: 3:32:24  lr_architecture: 0.001000  loss_cls: 7.0722 (7.0226)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0877 (2.1393)  time: 0.4905  data: 0.0001  max mem: 8652
[2024-01-21 16:40:28 root] (utils.py 302): INFO Epoch: [0]  [   70/19704]  eta: 3:24:58  lr_architecture: 0.001000  loss_cls: 7.0617 (7.0314)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0673 (1.9903)  time: 0.4904  data: 0.0001  max mem: 8652
[2024-01-21 16:40:33 root] (utils.py 302): INFO Epoch: [0]  [   80/19704]  eta: 3:19:24  lr_architecture: 0.001000  loss_cls: 7.0754 (7.0334)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0308 (1.8719)  time: 0.4906  data: 0.0001  max mem: 8652
[2024-01-21 16:40:37 root] (utils.py 302): INFO Epoch: [0]  [   90/19704]  eta: 3:15:01  lr_architecture: 0.001000  loss_cls: 7.0705 (7.0348)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 1.0060 (1.7741)  time: 0.4908  data: 0.0001  max mem: 8652
[2024-01-21 16:40:42 root] (utils.py 302): INFO Epoch: [0]  [  100/19704]  eta: 3:11:29  lr_architecture: 0.001000  loss_cls: 6.9774 (7.0299)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 0.9568 (1.6908)  time: 0.4904  data: 0.0001  max mem: 8652
[2024-01-21 16:40:47 root] (utils.py 302): INFO Epoch: [0]  [  110/19704]  eta: 3:08:35  lr_architecture: 0.001000  loss_cls: 6.9597 (7.0246)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 0.9513 (1.6245)  time: 0.4907  data: 0.0001  max mem: 8652
[2024-01-21 16:40:52 root] (utils.py 302): INFO Epoch: [0]  [  120/19704]  eta: 3:06:08  lr_architecture: 0.001000  loss_cls: 7.0307 (7.0237)  loss_flops: 1013.3037 (1013.3037)  flops: 34.8324 (34.8324)  grad_norm: 0.9818 (1.5726)  time: 0.4907  data: 0.0001  max mem: 8652
[2024-01-21 16:41:33 root] (main_pitome.py 219): INFO Namespace(batch_size=32, epochs=2, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:41:36 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:41:37 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:41:43 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:41:43 root] (main_pitome.py 446): INFO Start training for 2 epochs
[2024-01-21 16:41:53 root] (utils.py 302): INFO Epoch: [0]  [   0/9852]  eta: 1 day, 2:42:25  lr_architecture: 0.010000  loss_cls: 1.4444 (1.4444)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 12.0292 (12.0292)  time: 9.7590  data: 0.0008  max mem: 7078
[2024-01-21 16:41:58 root] (utils.py 302): INFO Epoch: [0]  [  10/9852]  eta: 3:46:17  lr_architecture: 0.010000  loss_cls: 7.7493 (7.3044)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.8356 (3.0640)  time: 1.3796  data: 0.0001  max mem: 10516
[2024-01-21 16:42:03 root] (utils.py 302): INFO Epoch: [0]  [  20/9852]  eta: 2:39:35  lr_architecture: 0.010000  loss_cls: 7.8681 (7.5779)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.5667 (2.2947)  time: 0.5346  data: 0.0001  max mem: 10516
[2024-01-21 16:42:09 root] (utils.py 302): INFO Epoch: [0]  [  30/9852]  eta: 2:15:49  lr_architecture: 0.010000  loss_cls: 7.6415 (7.5190)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.1799 (1.8932)  time: 0.5273  data: 0.0001  max mem: 10516
[2024-01-21 16:42:14 root] (utils.py 302): INFO Epoch: [0]  [  40/9852]  eta: 2:03:36  lr_architecture: 0.010000  loss_cls: 7.2137 (7.4391)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.9142 (1.6448)  time: 0.5270  data: 0.0001  max mem: 10516
[2024-01-21 16:42:19 root] (utils.py 302): INFO Epoch: [0]  [  50/9852]  eta: 1:56:07  lr_architecture: 0.010000  loss_cls: 7.1687 (7.3724)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8550 (1.4894)  time: 0.5265  data: 0.0001  max mem: 10516
[2024-01-21 16:42:24 root] (utils.py 302): INFO Epoch: [0]  [  60/9852]  eta: 1:51:04  lr_architecture: 0.010000  loss_cls: 7.0796 (7.3240)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8443 (1.3881)  time: 0.5264  data: 0.0001  max mem: 10516
[2024-01-21 16:42:30 root] (utils.py 302): INFO Epoch: [0]  [  70/9852]  eta: 1:47:24  lr_architecture: 0.010000  loss_cls: 7.0796 (7.2877)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8859 (1.3188)  time: 0.5263  data: 0.0001  max mem: 10516
[2024-01-21 16:42:35 root] (utils.py 302): INFO Epoch: [0]  [  80/9852]  eta: 1:44:37  lr_architecture: 0.010000  loss_cls: 7.0802 (7.2603)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8728 (1.2655)  time: 0.5258  data: 0.0001  max mem: 10516
[2024-01-21 16:42:40 root] (utils.py 302): INFO Epoch: [0]  [  90/9852]  eta: 1:42:25  lr_architecture: 0.010000  loss_cls: 7.0105 (7.2346)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8262 (1.2171)  time: 0.5256  data: 0.0001  max mem: 10516
[2024-01-21 16:42:45 root] (utils.py 302): INFO Epoch: [0]  [ 100/9852]  eta: 1:40:39  lr_architecture: 0.010000  loss_cls: 7.0077 (7.2163)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.8447 (1.1850)  time: 0.5256  data: 0.0001  max mem: 10516
[2024-01-21 16:43:23 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=3, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:43:26 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:43:27 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:43:35 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:43:35 root] (main_pitome.py 446): INFO Start training for 3 epochs
[2024-01-21 16:43:44 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:56:46  lr_architecture: 0.010000  loss_cls: 1.4604 (1.4604)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 9.5476 (9.5476)  time: 9.4613  data: 0.0008  max mem: 11145
[2024-01-21 16:43:50 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:57:26  lr_architecture: 0.010000  loss_cls: 7.7666 (7.1822)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.2086 (2.0786)  time: 1.4334  data: 0.0002  max mem: 14587
[2024-01-21 16:43:57 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:12  lr_architecture: 0.010000  loss_cls: 7.5886 (7.3011)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.0176 (1.4736)  time: 0.6211  data: 0.0001  max mem: 14587
[2024-01-21 16:44:03 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:13:43  lr_architecture: 0.010000  loss_cls: 7.2302 (7.2380)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6878 (1.1953)  time: 0.6120  data: 0.0001  max mem: 14587
[2024-01-21 16:44:09 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:07:44  lr_architecture: 0.010000  loss_cls: 7.1034 (7.2012)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5932 (1.0519)  time: 0.6112  data: 0.0001  max mem: 14587
[2024-01-21 16:44:15 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:04  lr_architecture: 0.010000  loss_cls: 7.0886 (7.1748)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6034 (0.9680)  time: 0.6099  data: 0.0001  max mem: 14587
[2024-01-21 16:44:21 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:01:33  lr_architecture: 0.010000  loss_cls: 7.0182 (7.1446)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5947 (0.9048)  time: 0.6098  data: 0.0001  max mem: 14587
[2024-01-21 16:44:27 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 0:59:43  lr_architecture: 0.010000  loss_cls: 6.9988 (7.1248)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5919 (0.8626)  time: 0.6096  data: 0.0001  max mem: 14587
[2024-01-21 16:44:33 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:20  lr_architecture: 0.010000  loss_cls: 6.9988 (7.1092)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5755 (0.8258)  time: 0.6103  data: 0.0001  max mem: 14587
[2024-01-21 16:44:39 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:12  lr_architecture: 0.010000  loss_cls: 7.0243 (7.1019)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5978 (0.8059)  time: 0.6100  data: 0.0001  max mem: 14587
[2024-01-21 16:44:45 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:18  lr_architecture: 0.010000  loss_cls: 7.0229 (7.0903)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6228 (0.7881)  time: 0.6103  data: 0.0001  max mem: 14587
[2024-01-21 16:44:51 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:55:32  lr_architecture: 0.010000  loss_cls: 7.0013 (7.0829)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6215 (0.7723)  time: 0.6105  data: 0.0001  max mem: 14587
[2024-01-21 16:44:58 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:54:53  lr_architecture: 0.010000  loss_cls: 6.9882 (7.0753)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5993 (0.7575)  time: 0.6100  data: 0.0001  max mem: 14587
[2024-01-21 16:45:04 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:18  lr_architecture: 0.010000  loss_cls: 6.9865 (7.0677)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5901 (0.7444)  time: 0.6101  data: 0.0001  max mem: 14587
[2024-01-21 16:45:10 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:53:47  lr_architecture: 0.010000  loss_cls: 6.9771 (7.0632)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6096 (0.7388)  time: 0.6093  data: 0.0001  max mem: 14587
[2024-01-21 16:45:16 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:21  lr_architecture: 0.010000  loss_cls: 7.0013 (7.0588)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6405 (0.7313)  time: 0.6098  data: 0.0001  max mem: 14587
[2024-01-21 16:45:22 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:52:56  lr_architecture: 0.010000  loss_cls: 7.0047 (7.0566)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6468 (0.7292)  time: 0.6110  data: 0.0001  max mem: 14587
[2024-01-21 16:45:28 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:52:34  lr_architecture: 0.010000  loss_cls: 6.9883 (7.0530)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6568 (0.7253)  time: 0.6111  data: 0.0001  max mem: 14587
[2024-01-21 16:45:34 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:14  lr_architecture: 0.010000  loss_cls: 6.9883 (7.0500)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6547 (0.7218)  time: 0.6102  data: 0.0001  max mem: 14587
[2024-01-21 16:45:40 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:51:55  lr_architecture: 0.010000  loss_cls: 7.0061 (7.0467)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6482 (0.7182)  time: 0.6100  data: 0.0001  max mem: 14587
[2024-01-21 16:45:46 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:51:37  lr_architecture: 0.010000  loss_cls: 6.9816 (7.0445)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6583 (0.7165)  time: 0.6103  data: 0.0001  max mem: 14587
[2024-01-21 16:45:53 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:20  lr_architecture: 0.010000  loss_cls: 6.9739 (7.0411)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6292 (0.7114)  time: 0.6105  data: 0.0001  max mem: 14587
[2024-01-21 16:45:59 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:04  lr_architecture: 0.010000  loss_cls: 6.9692 (7.0384)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6292 (0.7089)  time: 0.6104  data: 0.0001  max mem: 14587
[2024-01-21 16:46:05 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:50:50  lr_architecture: 0.010000  loss_cls: 6.9801 (7.0365)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6738 (0.7082)  time: 0.6106  data: 0.0001  max mem: 14587
[2024-01-21 16:46:11 root] (utils.py 302): INFO Epoch: [0]  [ 240/4926]  eta: 0:50:36  lr_architecture: 0.010000  loss_cls: 6.9945 (7.0344)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6858 (0.7072)  time: 0.6112  data: 0.0001  max mem: 14587
[2024-01-21 16:46:50 root] (main_pitome.py 219): INFO Namespace(batch_size=128, epochs=5, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:46:53 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:46:55 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:47:03 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:47:03 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:47:13 root] (utils.py 302): INFO Epoch: [0]  [   0/2463]  eta: 6:43:06  lr_architecture: 0.010000  loss_cls: 1.7568 (1.7568)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 5.5643 (5.5643)  time: 9.8200  data: 0.0009  max mem: 19309
[2024-01-21 16:47:41 root] (main_pitome.py 219): INFO Namespace(batch_size=80, epochs=5, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:47:45 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:47:46 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:47:53 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:47:53 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:48:02 root] (utils.py 302): INFO Epoch: [0]  [   0/3940]  eta: 10:18:16  lr_architecture: 0.010000  loss_cls: 1.5376 (1.5376)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 7.8372 (7.8372)  time: 9.4152  data: 0.0008  max mem: 13172
[2024-01-21 16:48:09 root] (utils.py 302): INFO Epoch: [0]  [  10/3940]  eta: 1:37:09  lr_architecture: 0.010000  loss_cls: 7.5515 (7.0943)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.1815 (1.9207)  time: 1.4832  data: 0.0002  max mem: 16611
[2024-01-21 16:48:16 root] (utils.py 302): INFO Epoch: [0]  [  20/3940]  eta: 1:11:24  lr_architecture: 0.010000  loss_cls: 7.5507 (7.2606)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.9072 (1.3803)  time: 0.6769  data: 0.0001  max mem: 16611
[2024-01-21 16:48:22 root] (utils.py 302): INFO Epoch: [0]  [  30/3940]  eta: 1:02:13  lr_architecture: 0.010000  loss_cls: 7.1789 (7.2224)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6089 (1.1205)  time: 0.6642  data: 0.0001  max mem: 16611
[2024-01-21 16:48:29 root] (utils.py 302): INFO Epoch: [0]  [  40/3940]  eta: 0:57:29  lr_architecture: 0.010000  loss_cls: 7.0909 (7.1818)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5684 (0.9822)  time: 0.6655  data: 0.0001  max mem: 16611
[2024-01-21 16:48:35 root] (utils.py 302): INFO Epoch: [0]  [  50/3940]  eta: 0:54:33  lr_architecture: 0.010000  loss_cls: 7.0291 (7.1453)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5125 (0.8878)  time: 0.6656  data: 0.0001  max mem: 16611
[2024-01-21 16:48:42 root] (utils.py 302): INFO Epoch: [0]  [  60/3940]  eta: 0:52:32  lr_architecture: 0.010000  loss_cls: 7.0075 (7.1212)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5004 (0.8275)  time: 0.6646  data: 0.0001  max mem: 16611
[2024-01-21 16:48:49 root] (utils.py 302): INFO Epoch: [0]  [  70/3940]  eta: 0:51:03  lr_architecture: 0.010000  loss_cls: 6.9987 (7.1047)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5141 (0.7836)  time: 0.6649  data: 0.0001  max mem: 16611
[2024-01-21 16:48:55 root] (utils.py 302): INFO Epoch: [0]  [  80/3940]  eta: 0:49:55  lr_architecture: 0.010000  loss_cls: 6.9930 (7.0914)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5245 (0.7501)  time: 0.6653  data: 0.0001  max mem: 16611
[2024-01-21 16:49:02 root] (utils.py 302): INFO Epoch: [0]  [  90/3940]  eta: 0:49:01  lr_architecture: 0.010000  loss_cls: 6.9902 (7.0799)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5218 (0.7254)  time: 0.6657  data: 0.0001  max mem: 16611
[2024-01-21 16:49:09 root] (utils.py 302): INFO Epoch: [0]  [ 100/3940]  eta: 0:48:16  lr_architecture: 0.010000  loss_cls: 6.9698 (7.0700)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5230 (0.7064)  time: 0.6660  data: 0.0001  max mem: 16611
[2024-01-21 16:49:15 root] (utils.py 302): INFO Epoch: [0]  [ 110/3940]  eta: 0:47:38  lr_architecture: 0.010000  loss_cls: 6.9641 (7.0605)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5271 (0.6889)  time: 0.6659  data: 0.0001  max mem: 16611
[2024-01-21 16:49:22 root] (utils.py 302): INFO Epoch: [0]  [ 120/3940]  eta: 0:47:05  lr_architecture: 0.010000  loss_cls: 6.9898 (7.0561)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5311 (0.6777)  time: 0.6657  data: 0.0001  max mem: 16611
[2024-01-21 16:49:29 root] (utils.py 302): INFO Epoch: [0]  [ 130/3940]  eta: 0:46:36  lr_architecture: 0.010000  loss_cls: 6.9831 (7.0499)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5357 (0.6670)  time: 0.6651  data: 0.0001  max mem: 16611
[2024-01-21 16:49:35 root] (utils.py 302): INFO Epoch: [0]  [ 140/3940]  eta: 0:46:10  lr_architecture: 0.010000  loss_cls: 6.9827 (7.0464)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5181 (0.6600)  time: 0.6656  data: 0.0001  max mem: 16611
[2024-01-21 16:49:42 root] (utils.py 302): INFO Epoch: [0]  [ 150/3940]  eta: 0:45:47  lr_architecture: 0.010000  loss_cls: 6.9888 (7.0416)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5398 (0.6537)  time: 0.6664  data: 0.0001  max mem: 16611
[2024-01-21 16:49:49 root] (utils.py 302): INFO Epoch: [0]  [ 160/3940]  eta: 0:45:26  lr_architecture: 0.010000  loss_cls: 6.9818 (7.0382)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5398 (0.6481)  time: 0.6671  data: 0.0001  max mem: 16611
[2024-01-21 16:49:55 root] (utils.py 302): INFO Epoch: [0]  [ 170/3940]  eta: 0:45:07  lr_architecture: 0.010000  loss_cls: 6.9818 (7.0346)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5600 (0.6432)  time: 0.6675  data: 0.0001  max mem: 16611
[2024-01-21 16:50:02 root] (utils.py 302): INFO Epoch: [0]  [ 180/3940]  eta: 0:44:50  lr_architecture: 0.010000  loss_cls: 6.9972 (7.0326)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5694 (0.6411)  time: 0.6674  data: 0.0001  max mem: 16611
[2024-01-21 16:50:09 root] (utils.py 302): INFO Epoch: [0]  [ 190/3940]  eta: 0:44:33  lr_architecture: 0.010000  loss_cls: 6.9976 (7.0299)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5969 (0.6390)  time: 0.6665  data: 0.0001  max mem: 16611
[2024-01-21 16:55:52 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.9425, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:55:56 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:55:57 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:56:04 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:56:04 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:56:13 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:52:48  lr_architecture: 0.010000  loss_cls: 1.4564 (1.4564)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 8.7332 (8.7332)  time: 9.4131  data: 0.0008  max mem: 11145
[2024-01-21 16:56:20 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:58:22  lr_architecture: 0.010000  loss_cls: 7.6033 (7.1133)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.2290 (2.5742)  time: 1.4449  data: 0.0001  max mem: 14587
[2024-01-21 16:56:26 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:58  lr_architecture: 0.010000  loss_cls: 7.4827 (7.2934)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 1.0066 (1.7664)  time: 0.6333  data: 0.0001  max mem: 14587
[2024-01-21 16:56:32 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:20  lr_architecture: 0.010000  loss_cls: 7.2693 (7.2528)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.7483 (1.4092)  time: 0.6174  data: 0.0001  max mem: 14587
[2024-01-21 16:56:38 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:18  lr_architecture: 0.010000  loss_cls: 7.1029 (7.2127)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6479 (1.2223)  time: 0.6157  data: 0.0001  max mem: 14587
[2024-01-21 16:56:44 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:35  lr_architecture: 0.010000  loss_cls: 7.0711 (7.1790)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6165 (1.1017)  time: 0.6148  data: 0.0001  max mem: 14587
[2024-01-21 16:56:51 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:04  lr_architecture: 0.010000  loss_cls: 7.0114 (7.1481)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5763 (1.0137)  time: 0.6149  data: 0.0001  max mem: 14587
[2024-01-21 16:56:57 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:15  lr_architecture: 0.010000  loss_cls: 6.9830 (7.1269)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5724 (0.9545)  time: 0.6161  data: 0.0001  max mem: 14587
[2024-01-21 16:57:03 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:51  lr_architecture: 0.010000  loss_cls: 6.9924 (7.1121)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5724 (0.9083)  time: 0.6165  data: 0.0001  max mem: 14587
[2024-01-21 16:57:09 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:44  lr_architecture: 0.010000  loss_cls: 7.0259 (7.1037)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5978 (0.8753)  time: 0.6168  data: 0.0001  max mem: 14587
[2024-01-21 16:57:15 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:50  lr_architecture: 0.010000  loss_cls: 7.0195 (7.0930)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6027 (0.8503)  time: 0.6182  data: 0.0001  max mem: 14587
[2024-01-21 16:57:22 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:56:05  lr_architecture: 0.010000  loss_cls: 7.0042 (7.0838)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.5893 (0.8274)  time: 0.6189  data: 0.0001  max mem: 14587
[2024-01-21 16:57:28 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:27  lr_architecture: 0.010000  loss_cls: 6.9987 (7.0762)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6008 (0.8094)  time: 0.6190  data: 0.0001  max mem: 14587
[2024-01-21 16:57:34 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:52  lr_architecture: 0.010000  loss_cls: 6.9866 (7.0682)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6098 (0.7932)  time: 0.6179  data: 0.0001  max mem: 14587
[2024-01-21 16:57:40 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:22  lr_architecture: 0.010000  loss_cls: 6.9729 (7.0631)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6150 (0.7830)  time: 0.6176  data: 0.0001  max mem: 14587
[2024-01-21 16:57:46 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:55  lr_architecture: 0.010000  loss_cls: 7.0038 (7.0591)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6277 (0.7720)  time: 0.6184  data: 0.0001  max mem: 14587
[2024-01-21 16:57:52 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:31  lr_architecture: 0.010000  loss_cls: 7.0038 (7.0563)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6486 (0.7663)  time: 0.6187  data: 0.0001  max mem: 14587
[2024-01-21 16:57:59 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:53:09  lr_architecture: 0.010000  loss_cls: 6.9914 (7.0526)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6558 (0.7601)  time: 0.6184  data: 0.0001  max mem: 14587
[2024-01-21 16:58:05 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:48  lr_architecture: 0.010000  loss_cls: 6.9937 (7.0493)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6606 (0.7542)  time: 0.6177  data: 0.0001  max mem: 14587
[2024-01-21 16:58:11 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:29  lr_architecture: 0.010000  loss_cls: 7.0026 (7.0461)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6469 (0.7486)  time: 0.6183  data: 0.0001  max mem: 14587
[2024-01-21 16:58:17 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:52:12  lr_architecture: 0.010000  loss_cls: 6.9762 (7.0435)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6659 (0.7464)  time: 0.6182  data: 0.0001  max mem: 14587
[2024-01-21 16:58:23 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:55  lr_architecture: 0.010000  loss_cls: 6.9762 (7.0401)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6335 (0.7410)  time: 0.6171  data: 0.0001  max mem: 14587
[2024-01-21 16:58:30 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:39  lr_architecture: 0.010000  loss_cls: 6.9745 (7.0374)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6337 (0.7367)  time: 0.6180  data: 0.0001  max mem: 14587
[2024-01-21 16:58:36 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:51:25  lr_architecture: 0.010000  loss_cls: 6.9745 (7.0354)  loss_flops: 857.0523 (857.0523)  flops: 32.2755 (32.2755)  grad_norm: 0.6513 (0.7340)  time: 0.6188  data: 0.0001  max mem: 14587
[2024-01-21 16:59:18 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 16:59:21 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 16:59:23 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 16:59:29 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 16:59:29 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 16:59:39 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 13:00:00  lr_architecture: 0.010000  loss_cls: 1.4759 (1.4759)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.4229 (9.4229)  time: 9.5006  data: 0.0008  max mem: 10987
[2024-01-21 16:59:45 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:59:06  lr_architecture: 0.010000  loss_cls: 7.6885 (7.1404)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 1.1803 (2.0192)  time: 1.4538  data: 0.0001  max mem: 14435
[2024-01-21 16:59:51 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:26:11  lr_architecture: 0.010000  loss_cls: 7.4638 (7.2614)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.9764 (1.4387)  time: 0.6318  data: 0.0001  max mem: 14435
[2024-01-21 16:59:58 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:24  lr_architecture: 0.010000  loss_cls: 7.2144 (7.2081)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6895 (1.1816)  time: 0.6138  data: 0.0001  max mem: 14435
[2024-01-21 17:00:04 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:20  lr_architecture: 0.010000  loss_cls: 7.0707 (7.1706)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6252 (1.0374)  time: 0.6137  data: 0.0001  max mem: 14435
[2024-01-21 17:00:10 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:36  lr_architecture: 0.010000  loss_cls: 7.0614 (7.1461)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5731 (0.9519)  time: 0.6140  data: 0.0001  max mem: 14435
[2024-01-21 17:00:16 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:03  lr_architecture: 0.010000  loss_cls: 7.0317 (7.1220)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5753 (0.8909)  time: 0.6135  data: 0.0001  max mem: 14435
[2024-01-21 17:00:22 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:13  lr_architecture: 0.010000  loss_cls: 7.0104 (7.1043)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5796 (0.8483)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:00:28 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:48  lr_architecture: 0.010000  loss_cls: 7.0104 (7.0939)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5887 (0.8158)  time: 0.6151  data: 0.0001  max mem: 14435
[2024-01-21 17:00:34 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:40  lr_architecture: 0.010000  loss_cls: 7.0207 (7.0867)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5943 (0.7935)  time: 0.6141  data: 0.0001  max mem: 14435
[2024-01-21 17:00:41 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:45  lr_architecture: 0.010000  loss_cls: 7.0184 (7.0782)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6133 (0.7774)  time: 0.6146  data: 0.0001  max mem: 14435
[2024-01-21 17:00:47 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:55:58  lr_architecture: 0.010000  loss_cls: 7.0080 (7.0709)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6184 (0.7631)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:00:53 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:18  lr_architecture: 0.010000  loss_cls: 7.0117 (7.0658)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6203 (0.7528)  time: 0.6132  data: 0.0001  max mem: 14435
[2024-01-21 17:00:59 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:42  lr_architecture: 0.010000  loss_cls: 6.9800 (7.0577)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5944 (0.7393)  time: 0.6124  data: 0.0001  max mem: 14435
[2024-01-21 17:01:05 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:11  lr_architecture: 0.010000  loss_cls: 6.9698 (7.0533)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5944 (0.7343)  time: 0.6123  data: 0.0001  max mem: 14435
[2024-01-21 17:01:11 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:43  lr_architecture: 0.010000  loss_cls: 7.0036 (7.0502)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6336 (0.7273)  time: 0.6133  data: 0.0001  max mem: 14435
[2024-01-21 17:01:17 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:19  lr_architecture: 0.010000  loss_cls: 7.0048 (7.0475)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6168 (0.7225)  time: 0.6144  data: 0.0001  max mem: 14435
[2024-01-21 17:01:24 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:52:56  lr_architecture: 0.010000  loss_cls: 6.9857 (7.0446)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6369 (0.7186)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:30 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:36  lr_architecture: 0.010000  loss_cls: 6.9970 (7.0414)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6385 (0.7139)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:36 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:16  lr_architecture: 0.010000  loss_cls: 6.9970 (7.0387)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6392 (0.7104)  time: 0.6148  data: 0.0001  max mem: 14435
[2024-01-21 17:01:42 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:51:58  lr_architecture: 0.010000  loss_cls: 6.9693 (7.0363)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6539 (0.7093)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:48 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:42  lr_architecture: 0.010000  loss_cls: 6.9737 (7.0336)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6626 (0.7069)  time: 0.6145  data: 0.0001  max mem: 14435
[2024-01-21 17:01:54 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:26  lr_architecture: 0.010000  loss_cls: 6.9845 (7.0311)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6700 (0.7048)  time: 0.6146  data: 0.0001  max mem: 14435
[2024-01-21 17:07:01 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 17:07:05 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:07:06 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:07:13 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 17:07:13 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 17:08:29 root] (main_pitome.py 219): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, arch_lr=0.01, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, arch_min_lr=0.001, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 17:08:33 root] (main_pitome.py 264): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:08:34 root] (main_pitome.py 304): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:08:40 root] (main_pitome.py 391): INFO number of params: 304326632
[2024-01-21 17:08:40 root] (main_pitome.py 446): INFO Start training for 5 epochs
[2024-01-21 17:08:50 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:49:22  lr_architecture: 0.010000  loss_cls: 1.4559 (1.4559)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 8.7347 (8.7347)  time: 9.3711  data: 0.0009  max mem: 10987
[2024-01-21 17:08:56 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:57:00  lr_architecture: 0.010000  loss_cls: 7.8023 (7.2504)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 1.2888 (2.1405)  time: 1.4281  data: 0.0002  max mem: 14435
[2024-01-21 17:09:02 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:01  lr_architecture: 0.010000  loss_cls: 7.7414 (7.4149)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 1.0581 (1.5520)  time: 0.6233  data: 0.0001  max mem: 14435
[2024-01-21 17:09:08 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:13:37  lr_architecture: 0.010000  loss_cls: 7.3733 (7.3439)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.7337 (1.2633)  time: 0.6129  data: 0.0001  max mem: 14435
[2024-01-21 17:09:15 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:07:44  lr_architecture: 0.010000  loss_cls: 7.1085 (7.2800)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.6310 (1.1065)  time: 0.6136  data: 0.0001  max mem: 14435
[2024-01-21 17:09:21 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:05  lr_architecture: 0.010000  loss_cls: 7.0474 (7.2300)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5784 (0.9989)  time: 0.6128  data: 0.0001  max mem: 14435
[2024-01-21 17:09:27 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:01:38  lr_architecture: 0.010000  loss_cls: 6.9867 (7.1892)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5475 (0.9240)  time: 0.6128  data: 0.0001  max mem: 14435
[2024-01-21 17:09:33 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 0:59:49  lr_architecture: 0.010000  loss_cls: 6.9852 (7.1606)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5477 (0.8724)  time: 0.6132  data: 0.0001  max mem: 14435
[2024-01-21 17:09:39 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:26  lr_architecture: 0.010000  loss_cls: 6.9881 (7.1396)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5538 (0.8330)  time: 0.6126  data: 0.0001  max mem: 14435
[2024-01-21 17:09:45 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:21  lr_architecture: 0.010000  loss_cls: 6.9977 (7.1281)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5538 (0.8083)  time: 0.6138  data: 0.0001  max mem: 14435
[2024-01-21 17:09:51 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:26  lr_architecture: 0.010000  loss_cls: 7.0243 (7.1150)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 0.5798 (0.7882)  time: 0.6134  data: 0.0001  max mem: 14435
[2024-01-21 17:40:55 root] (main_pitome.py 215): INFO Namespace(batch_size=64, epochs=5, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 17:40:59 root] (main_pitome.py 260): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:41:00 root] (main_pitome.py 300): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:41:07 root] (main_pitome.py 386): INFO number of params: 304326632
[2024-01-21 17:41:07 root] (main_pitome.py 441): INFO Start training for 5 epochs
[2024-01-21 17:41:16 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 12:59:05  lr_architecture: 0.000250  loss_cls: 1.4726 (1.4726)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.1179 (9.1179)  time: 9.4895  data: 0.0009  max mem: 10987
[2024-01-21 17:41:23 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:58:33  lr_architecture: 0.000250  loss_cls: 3.3316 (3.4443)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.0908 (6.7913)  time: 1.4471  data: 0.0002  max mem: 14435
[2024-01-21 17:41:29 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:26:03  lr_architecture: 0.000250  loss_cls: 3.4449 (3.3910)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2937 (6.0860)  time: 0.6305  data: 0.0001  max mem: 14435
[2024-01-21 17:41:35 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:26  lr_architecture: 0.000250  loss_cls: 3.4449 (3.3514)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9420 (5.7617)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:41:41 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:24  lr_architecture: 0.000250  loss_cls: 3.1913 (3.2815)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2213 (5.6275)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:41:48 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:43  lr_architecture: 0.000250  loss_cls: 3.1913 (3.3272)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8586 (5.4696)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:41:54 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:10  lr_architecture: 0.000250  loss_cls: 3.3563 (3.3275)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8267 (5.3690)  time: 0.6162  data: 0.0001  max mem: 14435
[2024-01-21 17:42:00 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:20  lr_architecture: 0.000250  loss_cls: 3.3684 (3.3665)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8266 (5.2887)  time: 0.6163  data: 0.0001  max mem: 14435
[2024-01-21 17:42:06 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:56  lr_architecture: 0.000250  loss_cls: 3.3122 (3.3194)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8266 (5.2455)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:42:12 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:49  lr_architecture: 0.000250  loss_cls: 3.3109 (3.3363)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6682 (5.1779)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:42:18 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:54  lr_architecture: 0.000250  loss_cls: 3.3109 (3.3223)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6899 (5.1399)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:42:25 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:56:08  lr_architecture: 0.000250  loss_cls: 3.0002 (3.2971)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7572 (5.0956)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:42:31 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:29  lr_architecture: 0.000250  loss_cls: 3.1246 (3.3039)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7733 (5.0728)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:42:37 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:55  lr_architecture: 0.000250  loss_cls: 3.2831 (3.2812)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7034 (5.0340)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:42:43 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:24  lr_architecture: 0.000250  loss_cls: 3.4456 (3.2969)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.4624 (4.9954)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:42:49 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:57  lr_architecture: 0.000250  loss_cls: 3.5293 (3.3044)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.4797 (4.9662)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 17:42:55 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:33  lr_architecture: 0.000250  loss_cls: 3.3355 (3.3075)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.4463 (4.9276)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:43:02 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:53:11  lr_architecture: 0.000250  loss_cls: 3.3355 (3.3148)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2543 (4.8925)  time: 0.6191  data: 0.0001  max mem: 14435
[2024-01-21 17:43:08 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:50  lr_architecture: 0.000250  loss_cls: 3.3552 (3.3069)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2543 (4.8659)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:43:14 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:31  lr_architecture: 0.000250  loss_cls: 3.1004 (3.2942)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3014 (4.8381)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:43:20 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:52:13  lr_architecture: 0.000250  loss_cls: 3.2463 (3.3053)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3791 (4.8190)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:43:26 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:57  lr_architecture: 0.000250  loss_cls: 3.5965 (3.3115)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3336 (4.7891)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:43:33 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:41  lr_architecture: 0.000250  loss_cls: 3.3923 (3.3024)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1175 (4.7570)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:43:39 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:51:26  lr_architecture: 0.000250  loss_cls: 3.3923 (3.3041)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0804 (4.7331)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:43:45 root] (utils.py 302): INFO Epoch: [0]  [ 240/4926]  eta: 0:51:12  lr_architecture: 0.000250  loss_cls: 3.5090 (3.3112)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2741 (4.7161)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:43:51 root] (utils.py 302): INFO Epoch: [0]  [ 250/4926]  eta: 0:50:59  lr_architecture: 0.000250  loss_cls: 3.5324 (3.3144)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3100 (4.7060)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:43:57 root] (utils.py 302): INFO Epoch: [0]  [ 260/4926]  eta: 0:50:45  lr_architecture: 0.000250  loss_cls: 3.1607 (3.3013)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2500 (4.6867)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:44:03 root] (utils.py 302): INFO Epoch: [0]  [ 270/4926]  eta: 0:50:33  lr_architecture: 0.000250  loss_cls: 3.1607 (3.3104)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1848 (4.6718)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:44:10 root] (utils.py 302): INFO Epoch: [0]  [ 280/4926]  eta: 0:50:21  lr_architecture: 0.000250  loss_cls: 3.4399 (3.3107)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1996 (4.6519)  time: 0.6191  data: 0.0001  max mem: 14435
[2024-01-21 17:44:16 root] (utils.py 302): INFO Epoch: [0]  [ 290/4926]  eta: 0:50:10  lr_architecture: 0.000250  loss_cls: 3.6685 (3.3218)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1996 (4.6436)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:44:22 root] (utils.py 302): INFO Epoch: [0]  [ 300/4926]  eta: 0:49:58  lr_architecture: 0.000250  loss_cls: 3.6541 (3.3190)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1383 (4.6274)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:44:28 root] (utils.py 302): INFO Epoch: [0]  [ 310/4926]  eta: 0:49:48  lr_architecture: 0.000250  loss_cls: 3.4123 (3.3219)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1383 (4.6173)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:44:34 root] (utils.py 302): INFO Epoch: [0]  [ 320/4926]  eta: 0:49:37  lr_architecture: 0.000250  loss_cls: 3.3741 (3.3184)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3412 (4.6076)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:44:41 root] (utils.py 302): INFO Epoch: [0]  [ 330/4926]  eta: 0:49:27  lr_architecture: 0.000250  loss_cls: 3.4290 (3.3218)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3926 (4.5995)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:44:47 root] (utils.py 302): INFO Epoch: [0]  [ 340/4926]  eta: 0:49:16  lr_architecture: 0.000250  loss_cls: 3.5798 (3.3319)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0816 (4.5827)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:44:53 root] (utils.py 302): INFO Epoch: [0]  [ 350/4926]  eta: 0:49:06  lr_architecture: 0.000250  loss_cls: 3.5798 (3.3396)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0780 (4.5714)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:44:59 root] (utils.py 302): INFO Epoch: [0]  [ 360/4926]  eta: 0:48:57  lr_architecture: 0.000250  loss_cls: 3.5758 (3.3438)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.2127 (4.5629)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:45:05 root] (utils.py 302): INFO Epoch: [0]  [ 370/4926]  eta: 0:48:47  lr_architecture: 0.000250  loss_cls: 3.6319 (3.3518)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1301 (4.5478)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:45:12 root] (utils.py 302): INFO Epoch: [0]  [ 380/4926]  eta: 0:48:38  lr_architecture: 0.000250  loss_cls: 3.6849 (3.3500)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0326 (4.5363)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:45:18 root] (utils.py 302): INFO Epoch: [0]  [ 390/4926]  eta: 0:48:29  lr_architecture: 0.000250  loss_cls: 3.3384 (3.3502)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0326 (4.5250)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:45:24 root] (utils.py 302): INFO Epoch: [0]  [ 400/4926]  eta: 0:48:20  lr_architecture: 0.000250  loss_cls: 3.5610 (3.3553)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0574 (4.5138)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:45:30 root] (utils.py 302): INFO Epoch: [0]  [ 410/4926]  eta: 0:48:11  lr_architecture: 0.000250  loss_cls: 3.7427 (3.3670)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1395 (4.5044)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:45:36 root] (utils.py 302): INFO Epoch: [0]  [ 420/4926]  eta: 0:48:02  lr_architecture: 0.000250  loss_cls: 3.6801 (3.3732)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0826 (4.4948)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:45:42 root] (utils.py 302): INFO Epoch: [0]  [ 430/4926]  eta: 0:47:54  lr_architecture: 0.000250  loss_cls: 3.4937 (3.3760)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0192 (4.4848)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:45:49 root] (utils.py 302): INFO Epoch: [0]  [ 440/4926]  eta: 0:47:45  lr_architecture: 0.000250  loss_cls: 3.3043 (3.3744)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9928 (4.4721)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:45:55 root] (utils.py 302): INFO Epoch: [0]  [ 450/4926]  eta: 0:47:37  lr_architecture: 0.000250  loss_cls: 3.4701 (3.3776)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7533 (4.4552)  time: 0.6194  data: 0.0001  max mem: 14435
[2024-01-21 17:46:01 root] (utils.py 302): INFO Epoch: [0]  [ 460/4926]  eta: 0:47:29  lr_architecture: 0.000250  loss_cls: 3.4806 (3.3786)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7370 (4.4458)  time: 0.6193  data: 0.0001  max mem: 14435
[2024-01-21 17:46:07 root] (utils.py 302): INFO Epoch: [0]  [ 470/4926]  eta: 0:47:20  lr_architecture: 0.000250  loss_cls: 3.3891 (3.3781)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9635 (4.4350)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:46:13 root] (utils.py 302): INFO Epoch: [0]  [ 480/4926]  eta: 0:47:12  lr_architecture: 0.000250  loss_cls: 3.5784 (3.3836)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9889 (4.4305)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:46:20 root] (utils.py 302): INFO Epoch: [0]  [ 490/4926]  eta: 0:47:04  lr_architecture: 0.000250  loss_cls: 3.4324 (3.3764)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.1521 (4.4282)  time: 0.6194  data: 0.0001  max mem: 14435
[2024-01-21 17:46:26 root] (utils.py 302): INFO Epoch: [0]  [ 500/4926]  eta: 0:46:56  lr_architecture: 0.000250  loss_cls: 3.5394 (3.3859)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.0871 (4.4180)  time: 0.6190  data: 0.0001  max mem: 14435
[2024-01-21 17:46:32 root] (utils.py 302): INFO Epoch: [0]  [ 510/4926]  eta: 0:46:48  lr_architecture: 0.000250  loss_cls: 3.6106 (3.3835)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7761 (4.4069)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:46:38 root] (utils.py 302): INFO Epoch: [0]  [ 520/4926]  eta: 0:46:41  lr_architecture: 0.000250  loss_cls: 3.5505 (3.3852)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8723 (4.3979)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:46:44 root] (utils.py 302): INFO Epoch: [0]  [ 530/4926]  eta: 0:46:34  lr_architecture: 0.000250  loss_cls: 3.5513 (3.3849)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8621 (4.3888)  time: 0.6249  data: 0.0001  max mem: 14435
[2024-01-21 17:46:51 root] (utils.py 302): INFO Epoch: [0]  [ 540/4926]  eta: 0:46:26  lr_architecture: 0.000250  loss_cls: 3.2093 (3.3813)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9374 (4.3807)  time: 0.6248  data: 0.0001  max mem: 14435
[2024-01-21 17:46:57 root] (utils.py 302): INFO Epoch: [0]  [ 550/4926]  eta: 0:46:18  lr_architecture: 0.000250  loss_cls: 3.0713 (3.3764)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9464 (4.3732)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:47:03 root] (utils.py 302): INFO Epoch: [0]  [ 560/4926]  eta: 0:46:11  lr_architecture: 0.000250  loss_cls: 3.1419 (3.3794)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9717 (4.3677)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:47:09 root] (utils.py 302): INFO Epoch: [0]  [ 570/4926]  eta: 0:46:03  lr_architecture: 0.000250  loss_cls: 3.7500 (3.3833)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7966 (4.3585)  time: 0.6194  data: 0.0001  max mem: 14435
[2024-01-21 17:47:15 root] (utils.py 302): INFO Epoch: [0]  [ 580/4926]  eta: 0:45:56  lr_architecture: 0.000250  loss_cls: 3.5404 (3.3820)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7333 (4.3502)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:47:22 root] (utils.py 302): INFO Epoch: [0]  [ 590/4926]  eta: 0:45:48  lr_architecture: 0.000250  loss_cls: 3.3455 (3.3767)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8094 (4.3409)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:47:28 root] (utils.py 302): INFO Epoch: [0]  [ 600/4926]  eta: 0:45:42  lr_architecture: 0.000250  loss_cls: 3.2500 (3.3752)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7948 (4.3303)  time: 0.6262  data: 0.0001  max mem: 14435
[2024-01-21 17:47:34 root] (utils.py 302): INFO Epoch: [0]  [ 610/4926]  eta: 0:45:34  lr_architecture: 0.000250  loss_cls: 3.5253 (3.3770)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8225 (4.3231)  time: 0.6255  data: 0.0001  max mem: 14435
[2024-01-21 17:47:40 root] (utils.py 302): INFO Epoch: [0]  [ 620/4926]  eta: 0:45:27  lr_architecture: 0.000250  loss_cls: 3.6362 (3.3795)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8725 (4.3156)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:47:46 root] (utils.py 302): INFO Epoch: [0]  [ 630/4926]  eta: 0:45:20  lr_architecture: 0.000250  loss_cls: 3.6890 (3.3864)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8833 (4.3100)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:47:53 root] (utils.py 302): INFO Epoch: [0]  [ 640/4926]  eta: 0:45:12  lr_architecture: 0.000250  loss_cls: 3.6728 (3.3859)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9091 (4.3019)  time: 0.6191  data: 0.0001  max mem: 14435
[2024-01-21 17:47:59 root] (utils.py 302): INFO Epoch: [0]  [ 650/4926]  eta: 0:45:05  lr_architecture: 0.000250  loss_cls: 3.4424 (3.3851)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7774 (4.2962)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:48:05 root] (utils.py 302): INFO Epoch: [0]  [ 660/4926]  eta: 0:44:58  lr_architecture: 0.000250  loss_cls: 3.4502 (3.3862)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7880 (4.2907)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:48:11 root] (utils.py 302): INFO Epoch: [0]  [ 670/4926]  eta: 0:44:51  lr_architecture: 0.000250  loss_cls: 3.6800 (3.3906)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.9316 (4.2878)  time: 0.6190  data: 0.0001  max mem: 14435
[2024-01-21 17:48:17 root] (utils.py 302): INFO Epoch: [0]  [ 680/4926]  eta: 0:44:44  lr_architecture: 0.000250  loss_cls: 3.7112 (3.3901)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8267 (4.2795)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:48:24 root] (utils.py 302): INFO Epoch: [0]  [ 690/4926]  eta: 0:44:36  lr_architecture: 0.000250  loss_cls: 3.5214 (3.3905)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7496 (4.2732)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:48:30 root] (utils.py 302): INFO Epoch: [0]  [ 700/4926]  eta: 0:44:29  lr_architecture: 0.000250  loss_cls: 3.5214 (3.3905)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.6901 (4.2645)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:48:36 root] (utils.py 302): INFO Epoch: [0]  [ 710/4926]  eta: 0:44:22  lr_architecture: 0.000250  loss_cls: 3.7641 (3.3952)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7017 (4.2592)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:48:42 root] (utils.py 302): INFO Epoch: [0]  [ 720/4926]  eta: 0:44:15  lr_architecture: 0.000250  loss_cls: 3.9387 (3.3986)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.8216 (4.2531)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:48:48 root] (utils.py 302): INFO Epoch: [0]  [ 730/4926]  eta: 0:44:08  lr_architecture: 0.000250  loss_cls: 3.9387 (3.4020)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 3.7615 (4.2469)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:49:19 root] (main_pitome.py 215): INFO Namespace(batch_size=64, epochs=10, ratio=0.94, model='vit_large_patch16_mae', multi_reso=False, input_size=224, drop=0.0, drop_path=0.1, model_ema=False, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.0, sched='cosine', lr=0.0001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', inat_category='name', output_dir='./log/temp', device='cuda', seed=0, resume='', autoresume=False, start_epoch=0, eval=False, dist_eval=True, num_workers=10, pin_mem=True, world_size=4, port='15662', dist_url='env://', target_flops=3.0, granularity=4, load_compression_rate=False, warmup_compression_rate=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[2024-01-21 17:49:23 root] (main_pitome.py 260): INFO Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. This will slightly alter validation results as extra duplicate entries are added to achieve equal num of samples per-process.
[2024-01-21 17:49:24 root] (main_pitome.py 300): INFO Creating model: vit_large_patch16_mae
[2024-01-21 17:49:31 root] (main_pitome.py 386): INFO number of params: 304326632
[2024-01-21 17:49:31 root] (main_pitome.py 441): INFO Start training for 10 epochs
[2024-01-21 17:49:40 root] (utils.py 302): INFO Epoch: [0]  [   0/4926]  eta: 13:02:19  lr_architecture: 0.000050  loss_cls: 1.4577 (1.4577)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.9756 (9.9756)  time: 9.5290  data: 0.0009  max mem: 10987
[2024-01-21 17:49:47 root] (utils.py 302): INFO Epoch: [0]  [  10/4926]  eta: 1:58:34  lr_architecture: 0.000050  loss_cls: 2.9176 (2.8496)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 9.9756 (10.1166)  time: 1.4473  data: 0.0002  max mem: 14435
[2024-01-21 17:49:53 root] (utils.py 302): INFO Epoch: [0]  [  20/4926]  eta: 1:25:58  lr_architecture: 0.000050  loss_cls: 2.9176 (2.8235)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 7.8741 (8.4506)  time: 0.6275  data: 0.0001  max mem: 14435
[2024-01-21 17:49:59 root] (utils.py 302): INFO Epoch: [0]  [  30/4926]  eta: 1:14:20  lr_architecture: 0.000050  loss_cls: 2.8844 (2.8086)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.3897 (7.8338)  time: 0.6162  data: 0.0001  max mem: 14435
[2024-01-21 17:50:05 root] (utils.py 302): INFO Epoch: [0]  [  40/4926]  eta: 1:08:14  lr_architecture: 0.000050  loss_cls: 2.7663 (2.7511)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.6886 (inf)  time: 0.6140  data: 0.0001  max mem: 14435
[2024-01-21 17:50:11 root] (utils.py 302): INFO Epoch: [0]  [  50/4926]  eta: 1:04:34  lr_architecture: 0.000050  loss_cls: 2.6943 (2.7910)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.6692 (inf)  time: 0.6143  data: 0.0001  max mem: 14435
[2024-01-21 17:50:17 root] (utils.py 302): INFO Epoch: [0]  [  60/4926]  eta: 1:02:03  lr_architecture: 0.000050  loss_cls: 2.8700 (2.7986)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.5364 (inf)  time: 0.6162  data: 0.0001  max mem: 14435
[2024-01-21 17:50:23 root] (utils.py 302): INFO Epoch: [0]  [  70/4926]  eta: 1:00:14  lr_architecture: 0.000050  loss_cls: 2.8719 (2.8320)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.3946 (inf)  time: 0.6157  data: 0.0001  max mem: 14435
[2024-01-21 17:50:30 root] (utils.py 302): INFO Epoch: [0]  [  80/4926]  eta: 0:58:50  lr_architecture: 0.000050  loss_cls: 2.7831 (2.7889)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.1701 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 17:50:36 root] (utils.py 302): INFO Epoch: [0]  [  90/4926]  eta: 0:57:43  lr_architecture: 0.000050  loss_cls: 2.8489 (2.8083)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.2078 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:50:42 root] (utils.py 302): INFO Epoch: [0]  [ 100/4926]  eta: 0:56:49  lr_architecture: 0.000050  loss_cls: 2.8489 (2.7994)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.4734 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:50:48 root] (utils.py 302): INFO Epoch: [0]  [ 110/4926]  eta: 0:56:03  lr_architecture: 0.000050  loss_cls: 2.6773 (2.7807)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.4934 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:50:54 root] (utils.py 302): INFO Epoch: [0]  [ 120/4926]  eta: 0:55:24  lr_architecture: 0.000050  loss_cls: 2.6773 (2.7838)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.4413 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:51:00 root] (utils.py 302): INFO Epoch: [0]  [ 130/4926]  eta: 0:54:49  lr_architecture: 0.000050  loss_cls: 2.7190 (2.7587)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 6.2094 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:51:07 root] (utils.py 302): INFO Epoch: [0]  [ 140/4926]  eta: 0:54:19  lr_architecture: 0.000050  loss_cls: 2.9024 (2.7722)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7837 (inf)  time: 0.6163  data: 0.0001  max mem: 14435
[2024-01-21 17:51:13 root] (utils.py 302): INFO Epoch: [0]  [ 150/4926]  eta: 0:53:52  lr_architecture: 0.000050  loss_cls: 3.0167 (2.7741)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8021 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:51:19 root] (utils.py 302): INFO Epoch: [0]  [ 160/4926]  eta: 0:53:28  lr_architecture: 0.000050  loss_cls: 2.8036 (2.7742)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8019 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:51:25 root] (utils.py 302): INFO Epoch: [0]  [ 170/4926]  eta: 0:53:05  lr_architecture: 0.000050  loss_cls: 2.7229 (2.7804)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6494 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 17:51:31 root] (utils.py 302): INFO Epoch: [0]  [ 180/4926]  eta: 0:52:45  lr_architecture: 0.000050  loss_cls: 2.7229 (2.7712)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6344 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:51:38 root] (utils.py 302): INFO Epoch: [0]  [ 190/4926]  eta: 0:52:26  lr_architecture: 0.000050  loss_cls: 2.5450 (2.7584)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6800 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:51:44 root] (utils.py 302): INFO Epoch: [0]  [ 200/4926]  eta: 0:52:08  lr_architecture: 0.000050  loss_cls: 2.6428 (2.7652)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6800 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:51:50 root] (utils.py 302): INFO Epoch: [0]  [ 210/4926]  eta: 0:51:52  lr_architecture: 0.000050  loss_cls: 2.7851 (2.7660)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6706 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:51:56 root] (utils.py 302): INFO Epoch: [0]  [ 220/4926]  eta: 0:51:36  lr_architecture: 0.000050  loss_cls: 2.7758 (2.7581)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7570 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:02 root] (utils.py 302): INFO Epoch: [0]  [ 230/4926]  eta: 0:51:21  lr_architecture: 0.000050  loss_cls: 2.8206 (2.7605)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7570 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:52:08 root] (utils.py 302): INFO Epoch: [0]  [ 240/4926]  eta: 0:51:07  lr_architecture: 0.000050  loss_cls: 2.8865 (2.7669)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8120 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:15 root] (utils.py 302): INFO Epoch: [0]  [ 250/4926]  eta: 0:50:54  lr_architecture: 0.000050  loss_cls: 2.9332 (2.7672)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.8120 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:21 root] (utils.py 302): INFO Epoch: [0]  [ 260/4926]  eta: 0:50:41  lr_architecture: 0.000050  loss_cls: 2.6596 (2.7559)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1731 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:52:27 root] (utils.py 302): INFO Epoch: [0]  [ 270/4926]  eta: 0:50:29  lr_architecture: 0.000050  loss_cls: 2.6596 (2.7609)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3726 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:52:33 root] (utils.py 302): INFO Epoch: [0]  [ 280/4926]  eta: 0:50:17  lr_architecture: 0.000050  loss_cls: 2.8025 (2.7604)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6691 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:39 root] (utils.py 302): INFO Epoch: [0]  [ 290/4926]  eta: 0:50:05  lr_architecture: 0.000050  loss_cls: 2.9961 (2.7658)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6690 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:52:45 root] (utils.py 302): INFO Epoch: [0]  [ 300/4926]  eta: 0:49:54  lr_architecture: 0.000050  loss_cls: 2.8379 (2.7609)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6340 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:52:52 root] (utils.py 302): INFO Epoch: [0]  [ 310/4926]  eta: 0:49:43  lr_architecture: 0.000050  loss_cls: 2.7537 (2.7613)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4217 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:52:58 root] (utils.py 302): INFO Epoch: [0]  [ 320/4926]  eta: 0:49:33  lr_architecture: 0.000050  loss_cls: 2.6982 (2.7553)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3677 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:53:04 root] (utils.py 302): INFO Epoch: [0]  [ 330/4926]  eta: 0:49:22  lr_architecture: 0.000050  loss_cls: 2.6982 (2.7563)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4007 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:53:10 root] (utils.py 302): INFO Epoch: [0]  [ 340/4926]  eta: 0:49:12  lr_architecture: 0.000050  loss_cls: 2.9523 (2.7619)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7561 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:53:16 root] (utils.py 302): INFO Epoch: [0]  [ 350/4926]  eta: 0:49:03  lr_architecture: 0.000050  loss_cls: 2.9882 (2.7671)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.9009 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:53:23 root] (utils.py 302): INFO Epoch: [0]  [ 360/4926]  eta: 0:48:53  lr_architecture: 0.000050  loss_cls: 2.9056 (2.7686)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.9009 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:53:29 root] (utils.py 302): INFO Epoch: [0]  [ 370/4926]  eta: 0:48:43  lr_architecture: 0.000050  loss_cls: 3.0117 (2.7743)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4450 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 17:53:35 root] (utils.py 302): INFO Epoch: [0]  [ 380/4926]  eta: 0:48:34  lr_architecture: 0.000050  loss_cls: 3.0030 (2.7704)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2555 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:53:41 root] (utils.py 302): INFO Epoch: [0]  [ 390/4926]  eta: 0:48:25  lr_architecture: 0.000050  loss_cls: 2.7180 (2.7693)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2525 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 17:53:47 root] (utils.py 302): INFO Epoch: [0]  [ 400/4926]  eta: 0:48:16  lr_architecture: 0.000050  loss_cls: 2.8689 (2.7722)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2586 (inf)  time: 0.6197  data: 0.0001  max mem: 14435
[2024-01-21 17:53:53 root] (utils.py 302): INFO Epoch: [0]  [ 410/4926]  eta: 0:48:07  lr_architecture: 0.000050  loss_cls: 2.9989 (2.7790)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.9010 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:54:00 root] (utils.py 302): INFO Epoch: [0]  [ 420/4926]  eta: 0:47:59  lr_architecture: 0.000050  loss_cls: 2.9989 (2.7826)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.7760 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 17:54:06 root] (utils.py 302): INFO Epoch: [0]  [ 430/4926]  eta: 0:47:50  lr_architecture: 0.000050  loss_cls: 2.8299 (2.7836)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4484 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:54:12 root] (utils.py 302): INFO Epoch: [0]  [ 440/4926]  eta: 0:47:42  lr_architecture: 0.000050  loss_cls: 2.7276 (2.7815)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4484 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:54:18 root] (utils.py 302): INFO Epoch: [0]  [ 450/4926]  eta: 0:47:33  lr_architecture: 0.000050  loss_cls: 2.7807 (2.7842)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4086 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:54:24 root] (utils.py 302): INFO Epoch: [0]  [ 460/4926]  eta: 0:47:25  lr_architecture: 0.000050  loss_cls: 2.7807 (2.7837)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4560 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:54:31 root] (utils.py 302): INFO Epoch: [0]  [ 470/4926]  eta: 0:47:17  lr_architecture: 0.000050  loss_cls: 2.7036 (2.7811)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4456 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:54:37 root] (utils.py 302): INFO Epoch: [0]  [ 480/4926]  eta: 0:47:09  lr_architecture: 0.000050  loss_cls: 2.8983 (2.7849)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2645 (inf)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:54:43 root] (utils.py 302): INFO Epoch: [0]  [ 490/4926]  eta: 0:47:01  lr_architecture: 0.000050  loss_cls: 2.8368 (2.7783)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2740 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:54:49 root] (utils.py 302): INFO Epoch: [0]  [ 500/4926]  eta: 0:46:53  lr_architecture: 0.000050  loss_cls: 2.9078 (2.7858)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2740 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 17:54:55 root] (utils.py 302): INFO Epoch: [0]  [ 510/4926]  eta: 0:46:45  lr_architecture: 0.000050  loss_cls: 2.9944 (2.7824)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3210 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 17:55:01 root] (utils.py 302): INFO Epoch: [0]  [ 520/4926]  eta: 0:46:37  lr_architecture: 0.000050  loss_cls: 2.8664 (2.7821)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2423 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:55:08 root] (utils.py 302): INFO Epoch: [0]  [ 530/4926]  eta: 0:46:31  lr_architecture: 0.000050  loss_cls: 2.8664 (2.7811)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0388 (inf)  time: 0.6262  data: 0.0001  max mem: 14435
[2024-01-21 17:55:14 root] (utils.py 302): INFO Epoch: [0]  [ 540/4926]  eta: 0:46:23  lr_architecture: 0.000050  loss_cls: 2.6567 (2.7770)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3543 (inf)  time: 0.6265  data: 0.0001  max mem: 14435
[2024-01-21 17:55:20 root] (utils.py 302): INFO Epoch: [0]  [ 550/4926]  eta: 0:46:15  lr_architecture: 0.000050  loss_cls: 2.4757 (2.7736)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4547 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:55:26 root] (utils.py 302): INFO Epoch: [0]  [ 560/4926]  eta: 0:46:08  lr_architecture: 0.000050  loss_cls: 2.5210 (2.7747)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4547 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:55:33 root] (utils.py 302): INFO Epoch: [0]  [ 570/4926]  eta: 0:46:00  lr_architecture: 0.000050  loss_cls: 3.0243 (2.7769)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2466 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:55:39 root] (utils.py 302): INFO Epoch: [0]  [ 580/4926]  eta: 0:45:53  lr_architecture: 0.000050  loss_cls: 2.8923 (2.7762)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2466 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:55:45 root] (utils.py 302): INFO Epoch: [0]  [ 590/4926]  eta: 0:45:45  lr_architecture: 0.000050  loss_cls: 2.8482 (2.7704)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2825 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:55:51 root] (utils.py 302): INFO Epoch: [0]  [ 600/4926]  eta: 0:45:40  lr_architecture: 0.000050  loss_cls: 2.6231 (2.7688)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3029 (inf)  time: 0.6312  data: 0.0001  max mem: 14435
[2024-01-21 17:55:58 root] (utils.py 302): INFO Epoch: [0]  [ 610/4926]  eta: 0:45:32  lr_architecture: 0.000050  loss_cls: 2.8041 (2.7704)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1365 (inf)  time: 0.6315  data: 0.0001  max mem: 14435
[2024-01-21 17:56:04 root] (utils.py 302): INFO Epoch: [0]  [ 620/4926]  eta: 0:45:25  lr_architecture: 0.000050  loss_cls: 2.8892 (2.7712)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1741 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 17:56:10 root] (utils.py 302): INFO Epoch: [0]  [ 630/4926]  eta: 0:45:17  lr_architecture: 0.000050  loss_cls: 2.9855 (2.7762)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3385 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:56:16 root] (utils.py 302): INFO Epoch: [0]  [ 640/4926]  eta: 0:45:10  lr_architecture: 0.000050  loss_cls: 3.0067 (2.7761)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1989 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 17:56:22 root] (utils.py 302): INFO Epoch: [0]  [ 650/4926]  eta: 0:45:03  lr_architecture: 0.000050  loss_cls: 2.6763 (2.7743)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3483 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 17:56:28 root] (utils.py 302): INFO Epoch: [0]  [ 660/4926]  eta: 0:44:56  lr_architecture: 0.000050  loss_cls: 2.8291 (2.7750)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3698 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:56:35 root] (utils.py 302): INFO Epoch: [0]  [ 670/4926]  eta: 0:44:48  lr_architecture: 0.000050  loss_cls: 2.9119 (2.7768)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6347 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 17:56:41 root] (utils.py 302): INFO Epoch: [0]  [ 680/4926]  eta: 0:44:41  lr_architecture: 0.000050  loss_cls: 2.9119 (2.7756)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6008 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 17:56:47 root] (utils.py 302): INFO Epoch: [0]  [ 690/4926]  eta: 0:44:34  lr_architecture: 0.000050  loss_cls: 2.9534 (2.7752)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4817 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 17:56:53 root] (utils.py 302): INFO Epoch: [0]  [ 700/4926]  eta: 0:44:27  lr_architecture: 0.000050  loss_cls: 2.9059 (2.7744)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4146 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:56:59 root] (utils.py 302): INFO Epoch: [0]  [ 710/4926]  eta: 0:44:20  lr_architecture: 0.000050  loss_cls: 2.9299 (2.7769)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2502 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:57:05 root] (utils.py 302): INFO Epoch: [0]  [ 720/4926]  eta: 0:44:13  lr_architecture: 0.000050  loss_cls: 3.0799 (2.7791)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.5115 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 17:57:12 root] (utils.py 302): INFO Epoch: [0]  [ 730/4926]  eta: 0:44:06  lr_architecture: 0.000050  loss_cls: 3.0815 (2.7809)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.5500 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:57:18 root] (utils.py 302): INFO Epoch: [0]  [ 740/4926]  eta: 0:43:59  lr_architecture: 0.000050  loss_cls: 2.9726 (2.7798)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1203 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:57:24 root] (utils.py 302): INFO Epoch: [0]  [ 750/4926]  eta: 0:43:52  lr_architecture: 0.000050  loss_cls: 2.8648 (2.7797)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0405 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 17:57:30 root] (utils.py 302): INFO Epoch: [0]  [ 760/4926]  eta: 0:43:45  lr_architecture: 0.000050  loss_cls: 2.8855 (2.7829)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3628 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:57:36 root] (utils.py 302): INFO Epoch: [0]  [ 770/4926]  eta: 0:43:38  lr_architecture: 0.000050  loss_cls: 2.8767 (2.7809)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.6187 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:57:43 root] (utils.py 302): INFO Epoch: [0]  [ 780/4926]  eta: 0:43:31  lr_architecture: 0.000050  loss_cls: 2.7014 (2.7802)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4230 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 17:57:49 root] (utils.py 302): INFO Epoch: [0]  [ 790/4926]  eta: 0:43:24  lr_architecture: 0.000050  loss_cls: 2.7014 (2.7788)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1810 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:57:55 root] (utils.py 302): INFO Epoch: [0]  [ 800/4926]  eta: 0:43:17  lr_architecture: 0.000050  loss_cls: 2.9168 (2.7805)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1586 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:58:01 root] (utils.py 302): INFO Epoch: [0]  [ 810/4926]  eta: 0:43:10  lr_architecture: 0.000050  loss_cls: 2.9179 (2.7795)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1471 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 17:58:07 root] (utils.py 302): INFO Epoch: [0]  [ 820/4926]  eta: 0:43:03  lr_architecture: 0.000050  loss_cls: 2.9971 (2.7826)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9418 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 17:58:13 root] (utils.py 302): INFO Epoch: [0]  [ 830/4926]  eta: 0:42:56  lr_architecture: 0.000050  loss_cls: 2.9367 (2.7817)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0879 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:58:20 root] (utils.py 302): INFO Epoch: [0]  [ 840/4926]  eta: 0:42:49  lr_architecture: 0.000050  loss_cls: 2.7219 (2.7812)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2562 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 17:58:26 root] (utils.py 302): INFO Epoch: [0]  [ 850/4926]  eta: 0:42:43  lr_architecture: 0.000050  loss_cls: 2.7390 (2.7814)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4314 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:58:32 root] (utils.py 302): INFO Epoch: [0]  [ 860/4926]  eta: 0:42:36  lr_architecture: 0.000050  loss_cls: 2.6011 (2.7788)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0060 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 17:58:38 root] (utils.py 302): INFO Epoch: [0]  [ 870/4926]  eta: 0:42:29  lr_architecture: 0.000050  loss_cls: 2.6972 (2.7809)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9770 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:58:44 root] (utils.py 302): INFO Epoch: [0]  [ 880/4926]  eta: 0:42:22  lr_architecture: 0.000050  loss_cls: 2.8948 (2.7814)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2608 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:58:50 root] (utils.py 302): INFO Epoch: [0]  [ 890/4926]  eta: 0:42:15  lr_architecture: 0.000050  loss_cls: 2.9769 (2.7842)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1742 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 17:58:57 root] (utils.py 302): INFO Epoch: [0]  [ 900/4926]  eta: 0:42:09  lr_architecture: 0.000050  loss_cls: 2.8889 (2.7829)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1742 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 17:59:03 root] (utils.py 302): INFO Epoch: [0]  [ 910/4926]  eta: 0:42:02  lr_architecture: 0.000050  loss_cls: 2.5974 (2.7815)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2509 (inf)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 17:59:09 root] (utils.py 302): INFO Epoch: [0]  [ 920/4926]  eta: 0:41:55  lr_architecture: 0.000050  loss_cls: 2.6158 (2.7794)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4124 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 17:59:15 root] (utils.py 302): INFO Epoch: [0]  [ 930/4926]  eta: 0:41:49  lr_architecture: 0.000050  loss_cls: 2.7170 (2.7801)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4424 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 17:59:21 root] (utils.py 302): INFO Epoch: [0]  [ 940/4926]  eta: 0:41:42  lr_architecture: 0.000050  loss_cls: 2.7648 (2.7794)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3109 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 17:59:28 root] (utils.py 302): INFO Epoch: [0]  [ 950/4926]  eta: 0:41:35  lr_architecture: 0.000050  loss_cls: 2.5603 (2.7777)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2997 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 17:59:34 root] (utils.py 302): INFO Epoch: [0]  [ 960/4926]  eta: 0:41:28  lr_architecture: 0.000050  loss_cls: 2.5445 (2.7761)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1092 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:59:40 root] (utils.py 302): INFO Epoch: [0]  [ 970/4926]  eta: 0:41:22  lr_architecture: 0.000050  loss_cls: 2.8145 (2.7756)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1582 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 17:59:46 root] (utils.py 302): INFO Epoch: [0]  [ 980/4926]  eta: 0:41:15  lr_architecture: 0.000050  loss_cls: 2.9791 (2.7784)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2626 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 17:59:52 root] (utils.py 302): INFO Epoch: [0]  [ 990/4926]  eta: 0:41:08  lr_architecture: 0.000050  loss_cls: 2.9989 (2.7793)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4092 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 17:59:58 root] (utils.py 302): INFO Epoch: [0]  [1000/4926]  eta: 0:41:02  lr_architecture: 0.000050  loss_cls: 2.9556 (2.7812)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1340 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:00:05 root] (utils.py 302): INFO Epoch: [0]  [1010/4926]  eta: 0:40:55  lr_architecture: 0.000050  loss_cls: 2.9125 (2.7822)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0145 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:00:11 root] (utils.py 302): INFO Epoch: [0]  [1020/4926]  eta: 0:40:48  lr_architecture: 0.000050  loss_cls: 2.9125 (2.7835)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1103 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:00:17 root] (utils.py 302): INFO Epoch: [0]  [1030/4926]  eta: 0:40:42  lr_architecture: 0.000050  loss_cls: 2.9398 (2.7837)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2242 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:00:23 root] (utils.py 302): INFO Epoch: [0]  [1040/4926]  eta: 0:40:35  lr_architecture: 0.000050  loss_cls: 2.7347 (2.7821)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1670 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:00:29 root] (utils.py 302): INFO Epoch: [0]  [1050/4926]  eta: 0:40:29  lr_architecture: 0.000050  loss_cls: 2.7936 (2.7830)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1596 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:00:36 root] (utils.py 302): INFO Epoch: [0]  [1060/4926]  eta: 0:40:22  lr_architecture: 0.000050  loss_cls: 2.7936 (2.7825)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0127 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:00:42 root] (utils.py 302): INFO Epoch: [0]  [1070/4926]  eta: 0:40:15  lr_architecture: 0.000050  loss_cls: 2.8000 (2.7815)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9255 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:00:48 root] (utils.py 302): INFO Epoch: [0]  [1080/4926]  eta: 0:40:09  lr_architecture: 0.000050  loss_cls: 2.8081 (2.7818)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0092 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:00:54 root] (utils.py 302): INFO Epoch: [0]  [1090/4926]  eta: 0:40:02  lr_architecture: 0.000050  loss_cls: 2.8235 (2.7813)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0889 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
[2024-01-21 18:01:00 root] (utils.py 302): INFO Epoch: [0]  [1100/4926]  eta: 0:39:56  lr_architecture: 0.000050  loss_cls: 2.6282 (2.7797)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1153 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:01:06 root] (utils.py 302): INFO Epoch: [0]  [1110/4926]  eta: 0:39:49  lr_architecture: 0.000050  loss_cls: 2.6282 (2.7799)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2399 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 18:01:13 root] (utils.py 302): INFO Epoch: [0]  [1120/4926]  eta: 0:39:43  lr_architecture: 0.000050  loss_cls: 2.9805 (2.7810)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2399 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:01:19 root] (utils.py 302): INFO Epoch: [0]  [1130/4926]  eta: 0:39:36  lr_architecture: 0.000050  loss_cls: 2.8466 (2.7811)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0182 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:01:25 root] (utils.py 302): INFO Epoch: [0]  [1140/4926]  eta: 0:39:30  lr_architecture: 0.000050  loss_cls: 2.7685 (2.7809)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8994 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:01:31 root] (utils.py 302): INFO Epoch: [0]  [1150/4926]  eta: 0:39:23  lr_architecture: 0.000050  loss_cls: 2.5523 (2.7775)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8269 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 18:01:37 root] (utils.py 302): INFO Epoch: [0]  [1160/4926]  eta: 0:39:17  lr_architecture: 0.000050  loss_cls: 2.8473 (2.7794)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8349 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:01:44 root] (utils.py 302): INFO Epoch: [0]  [1170/4926]  eta: 0:39:10  lr_architecture: 0.000050  loss_cls: 2.7691 (2.7784)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7490 (inf)  time: 0.6157  data: 0.0001  max mem: 14435
[2024-01-21 18:01:50 root] (utils.py 302): INFO Epoch: [0]  [1180/4926]  eta: 0:39:04  lr_architecture: 0.000050  loss_cls: 2.7691 (2.7814)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8116 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 18:01:56 root] (utils.py 302): INFO Epoch: [0]  [1190/4926]  eta: 0:38:57  lr_architecture: 0.000050  loss_cls: 2.9732 (2.7821)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8462 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:02:02 root] (utils.py 302): INFO Epoch: [0]  [1200/4926]  eta: 0:38:51  lr_architecture: 0.000050  loss_cls: 2.8332 (2.7817)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0580 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:02:08 root] (utils.py 302): INFO Epoch: [0]  [1210/4926]  eta: 0:38:44  lr_architecture: 0.000050  loss_cls: 2.8086 (2.7819)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2814 (inf)  time: 0.6166  data: 0.0001  max mem: 14435
[2024-01-21 18:02:14 root] (utils.py 302): INFO Epoch: [0]  [1220/4926]  eta: 0:38:37  lr_architecture: 0.000050  loss_cls: 2.8857 (2.7817)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2515 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:02:21 root] (utils.py 302): INFO Epoch: [0]  [1230/4926]  eta: 0:38:31  lr_architecture: 0.000050  loss_cls: 2.8700 (2.7806)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2051 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:02:27 root] (utils.py 302): INFO Epoch: [0]  [1240/4926]  eta: 0:38:24  lr_architecture: 0.000050  loss_cls: 2.9342 (2.7823)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0342 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 18:02:33 root] (utils.py 302): INFO Epoch: [0]  [1250/4926]  eta: 0:38:18  lr_architecture: 0.000050  loss_cls: 3.0495 (2.7847)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8747 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:02:39 root] (utils.py 302): INFO Epoch: [0]  [1260/4926]  eta: 0:38:12  lr_architecture: 0.000050  loss_cls: 2.9982 (2.7849)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7353 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:02:45 root] (utils.py 302): INFO Epoch: [0]  [1270/4926]  eta: 0:38:05  lr_architecture: 0.000050  loss_cls: 2.9877 (2.7867)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0784 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 18:02:51 root] (utils.py 302): INFO Epoch: [0]  [1280/4926]  eta: 0:37:59  lr_architecture: 0.000050  loss_cls: 2.8736 (2.7847)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2665 (inf)  time: 0.6178  data: 0.0001  max mem: 14435
[2024-01-21 18:02:58 root] (utils.py 302): INFO Epoch: [0]  [1290/4926]  eta: 0:37:52  lr_architecture: 0.000050  loss_cls: 2.8736 (2.7856)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.4759 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:03:04 root] (utils.py 302): INFO Epoch: [0]  [1300/4926]  eta: 0:37:46  lr_architecture: 0.000050  loss_cls: 2.9026 (2.7859)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.3382 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:03:10 root] (utils.py 302): INFO Epoch: [0]  [1310/4926]  eta: 0:37:39  lr_architecture: 0.000050  loss_cls: 2.8141 (2.7860)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1645 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:03:16 root] (utils.py 302): INFO Epoch: [0]  [1320/4926]  eta: 0:37:33  lr_architecture: 0.000050  loss_cls: 2.7934 (2.7865)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0273 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:03:22 root] (utils.py 302): INFO Epoch: [0]  [1330/4926]  eta: 0:37:26  lr_architecture: 0.000050  loss_cls: 2.7259 (2.7865)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8058 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:03:28 root] (utils.py 302): INFO Epoch: [0]  [1340/4926]  eta: 0:37:20  lr_architecture: 0.000050  loss_cls: 2.8051 (2.7874)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8058 (inf)  time: 0.6165  data: 0.0001  max mem: 14435
[2024-01-21 18:03:35 root] (utils.py 302): INFO Epoch: [0]  [1350/4926]  eta: 0:37:13  lr_architecture: 0.000050  loss_cls: 2.8051 (2.7872)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8322 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:03:41 root] (utils.py 302): INFO Epoch: [0]  [1360/4926]  eta: 0:37:07  lr_architecture: 0.000050  loss_cls: 2.7941 (2.7869)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8372 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:03:47 root] (utils.py 302): INFO Epoch: [0]  [1370/4926]  eta: 0:37:00  lr_architecture: 0.000050  loss_cls: 2.8239 (2.7879)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2666 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 18:03:53 root] (utils.py 302): INFO Epoch: [0]  [1380/4926]  eta: 0:36:54  lr_architecture: 0.000050  loss_cls: 2.7832 (2.7857)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0463 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:03:59 root] (utils.py 302): INFO Epoch: [0]  [1390/4926]  eta: 0:36:48  lr_architecture: 0.000050  loss_cls: 2.5922 (2.7859)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0112 (inf)  time: 0.6198  data: 0.0001  max mem: 14435
[2024-01-21 18:04:06 root] (utils.py 302): INFO Epoch: [0]  [1400/4926]  eta: 0:36:41  lr_architecture: 0.000050  loss_cls: 2.7980 (2.7860)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0490 (inf)  time: 0.6198  data: 0.0001  max mem: 14435
[2024-01-21 18:04:12 root] (utils.py 302): INFO Epoch: [0]  [1410/4926]  eta: 0:36:35  lr_architecture: 0.000050  loss_cls: 2.7324 (2.7853)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9136 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:04:18 root] (utils.py 302): INFO Epoch: [0]  [1420/4926]  eta: 0:36:28  lr_architecture: 0.000050  loss_cls: 2.7187 (2.7847)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9136 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:04:24 root] (utils.py 302): INFO Epoch: [0]  [1430/4926]  eta: 0:36:22  lr_architecture: 0.000050  loss_cls: 3.0037 (2.7866)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0128 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:04:30 root] (utils.py 302): INFO Epoch: [0]  [1440/4926]  eta: 0:36:16  lr_architecture: 0.000050  loss_cls: 2.9097 (2.7871)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0694 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:04:36 root] (utils.py 302): INFO Epoch: [0]  [1450/4926]  eta: 0:36:09  lr_architecture: 0.000050  loss_cls: 2.9097 (2.7878)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0275 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:04:43 root] (utils.py 302): INFO Epoch: [0]  [1460/4926]  eta: 0:36:03  lr_architecture: 0.000050  loss_cls: 2.9031 (2.7878)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7960 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:04:49 root] (utils.py 302): INFO Epoch: [0]  [1470/4926]  eta: 0:35:56  lr_architecture: 0.000050  loss_cls: 2.7449 (2.7867)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8363 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:04:55 root] (utils.py 302): INFO Epoch: [0]  [1480/4926]  eta: 0:35:50  lr_architecture: 0.000050  loss_cls: 2.7344 (2.7872)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8363 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:05:01 root] (utils.py 302): INFO Epoch: [0]  [1490/4926]  eta: 0:35:44  lr_architecture: 0.000050  loss_cls: 2.8094 (2.7882)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9056 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:05:07 root] (utils.py 302): INFO Epoch: [0]  [1500/4926]  eta: 0:35:37  lr_architecture: 0.000050  loss_cls: 2.9390 (2.7879)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0355 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:05:13 root] (utils.py 302): INFO Epoch: [0]  [1510/4926]  eta: 0:35:31  lr_architecture: 0.000050  loss_cls: 2.8939 (2.7891)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0300 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:05:20 root] (utils.py 302): INFO Epoch: [0]  [1520/4926]  eta: 0:35:24  lr_architecture: 0.000050  loss_cls: 2.8444 (2.7882)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8817 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:05:26 root] (utils.py 302): INFO Epoch: [0]  [1530/4926]  eta: 0:35:18  lr_architecture: 0.000050  loss_cls: 2.5515 (2.7871)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8144 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:05:32 root] (utils.py 302): INFO Epoch: [0]  [1540/4926]  eta: 0:35:12  lr_architecture: 0.000050  loss_cls: 2.8638 (2.7883)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6871 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 18:05:38 root] (utils.py 302): INFO Epoch: [0]  [1550/4926]  eta: 0:35:05  lr_architecture: 0.000050  loss_cls: 3.0649 (2.7895)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7930 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:05:44 root] (utils.py 302): INFO Epoch: [0]  [1560/4926]  eta: 0:34:59  lr_architecture: 0.000050  loss_cls: 3.0736 (2.7903)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1756 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:05:51 root] (utils.py 302): INFO Epoch: [0]  [1570/4926]  eta: 0:34:53  lr_architecture: 0.000050  loss_cls: 2.8747 (2.7895)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0830 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:05:57 root] (utils.py 302): INFO Epoch: [0]  [1580/4926]  eta: 0:34:46  lr_architecture: 0.000050  loss_cls: 2.8563 (2.7904)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8671 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:06:03 root] (utils.py 302): INFO Epoch: [0]  [1590/4926]  eta: 0:34:40  lr_architecture: 0.000050  loss_cls: 2.7953 (2.7907)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2101 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:06:09 root] (utils.py 302): INFO Epoch: [0]  [1600/4926]  eta: 0:34:33  lr_architecture: 0.000050  loss_cls: 2.7834 (2.7905)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.2909 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:06:15 root] (utils.py 302): INFO Epoch: [0]  [1610/4926]  eta: 0:34:27  lr_architecture: 0.000050  loss_cls: 2.9494 (2.7916)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9003 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:06:21 root] (utils.py 302): INFO Epoch: [0]  [1620/4926]  eta: 0:34:21  lr_architecture: 0.000050  loss_cls: 3.1542 (2.7927)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0110 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:06:28 root] (utils.py 302): INFO Epoch: [0]  [1630/4926]  eta: 0:34:14  lr_architecture: 0.000050  loss_cls: 2.9836 (2.7932)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7389 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:06:34 root] (utils.py 302): INFO Epoch: [0]  [1640/4926]  eta: 0:34:08  lr_architecture: 0.000050  loss_cls: 2.8748 (2.7938)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7246 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:06:40 root] (utils.py 302): INFO Epoch: [0]  [1650/4926]  eta: 0:34:02  lr_architecture: 0.000050  loss_cls: 2.9359 (2.7944)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7044 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:06:46 root] (utils.py 302): INFO Epoch: [0]  [1660/4926]  eta: 0:33:55  lr_architecture: 0.000050  loss_cls: 2.9234 (2.7953)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6887 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:06:52 root] (utils.py 302): INFO Epoch: [0]  [1670/4926]  eta: 0:33:49  lr_architecture: 0.000050  loss_cls: 2.8706 (2.7951)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8399 (inf)  time: 0.6166  data: 0.0001  max mem: 14435
[2024-01-21 18:06:58 root] (utils.py 302): INFO Epoch: [0]  [1680/4926]  eta: 0:33:43  lr_architecture: 0.000050  loss_cls: 2.9635 (2.7961)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9823 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:07:05 root] (utils.py 302): INFO Epoch: [0]  [1690/4926]  eta: 0:33:36  lr_architecture: 0.000050  loss_cls: 2.9579 (2.7959)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9823 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:07:11 root] (utils.py 302): INFO Epoch: [0]  [1700/4926]  eta: 0:33:30  lr_architecture: 0.000050  loss_cls: 2.9638 (2.7970)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9441 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:07:17 root] (utils.py 302): INFO Epoch: [0]  [1710/4926]  eta: 0:33:24  lr_architecture: 0.000050  loss_cls: 2.9664 (2.7961)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9527 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:07:23 root] (utils.py 302): INFO Epoch: [0]  [1720/4926]  eta: 0:33:17  lr_architecture: 0.000050  loss_cls: 2.7389 (2.7955)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8780 (inf)  time: 0.6230  data: 0.0001  max mem: 14435
[2024-01-21 18:07:29 root] (utils.py 302): INFO Epoch: [0]  [1730/4926]  eta: 0:33:11  lr_architecture: 0.000050  loss_cls: 2.7971 (2.7964)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9376 (inf)  time: 0.6232  data: 0.0001  max mem: 14435
[2024-01-21 18:07:36 root] (utils.py 302): INFO Epoch: [0]  [1740/4926]  eta: 0:33:05  lr_architecture: 0.000050  loss_cls: 2.8358 (2.7949)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0414 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:07:42 root] (utils.py 302): INFO Epoch: [0]  [1750/4926]  eta: 0:32:58  lr_architecture: 0.000050  loss_cls: 2.8322 (2.7941)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9026 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:07:48 root] (utils.py 302): INFO Epoch: [0]  [1760/4926]  eta: 0:32:52  lr_architecture: 0.000050  loss_cls: 2.7498 (2.7934)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8929 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:07:54 root] (utils.py 302): INFO Epoch: [0]  [1770/4926]  eta: 0:32:46  lr_architecture: 0.000050  loss_cls: 2.8272 (2.7938)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9366 (inf)  time: 0.6230  data: 0.0001  max mem: 14435
[2024-01-21 18:08:01 root] (utils.py 302): INFO Epoch: [0]  [1780/4926]  eta: 0:32:40  lr_architecture: 0.000050  loss_cls: 2.8272 (2.7926)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9497 (inf)  time: 0.6294  data: 0.0001  max mem: 14435
[2024-01-21 18:08:07 root] (utils.py 302): INFO Epoch: [0]  [1790/4926]  eta: 0:32:34  lr_architecture: 0.000050  loss_cls: 2.6617 (2.7926)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6417 (inf)  time: 0.6240  data: 0.0001  max mem: 14435
[2024-01-21 18:08:13 root] (utils.py 302): INFO Epoch: [0]  [1800/4926]  eta: 0:32:27  lr_architecture: 0.000050  loss_cls: 2.6617 (2.7927)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6119 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:08:19 root] (utils.py 302): INFO Epoch: [0]  [1810/4926]  eta: 0:32:21  lr_architecture: 0.000050  loss_cls: 2.6617 (2.7920)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.5780 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:08:25 root] (utils.py 302): INFO Epoch: [0]  [1820/4926]  eta: 0:32:15  lr_architecture: 0.000050  loss_cls: 2.6772 (2.7916)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6436 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:08:31 root] (utils.py 302): INFO Epoch: [0]  [1830/4926]  eta: 0:32:08  lr_architecture: 0.000050  loss_cls: 2.6541 (2.7906)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7217 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:08:38 root] (utils.py 302): INFO Epoch: [0]  [1840/4926]  eta: 0:32:02  lr_architecture: 0.000050  loss_cls: 2.5352 (2.7893)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7587 (inf)  time: 0.6171  data: 0.0001  max mem: 14435
[2024-01-21 18:08:44 root] (utils.py 302): INFO Epoch: [0]  [1850/4926]  eta: 0:31:56  lr_architecture: 0.000050  loss_cls: 2.6606 (2.7896)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8273 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:08:50 root] (utils.py 302): INFO Epoch: [0]  [1860/4926]  eta: 0:31:49  lr_architecture: 0.000050  loss_cls: 2.8619 (2.7896)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9505 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:08:56 root] (utils.py 302): INFO Epoch: [0]  [1870/4926]  eta: 0:31:43  lr_architecture: 0.000050  loss_cls: 2.9769 (2.7904)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9542 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:09:02 root] (utils.py 302): INFO Epoch: [0]  [1880/4926]  eta: 0:31:37  lr_architecture: 0.000050  loss_cls: 2.5506 (2.7876)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8330 (inf)  time: 0.6172  data: 0.0001  max mem: 14435
[2024-01-21 18:09:08 root] (utils.py 302): INFO Epoch: [0]  [1890/4926]  eta: 0:31:30  lr_architecture: 0.000050  loss_cls: 2.2791 (2.7874)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0078 (inf)  time: 0.6169  data: 0.0001  max mem: 14435
[2024-01-21 18:09:15 root] (utils.py 302): INFO Epoch: [0]  [1900/4926]  eta: 0:31:24  lr_architecture: 0.000050  loss_cls: 2.4746 (2.7857)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8176 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:09:21 root] (utils.py 302): INFO Epoch: [0]  [1910/4926]  eta: 0:31:18  lr_architecture: 0.000050  loss_cls: 2.8419 (2.7860)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7115 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:09:27 root] (utils.py 302): INFO Epoch: [0]  [1920/4926]  eta: 0:31:11  lr_architecture: 0.000050  loss_cls: 2.8455 (2.7852)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6049 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:09:33 root] (utils.py 302): INFO Epoch: [0]  [1930/4926]  eta: 0:31:05  lr_architecture: 0.000050  loss_cls: 2.8455 (2.7852)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6834 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:09:39 root] (utils.py 302): INFO Epoch: [0]  [1940/4926]  eta: 0:30:59  lr_architecture: 0.000050  loss_cls: 2.6528 (2.7850)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7753 (inf)  time: 0.6174  data: 0.0001  max mem: 14435
[2024-01-21 18:09:45 root] (utils.py 302): INFO Epoch: [0]  [1950/4926]  eta: 0:30:52  lr_architecture: 0.000050  loss_cls: 2.7610 (2.7844)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6946 (inf)  time: 0.6177  data: 0.0001  max mem: 14435
[2024-01-21 18:09:52 root] (utils.py 302): INFO Epoch: [0]  [1960/4926]  eta: 0:30:46  lr_architecture: 0.000050  loss_cls: 2.7615 (2.7846)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8224 (inf)  time: 0.6181  data: 0.0001  max mem: 14435
[2024-01-21 18:09:58 root] (utils.py 302): INFO Epoch: [0]  [1970/4926]  eta: 0:30:40  lr_architecture: 0.000050  loss_cls: 2.7279 (2.7832)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9604 (inf)  time: 0.6192  data: 0.0001  max mem: 14435
[2024-01-21 18:10:04 root] (utils.py 302): INFO Epoch: [0]  [1980/4926]  eta: 0:30:34  lr_architecture: 0.000050  loss_cls: 2.8427 (2.7837)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.0287 (inf)  time: 0.6188  data: 0.0001  max mem: 14435
[2024-01-21 18:10:10 root] (utils.py 302): INFO Epoch: [0]  [1990/4926]  eta: 0:30:27  lr_architecture: 0.000050  loss_cls: 2.9947 (2.7845)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9067 (inf)  time: 0.6180  data: 0.0001  max mem: 14435
[2024-01-21 18:10:16 root] (utils.py 302): INFO Epoch: [0]  [2000/4926]  eta: 0:30:21  lr_architecture: 0.000050  loss_cls: 2.9348 (2.7846)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6025 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:10:23 root] (utils.py 302): INFO Epoch: [0]  [2010/4926]  eta: 0:30:15  lr_architecture: 0.000050  loss_cls: 2.7061 (2.7845)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8850 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:10:29 root] (utils.py 302): INFO Epoch: [0]  [2020/4926]  eta: 0:30:08  lr_architecture: 0.000050  loss_cls: 2.8839 (2.7845)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7547 (inf)  time: 0.6167  data: 0.0001  max mem: 14435
[2024-01-21 18:10:35 root] (utils.py 302): INFO Epoch: [0]  [2030/4926]  eta: 0:30:02  lr_architecture: 0.000050  loss_cls: 2.9040 (2.7850)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6635 (inf)  time: 0.6164  data: 0.0001  max mem: 14435
[2024-01-21 18:10:41 root] (utils.py 302): INFO Epoch: [0]  [2040/4926]  eta: 0:29:56  lr_architecture: 0.000050  loss_cls: 2.9197 (2.7859)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7570 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:10:47 root] (utils.py 302): INFO Epoch: [0]  [2050/4926]  eta: 0:29:50  lr_architecture: 0.000050  loss_cls: 2.8261 (2.7852)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7615 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 18:10:53 root] (utils.py 302): INFO Epoch: [0]  [2060/4926]  eta: 0:29:43  lr_architecture: 0.000050  loss_cls: 2.7854 (2.7850)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1213 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 18:11:00 root] (utils.py 302): INFO Epoch: [0]  [2070/4926]  eta: 0:29:37  lr_architecture: 0.000050  loss_cls: 2.7854 (2.7846)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 5.1233 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:11:06 root] (utils.py 302): INFO Epoch: [0]  [2080/4926]  eta: 0:29:31  lr_architecture: 0.000050  loss_cls: 2.7721 (2.7840)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9250 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:11:12 root] (utils.py 302): INFO Epoch: [0]  [2090/4926]  eta: 0:29:24  lr_architecture: 0.000050  loss_cls: 2.6503 (2.7830)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9216 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:11:18 root] (utils.py 302): INFO Epoch: [0]  [2100/4926]  eta: 0:29:18  lr_architecture: 0.000050  loss_cls: 2.6982 (2.7830)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9333 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 18:11:24 root] (utils.py 302): INFO Epoch: [0]  [2110/4926]  eta: 0:29:12  lr_architecture: 0.000050  loss_cls: 2.7874 (2.7835)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8400 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:11:31 root] (utils.py 302): INFO Epoch: [0]  [2120/4926]  eta: 0:29:05  lr_architecture: 0.000050  loss_cls: 2.8160 (2.7830)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.3897 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:11:37 root] (utils.py 302): INFO Epoch: [0]  [2130/4926]  eta: 0:28:59  lr_architecture: 0.000050  loss_cls: 2.7187 (2.7822)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6892 (inf)  time: 0.6170  data: 0.0001  max mem: 14435
[2024-01-21 18:11:43 root] (utils.py 302): INFO Epoch: [0]  [2140/4926]  eta: 0:28:53  lr_architecture: 0.000050  loss_cls: 2.8428 (2.7826)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7667 (inf)  time: 0.6168  data: 0.0001  max mem: 14435
[2024-01-21 18:11:49 root] (utils.py 302): INFO Epoch: [0]  [2150/4926]  eta: 0:28:47  lr_architecture: 0.000050  loss_cls: 2.9527 (2.7842)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9896 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:11:55 root] (utils.py 302): INFO Epoch: [0]  [2160/4926]  eta: 0:28:40  lr_architecture: 0.000050  loss_cls: 2.9527 (2.7848)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8249 (inf)  time: 0.6183  data: 0.0001  max mem: 14435
[2024-01-21 18:12:01 root] (utils.py 302): INFO Epoch: [0]  [2170/4926]  eta: 0:28:34  lr_architecture: 0.000050  loss_cls: 2.8797 (2.7839)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8249 (inf)  time: 0.6182  data: 0.0001  max mem: 14435
[2024-01-21 18:12:08 root] (utils.py 302): INFO Epoch: [0]  [2180/4926]  eta: 0:28:28  lr_architecture: 0.000050  loss_cls: 2.5052 (2.7828)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9852 (inf)  time: 0.6175  data: 0.0001  max mem: 14435
[2024-01-21 18:12:14 root] (utils.py 302): INFO Epoch: [0]  [2190/4926]  eta: 0:28:22  lr_architecture: 0.000050  loss_cls: 2.8313 (2.7832)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9460 (inf)  time: 0.6179  data: 0.0001  max mem: 14435
[2024-01-21 18:12:20 root] (utils.py 302): INFO Epoch: [0]  [2200/4926]  eta: 0:28:15  lr_architecture: 0.000050  loss_cls: 2.8956 (2.7836)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8984 (inf)  time: 0.6193  data: 0.0001  max mem: 14435
[2024-01-21 18:12:26 root] (utils.py 302): INFO Epoch: [0]  [2210/4926]  eta: 0:28:09  lr_architecture: 0.000050  loss_cls: 2.8439 (2.7833)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8984 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:12:32 root] (utils.py 302): INFO Epoch: [0]  [2220/4926]  eta: 0:28:03  lr_architecture: 0.000050  loss_cls: 2.7212 (2.7832)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6894 (inf)  time: 0.6173  data: 0.0001  max mem: 14435
[2024-01-21 18:12:38 root] (utils.py 302): INFO Epoch: [0]  [2230/4926]  eta: 0:27:56  lr_architecture: 0.000050  loss_cls: 2.7789 (2.7839)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.6500 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:12:45 root] (utils.py 302): INFO Epoch: [0]  [2240/4926]  eta: 0:27:50  lr_architecture: 0.000050  loss_cls: 2.9291 (2.7853)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7908 (inf)  time: 0.6176  data: 0.0001  max mem: 14435
[2024-01-21 18:12:51 root] (utils.py 302): INFO Epoch: [0]  [2250/4926]  eta: 0:27:44  lr_architecture: 0.000050  loss_cls: 3.0239 (2.7854)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9979 (inf)  time: 0.6185  data: 0.0001  max mem: 14435
[2024-01-21 18:12:57 root] (utils.py 302): INFO Epoch: [0]  [2260/4926]  eta: 0:27:38  lr_architecture: 0.000050  loss_cls: 2.8347 (2.7865)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.9130 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:13:03 root] (utils.py 302): INFO Epoch: [0]  [2270/4926]  eta: 0:27:31  lr_architecture: 0.000050  loss_cls: 2.9074 (2.7868)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7291 (inf)  time: 0.6184  data: 0.0001  max mem: 14435
[2024-01-21 18:13:09 root] (utils.py 302): INFO Epoch: [0]  [2280/4926]  eta: 0:27:25  lr_architecture: 0.000050  loss_cls: 2.8983 (2.7869)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.7291 (inf)  time: 0.6186  data: 0.0001  max mem: 14435
[2024-01-21 18:13:16 root] (utils.py 302): INFO Epoch: [0]  [2290/4926]  eta: 0:27:19  lr_architecture: 0.000050  loss_cls: 2.7196 (2.7864)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8465 (inf)  time: 0.6189  data: 0.0001  max mem: 14435
[2024-01-21 18:13:22 root] (utils.py 302): INFO Epoch: [0]  [2300/4926]  eta: 0:27:13  lr_architecture: 0.000050  loss_cls: 2.8315 (2.7871)  loss_flops: 811.2119 (811.2119)  flops: 31.4818 (31.4818)  grad_norm: 4.8706 (inf)  time: 0.6187  data: 0.0001  max mem: 14435
