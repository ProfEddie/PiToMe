{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /media/caduser/MyBook/chau/.cache/flickr30k/annotations/train.json\n",
      "Using downloaded and verified file: /media/caduser/MyBook/chau/.cache/flickr30k/annotations/val.json\n",
      "Using downloaded and verified file: /media/caduser/MyBook/chau/.cache/flickr30k/annotations/test.json\n"
     ]
    }
   ],
   "source": [
    "from lavis.datasets.builders import load_dataset\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "from algo import (\n",
    "    PITOME,\n",
    "    TOME,\n",
    "    DIFFRATE,\n",
    "    DCT,\n",
    "    TOFU,\n",
    "    LTMP,\n",
    "    NONE, \n",
    "    pitome,\n",
    "    tome,\n",
    "    DiffRate,\n",
    "    tofu,\n",
    "    # ltmp\n",
    ")\n",
    "\n",
    "FLICKR_PATH='/media/caduser/MyBook/chau/.cache/flickr30k/images'\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(\"blip_retrieval\", \"coco\", is_eval=False)\n",
    "dataset = load_dataset(\"flickr30k\", vis_path=FLICKR_PATH, cfg_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <lavis.datasets.datasets.retrieval_datasets.RetrievalDataset at 0x7f3cd2e54610>,\n",
       " 'val': <lavis.datasets.datasets.retrieval_datasets.RetrievalEvalDataset at 0x7f3b30811390>,\n",
       " 'test': <lavis.datasets.datasets.retrieval_datasets.RetrievalEvalDataset at 0x7f3b3064f8d0>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lavis.processors.base_processor import BaseProcessor\n",
    "from lavis.processors.randaugment import RandomAugment\n",
    "from omegaconf import OmegaConf\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from lavis.processors.blip_processors import BlipImageBaseProcessor\n",
    "\n",
    "img = dataset['train'][0]['image']\n",
    "img_input = vis_processors['eval'](img)\n",
    "# img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pitome\n",
      "using pitome\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlipRetrieval(\n",
       "  (visual_encoder): PiToMeVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x PiToMeBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): XBertEncoder(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vision_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (text_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (itm_head): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (visual_encoder_m): PiToMeVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x PiToMeBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder_m): XBertEncoder(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vision_proj_m): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (text_proj_m): Linear(in_features=768, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main_vl import (\n",
    "    PITOME,\n",
    "    TOME, \n",
    "    TOFU, \n",
    "    DIFFRATE, \n",
    ")\n",
    "\n",
    "def get_model(model, algo, ratio):\n",
    "    if algo == PITOME:\n",
    "        pitome.patch.blip(model.visual_encoder,use_k=False, trace_source=True)\n",
    "        pitome.patch.blip(model.visual_encoder_m,use_k=False, trace_source=True)\n",
    "        model.visual_encoder.ratio=ratio\n",
    "        model.visual_encoder_m.ratio=ratio\n",
    "    elif algo == TOME:\n",
    "        tome.patch.blip(model.visual_encoder,use_k=False, trace_source=True)\n",
    "        tome.patch.blip(model.visual_encoder_m,use_k=False, trace_source=True)\n",
    "        model.visual_encoder.ratio=ratio\n",
    "        model.visual_encoder_m.ratio=ratio\n",
    "    elif algo == TOFU:\n",
    "        tofu.patch.blip(model.visual_encoder,use_k=False, trace_source=True)\n",
    "        tofu.patch.blip(model.visual_encoder_m,use_k=False, trace_source=True)\n",
    "        model.visual_encoder.ratio=ratio\n",
    "        model.visual_encoder_m.ratio=ratio\n",
    "    elif algo == DIFFRATE:\n",
    "        DiffRate.patch.blip(model.visual_encoder,use_k=False, trace_source=True)\n",
    "        DiffRate.patch.blip(model.visual_encoder_m,use_k=False, trace_source=True)\n",
    "        model.visual_encoder.init_kept_num_using_ratio(ratio)\n",
    "        model.visual_encoder_m.init_kept_num_using_ratio(ratio)\n",
    "\n",
    "\n",
    "get_model(model,PITOME, 0.9)\n",
    "\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1561,  0.3989, -0.6911,  ..., -0.7228, -0.7252, -0.6876],\n",
       "         [-0.0675, -0.3633, -0.4782,  ...,  0.3754,  0.4632, -0.8394],\n",
       "         [ 0.2254, -0.2778,  0.3667,  ..., -0.3522, -0.1329, -0.4285],\n",
       "         ...,\n",
       "         [ 0.1090, -0.5618, -0.5600,  ...,  0.4161, -0.3511, -0.5478],\n",
       "         [ 0.4815, -0.8285, -0.4550,  ...,  0.2384, -0.6939, -0.6153],\n",
       "         [-0.7430,  0.0312,  0.2361,  ..., -1.4236,  0.0519, -0.5049]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual_encoder(img_input[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlipImageEvalProcessor(BlipImageBaseProcessor):\n",
    "    def __init__(self, image_size=384, mean=None, std=None):\n",
    "        super().__init__(mean=mean, std=std)\n",
    "\n",
    "      \n",
    "        transform_list = [\n",
    "            transforms.Resize(\n",
    "                (image_size, image_size), interpolation=InterpolationMode.BICUBIC\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # The visualization and model need different transforms\n",
    "        self.transform_vis  = transforms.Compose(transform_list)\n",
    "        self.transform_norm = transforms.Compose(transform_list + [\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize,\n",
    "        ])\n",
    "\n",
    "        def __call__(self, item):\n",
    "            return self.transform_vis(item), self.transo\n",
    "\n",
    "        @classmethod\n",
    "        def from_config(cls, cfg=None):\n",
    "            if cfg is None:\n",
    "                cfg = OmegaConf.create()\n",
    "\n",
    "            image_size = cfg.get(\"image_size\", 384)\n",
    "\n",
    "            mean = cfg.get(\"mean\", None)\n",
    "            std = cfg.get(\"std\", None)\n",
    "\n",
    "            return cls(image_size=image_size, mean=mean, std=std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "source = model.visual_encoder._tome_info['source']\n",
    "\n",
    "DiffRate.make_visualization(img_vis, source, patch_size=16, class_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PiToMe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
